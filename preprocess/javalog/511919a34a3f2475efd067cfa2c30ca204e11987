commit 511919a34a3f2475efd067cfa2c30ca204e11987
Author:     Benjamin Niemann <pink@odahoda.de>
AuthorDate: Fri Apr 8 09:28:27 2011 -0800
Commit:     Benjamin Niemann <pink@odahoda.de>
CommitDate: Fri Apr 8 09:28:27 2011 -0800

[Python] More catching up. Getting closer to HEAD.

[git-p4: depot-paths = "//depot/code/antlr/antlr3-main/": change = 8106]

diff --git a/runtime/Python/antlr3/debug.py b/runtime/Python/antlr3/debug.py
index f20d2d4..a71b0d1 100644
--- a/runtime/Python/antlr3/debug.py
+++ b/runtime/Python/antlr3/debug.py
@@ -79,12 +79,11 @@ class DebugParser(Parser):


def reportError(self, exc):
+        Parser.reportError(self, exc)
+
if isinstance(exc, RecognitionException):
self._dbg.recognitionException(exc)

-        else:
-            traceback.print_exc(exc)
-

class DebugTokenStream(TokenStream):
def __init__(self, input, dbg=None):
@@ -128,7 +127,7 @@ class DebugTokenStream(TokenStream):

def consumeInitialHiddenTokens(self):
"""consume all initial off-channel tokens"""
-
+
firstOnChannelTokenIndex = self.input.index()
for idx in range(firstOnChannelTokenIndex):
self._dbg.consumeHiddenToken(self.input.get(idx))
@@ -399,7 +398,7 @@ class DebugTreeAdaptor(TreeAdaptor):

class DebugEventListener(object):
"""All debugging events that a recognizer can trigger.
-
+
I did not create a separate AST debugging interface as it would create
lots of extra classes and DebugParser has a dbg var defined, which makes
it hard to change to ASTDebugEventListener.  I looked hard at this issue
@@ -449,7 +448,7 @@ class DebugEventListener(object):
pass


-    def enterDecision(self, decisionNumber):
+    def enterDecision(self, decisionNumber, couldBacktrack):
"""Every decision, fixed k or arbitrary, has an enter/exit event
so that a GUI can easily track what LT/consume events are
associated with prediction.  You will see a single enter/exit
@@ -592,7 +591,7 @@ class DebugEventListener(object):
"""
pass

-
+
def endResync(self):
"""Indicates that the recognizer has finished consuming tokens in order
to resychronize.  There may be multiple beginResync/endResync pairs
@@ -940,7 +939,7 @@ class DebugEventSocketProxy(DebugEventListener):
self.transmit("exitSubRule\t%d" % decisionNumber)


-    def enterDecision(self, decisionNumber):
+    def enterDecision(self, decisionNumber, couldBacktrack):
self.transmit("enterDecision\t%d" % decisionNumber)


@@ -1061,7 +1060,7 @@ class DebugEventSocketProxy(DebugEventListener):
#         buf.append(tokenIndex);
#         serializeText(buf, text);

-
+
## A S T  E v e n t s

def nilNode(self, t):
diff --git a/runtime/Python/antlr3/recognizers.py b/runtime/Python/antlr3/recognizers.py
index 9ea82ec..f7c295d 100644
--- a/runtime/Python/antlr3/recognizers.py
+++ b/runtime/Python/antlr3/recognizers.py
@@ -41,7 +41,7 @@ from antlr3.exceptions import RecognitionException, MismatchedTokenException, \
NoViableAltException, EarlyExitException, MismatchedSetException, \
MismatchedNotSetException, FailedPredicateException, \
BacktrackingFailed, UnwantedTokenException, MissingTokenException
-from antlr3.tokens import CommonToken, EOF_TOKEN, SKIP_TOKEN
+from antlr3.tokens import CommonToken, SKIP_TOKEN
from antlr3.compat import set, frozenset, reversed


@@ -1079,6 +1079,15 @@ class Lexer(BaseRecognizer, TokenSource):
self._state.text = None


+    def makeEOFToken(self):
+        eof = CommonToken(
+            type=EOF, channel=DEFAULT_CHANNEL,
+            input=self.input,
+            start=self.input.index(), stop=self.input.index())
+        eof.line = self.input.line
+        eof.charPositionInLine = self.input.charPositionInLine
+        return eof
+
def nextToken(self):
"""
Return a token from this source; i.e., match a token on the char
@@ -1093,7 +1102,7 @@ class Lexer(BaseRecognizer, TokenSource):
self._state.tokenStartLine = self.input.line
self._state.text = None
if self.input.LA(1) == EOF:
-                return EOF_TOKEN
+                return self.makeEOFToken()

try:
self.mTokens()
@@ -1355,7 +1364,7 @@ class Parser(BaseRecognizer):
def __init__(self, lexer, state=None):
BaseRecognizer.__init__(self, state)

-        self.setTokenStream(lexer)
+        self.input = lexer


def reset(self):
diff --git a/runtime/Python/antlr3/streams.py b/runtime/Python/antlr3/streams.py
index 0dbe0f1..ce1929a 100644
--- a/runtime/Python/antlr3/streams.py
+++ b/runtime/Python/antlr3/streams.py
@@ -34,7 +34,7 @@ import codecs
from StringIO import StringIO

from antlr3.constants import DEFAULT_CHANNEL, EOF
-from antlr3.tokens import Token, EOF_TOKEN
+from antlr3.tokens import Token, CommonToken


############################################################################
@@ -58,7 +58,7 @@ class IntStream(object):

def consume(self):
raise NotImplementedError
-
+

def LA(self, i):
"""Get int at current input pointer + i ahead where i=1 is next int.
@@ -67,9 +67,9 @@ class IntStream(object):
just matched).  LA(-i) where i is before first token should
yield -1, invalid char / EOF.
"""
-
+
raise NotImplementedError
-
+

def mark(self):
"""
@@ -112,7 +112,7 @@ class IntStream(object):
Do not "pop" the marker off the state.  mark(i)
and rewind(i) should balance still. It is
like invoking rewind(last marker) but it should not "pop"
-        the marker off.  It's like seek(last marker's input position).
+        the marker off.  It's like seek(last marker's input position).
"""

raise NotImplementedError
@@ -153,7 +153,7 @@ class IntStream(object):

The index is 0..n-1.  A seek to position i means that LA(1) will
return the ith symbol.  So, seeking to 0 means LA(1) will return the
-        first element in the stream.
+        first element in the stream.
"""

raise NotImplementedError
@@ -184,12 +184,12 @@ class CharStream(IntStream):
@brief A source of characters for an ANTLR lexer.

This is an abstract class that must be implemented by a subclass.
-
+
"""

# pylint does not realize that this is an interface, too
#pylint: disable-msg=W0223
-
+
EOF = -1


@@ -201,8 +201,8 @@ class CharStream(IntStream):
"""

raise NotImplementedError
-
-
+
+
def LT(self, i):
"""
Get the ith character of lookahead.  This is the same usually as
@@ -246,12 +246,12 @@ class TokenStream(IntStream):
@brief A stream of tokens accessing tokens from a TokenSource

This is an abstract class that must be implemented by a subclass.
-
+
"""
-
+
# pylint does not realize that this is an interface, too
#pylint: disable-msg=W0223
-
+
def LT(self, k):
"""
Get Token at current input pointer + i ahead where i=1 is next Token.
@@ -264,6 +264,15 @@ class TokenStream(IntStream):
raise NotImplementedError


+    def range(self):
+        """
+        How far ahead has the stream been asked to look?  The return
+        value is a valid index from 0..n-1.
+        """
+
+        raise NotImplementedError
+
+
def get(self, i):
"""
Get a token at an absolute index i; 0..n-1.  This is really only
@@ -302,7 +311,7 @@ class TokenStream(IntStream):

raise NotImplementedError

-
+
############################################################################
#
# character streams for use in lexers
@@ -315,27 +324,27 @@ class TokenStream(IntStream):
class ANTLRStringStream(CharStream):
"""
@brief CharStream that pull data from a unicode string.
-
+
A pretty quick CharStream that pulls all data from an array
directly.  Every method call counts in the lexer.

"""

-
+
def __init__(self, data):
"""
@param data This should be a unicode string holding the data you want
to parse. If you pass in a byte string, the Lexer will choke on
non-ascii data.
-
+
"""
-
+
CharStream.__init__(self)
-
+
# The data being scanned
self.strdata = unicode(data)
self.data = [ord(c) for c in self.strdata]
-
+
# How many characters are actually in the buffer
self.n = len(data)

@@ -366,7 +375,7 @@ class ANTLRStringStream(CharStream):
when the object was created *except* the data array is not
touched.
"""
-
+
self.p = 0
self.line = 1
self.charPositionInLine = 0
@@ -382,7 +391,7 @@ class ANTLRStringStream(CharStream):
self.charPositionInLine += 1

self.p += 1
-
+
except IndexError:
# happend when we reached EOF and self.data[self.p] fails
# just do nothing
@@ -423,7 +432,7 @@ class ANTLRStringStream(CharStream):
last symbol has been read.  The index is the index of char to
be returned from LA(1).
"""
-
+
return self.p


@@ -438,9 +447,9 @@ class ANTLRStringStream(CharStream):
except IndexError:
self._markers.append(state)
self.markDepth += 1
-
+
self.lastMarker = self.markDepth
-
+
return self.lastMarker


@@ -468,7 +477,7 @@ class ANTLRStringStream(CharStream):
consume() ahead until p==index; can't just set p=index as we must
update line and charPositionInLine.
"""
-
+
if index <= self.p:
self.p = index # just jump; don't update stream state (line, ...)
return
@@ -515,7 +524,7 @@ class ANTLRStringStream(CharStream):
class ANTLRFileStream(ANTLRStringStream):
"""
@brief CharStream that opens a file to read the data.
-
+
This is a char buffer stream that is loaded from a file
all at once when you construct the object.
"""
@@ -527,9 +536,9 @@ class ANTLRFileStream(ANTLRStringStream):

@param encoding If you set the optional encoding argument, then the
data will be decoded on the fly.
-
+
"""
-
+
self.fileName = fileName

fp = codecs.open(fileName, 'rb', encoding)
@@ -537,13 +546,13 @@ class ANTLRFileStream(ANTLRStringStream):
data = fp.read()
finally:
fp.close()
-
+
ANTLRStringStream.__init__(self, data)


def getSourceName(self):
"""Deprecated, access o.fileName directly."""
-
+
return self.fileName


@@ -553,7 +562,7 @@ class ANTLRInputStream(ANTLRStringStream):

This is a char buffer stream that is loaded from a file like object
all at once when you construct the object.
-
+
All input is consumed from the file, but it is not closed.
"""

@@ -564,16 +573,16 @@ class ANTLRInputStream(ANTLRStringStream):

@param encoding If you set the optional encoding argument, then the
data will be decoded on the fly.
-
+
"""
-
+
if encoding is not None:
# wrap input in a decoding reader
reader = codecs.lookup(encoding)[2]
file = reader(file)

data = file.read()
-
+
ANTLRStringStream.__init__(self, data)


@@ -598,7 +607,7 @@ InputStream = ANTLRInputStream
class CommonTokenStream(TokenStream):
"""
@brief The most common stream of tokens
-
+
The most common stream of tokens is one where every token is buffered up
and tokens are prefiltered for a certain channel (the parser will only
see these tokens and cannot change the filter channel number during the
@@ -612,11 +621,11 @@ class CommonTokenStream(TokenStream):

@param channel Skip tokens on any channel but this one; this is how we
skip whitespace...
-
+
"""
-
+
TokenStream.__init__(self)
-
+
self.tokenSource = tokenSource

# Record every single token pulled from the source so we can reproduce
@@ -629,7 +638,8 @@ class CommonTokenStream(TokenStream):
# Set<tokentype>; discard any tokens with this type
self.discardSet = set()

-	# Skip tokens on any channel but this one; this is how we skip whitespace...
+	# Skip tokens on any channel but this one; this is how we skip
+        # whitespace...
self.channel = channel

# By default, track all incoming tokens
@@ -641,11 +651,18 @@ class CommonTokenStream(TokenStream):

# Remember last marked position
self.lastMarker = None
-
+
+        # how deep have we gone?
+        self._range = -1
+
+
+    def makeEOFToken(self):
+        return self.tokenSource.makeEOFToken()
+

def setTokenSource(self, tokenSource):
"""Reset this token stream by setting its token source."""
-
+
self.tokenSource = tokenSource
self.tokens = []
self.p = -1
@@ -663,13 +680,13 @@ class CommonTokenStream(TokenStream):
This is done upon first LT request because you might want to
set some token type / channel overrides before filling buffer.
"""
-
+

index = 0
t = self.tokenSource.nextToken()
while t is not None and t.type != EOF:
discard = False
-
+
if self.discardSet is not None and t.type in self.discardSet:
discard = True

@@ -679,24 +696,24 @@ class CommonTokenStream(TokenStream):
# is there a channel override for token type?
try:
overrideChannel = self.channelOverrideMap[t.type]
-
+
except KeyError:
# no override for this type
pass
-
+
else:
if overrideChannel == self.channel:
t.channel = overrideChannel
else:
discard = True
-
+
if not discard:
t.index = index
self.tokens.append(t)
index += 1

t = self.tokenSource.nextToken()
-
+
# leave p pointing at first token on channel
self.p = 0
self.p = self.skipOffTokenChannels(self.p)
@@ -711,7 +728,7 @@ class CommonTokenStream(TokenStream):

Walk past any token not on the channel the parser is listening to.
"""
-
+
if self.p < len(self.tokens):
self.p += 1

@@ -730,7 +747,7 @@ class CommonTokenStream(TokenStream):
except IndexError:
# hit the end of token stream
pass
-
+
return i


@@ -749,7 +766,7 @@ class CommonTokenStream(TokenStream):
the stream to force all WS and NEWLINE to be a different, ignored
channel.
"""
-
+
self.channelOverrideMap[ttype] = channel


@@ -769,7 +786,7 @@ class CommonTokenStream(TokenStream):

if stop is None or stop >= len(self.tokens):
stop = len(self.tokens) - 1
-
+
if start is None or stop < 0:
start = 0

@@ -779,7 +796,7 @@ class CommonTokenStream(TokenStream):
if isinstance(types, (int, long)):
# called with a single type, wrap into set
types = set([types])
-
+
filteredTokens = [
token for token in self.tokens[start:stop]
if types is None or token.type in types
@@ -805,7 +822,7 @@ class CommonTokenStream(TokenStream):

if k < 0:
return self.LB(-k)
-
+
i = self.p
n = 1
# find k good tokens
@@ -814,10 +831,13 @@ class CommonTokenStream(TokenStream):
i = self.skipOffTokenChannels(i+1) # leave p on valid token
n += 1

+        if i > self._range:
+            self._range = i
+
try:
return self.tokens[i]
except IndexError:
-            return EOF_TOKEN
+            return self.makeEOFToken()


def LB(self, k):
@@ -842,7 +862,7 @@ class CommonTokenStream(TokenStream):

if i < 0:
return None
-
+
return self.tokens[i]


@@ -855,6 +875,16 @@ class CommonTokenStream(TokenStream):
return self.tokens[i]


+    def slice(self, start, stop):
+        if self.p == -1:
+            self.fillBuffer()
+
+        if start < 0 or stop < 0:
+            return None
+
+        return self.tokens[start:stop+1]
+
+
def LA(self, i):
return self.LT(i).type

@@ -862,17 +892,21 @@ class CommonTokenStream(TokenStream):
def mark(self):
self.lastMarker = self.index()
return self.lastMarker
-
+

def release(self, marker=None):
# no resources to release
pass
-
+

def size(self):
return len(self.tokens)


+    def range(self):
+        return self._range
+
+
def index(self):
return self.p

@@ -880,7 +914,7 @@ class CommonTokenStream(TokenStream):
def rewind(self, marker=None):
if marker is None:
marker = self.lastMarker
-
+
self.seek(marker)


@@ -909,7 +943,7 @@ class CommonTokenStream(TokenStream):
stop = len(self.tokens) - 1
elif not isinstance(stop, int):
stop = stop.index
-
+
if stop >= len(self.tokens):
stop = len(self.tokens) - 1

@@ -918,7 +952,7 @@ class CommonTokenStream(TokenStream):

class RewriteOperation(object):
"""@brief Internal helper class."""
-
+
def __init__(self, stream, index, text):
self.stream = stream
self.index = index
@@ -944,14 +978,15 @@ class InsertBeforeOp(RewriteOperation):

def execute(self, buf):
buf.write(self.text)
-        buf.write(self.stream.tokens[self.index].text)
+        if self.stream.tokens[self.index].type != EOF:
+            buf.write(self.stream.tokens[self.index].text)
return self.index + 1


class ReplaceOp(RewriteOperation):
"""
@brief Internal helper class.
-
+
I'm going to try replacing range from x..y with (y-x)+1 ReplaceOp
instructions.
"""
@@ -1046,7 +1081,7 @@ class TokenRewriteStream(CommonTokenStream):
If you don't use named rewrite streams, a "default" stream is used as
the first example shows.
"""
-
+
DEFAULT_PROGRAM_NAME = "default"
MIN_TOKEN_INDEX = 0

@@ -1058,10 +1093,10 @@ class TokenRewriteStream(CommonTokenStream):
#  Maps String (name) -> rewrite (List)
self.programs = {}
self.programs[self.DEFAULT_PROGRAM_NAME] = []
-
+
# Map String (program name) -> Integer index
self.lastRewriteTokenIndexes = {}
-
+

def rollback(self, *args):
"""
@@ -1078,7 +1113,7 @@ class TokenRewriteStream(CommonTokenStream):
instructionIndex = args[0]
else:
raise TypeError("Invalid arguments")
-
+
p = self.programs.get(programName, None)
if p is not None:
self.programs[programName] = (
@@ -1087,7 +1122,7 @@ class TokenRewriteStream(CommonTokenStream):

def deleteProgram(self, programName=DEFAULT_PROGRAM_NAME):
"""Reset the program so that no instructions exist"""
-
+
self.rollback(programName, self.MIN_TOKEN_INDEX)


@@ -1096,7 +1131,7 @@ class TokenRewriteStream(CommonTokenStream):
programName = self.DEFAULT_PROGRAM_NAME
index = args[0]
text = args[1]
-
+
elif len(args) == 3:
programName = args[0]
index = args[1]
@@ -1118,7 +1153,7 @@ class TokenRewriteStream(CommonTokenStream):
programName = self.DEFAULT_PROGRAM_NAME
index = args[0]
text = args[1]
-
+
elif len(args) == 3:
programName = args[0]
index = args[1]
@@ -1142,13 +1177,13 @@ class TokenRewriteStream(CommonTokenStream):
first = args[0]
last = args[0]
text = args[1]
-
+
elif len(args) == 3:
programName = self.DEFAULT_PROGRAM_NAME
first = args[0]
last = args[1]
text = args[2]
-
+
elif len(args) == 4:
programName = args[0]
first = args[1]
@@ -1174,7 +1209,7 @@ class TokenRewriteStream(CommonTokenStream):
op = ReplaceOp(self, first, last, text)
rewrites = self.getProgram(programName)
rewrites.append(op)
-
+

def delete(self, *args):
self.replace(*(list(args) + [None]))
@@ -1203,26 +1238,33 @@ class TokenRewriteStream(CommonTokenStream):


def toOriginalString(self, start=None, end=None):
+        if self.p == -1:
+            self.fillBuffer()
+
if start is None:
start = self.MIN_TOKEN_INDEX
if end is None:
end = self.size() - 1
-
+
buf = StringIO()
i = start
while i >= self.MIN_TOKEN_INDEX and i <= end and i < len(self.tokens):
-            buf.write(self.get(i).text)
+            if self.get(i).type != EOF:
+                buf.write(self.get(i).text)
i += 1

return buf.getvalue()


def toString(self, *args):
+        if self.p == -1:
+            self.fillBuffer()
+
if len(args) == 0:
programName = self.DEFAULT_PROGRAM_NAME
start = self.MIN_TOKEN_INDEX
end = self.size() - 1
-
+
elif len(args) == 1:
programName = args[0]
start = self.MIN_TOKEN_INDEX
@@ -1232,7 +1274,7 @@ class TokenRewriteStream(CommonTokenStream):
programName = self.DEFAULT_PROGRAM_NAME
start = args[0]
end = args[1]
-
+
if start is None:
start = self.MIN_TOKEN_INDEX
elif not isinstance(start, int):
@@ -1254,7 +1296,7 @@ class TokenRewriteStream(CommonTokenStream):
if rewrites is None or len(rewrites) == 0:
# no instructions to execute
return self.toOriginalString(start, end)
-
+
buf = StringIO()

# First, optimize instruction stream
@@ -1273,7 +1315,8 @@ class TokenRewriteStream(CommonTokenStream):
t = self.tokens[i]
if op is None:
# no operation at that index, just dump token
-                buf.write(t.text)
+                if t.type != EOF:
+                    buf.write(t.text)
i += 1 # move to next token

else:
@@ -1342,7 +1385,7 @@ class TokenRewriteStream(CommonTokenStream):

Return a map from token index to operation.
"""
-
+
# WALK REPLACES
for i, rop in enumerate(rewrites):
if rop is None:
@@ -1401,7 +1444,7 @@ class TokenRewriteStream(CommonTokenStream):
raise ValueError(
"insert op %s within boundaries of previous %s"
% (iop, rop))
-
+
m = {}
for i, op in enumerate(rewrites):
if op is None:
diff --git a/runtime/Python/antlr3/tokens.py b/runtime/Python/antlr3/tokens.py
index 340ac8a..d3f39b8 100644
--- a/runtime/Python/antlr3/tokens.py
+++ b/runtime/Python/antlr3/tokens.py
@@ -47,7 +47,7 @@ class Token(object):
Using setter/getter methods is deprecated. Use o.text instead.
"""
raise NotImplementedError
-
+
def setText(self, text):
"""@brief Set the text of the token.

@@ -62,48 +62,48 @@ class Token(object):
Using setter/getter methods is deprecated. Use o.type instead."""

raise NotImplementedError
-
+
def setType(self, ttype):
"""@brief Get the type of the token.

Using setter/getter methods is deprecated. Use o.type instead."""

raise NotImplementedError
-
-
+
+
def getLine(self):
"""@brief Get the line number on which this token was matched

Lines are numbered 1..n
-
+
Using setter/getter methods is deprecated. Use o.line instead."""

raise NotImplementedError
-
+
def setLine(self, line):
"""@brief Set the line number on which this token was matched

Using setter/getter methods is deprecated. Use o.line instead."""

raise NotImplementedError
-
-
+
+
def getCharPositionInLine(self):
"""@brief Get the column of the tokens first character,
-
+
Columns are numbered 0..n-1
-
+
Using setter/getter methods is deprecated. Use o.charPositionInLine instead."""

raise NotImplementedError
-
+
def setCharPositionInLine(self, pos):
"""@brief Set the column of the tokens first character,

Using setter/getter methods is deprecated. Use o.charPositionInLine instead."""

raise NotImplementedError
-
+

def getChannel(self):
"""@brief Get the channel of the token
@@ -111,25 +111,25 @@ class Token(object):
Using setter/getter methods is deprecated. Use o.channel instead."""

raise NotImplementedError
-
+
def setChannel(self, channel):
"""@brief Set the channel of the token

Using setter/getter methods is deprecated. Use o.channel instead."""

raise NotImplementedError
-
+

def getTokenIndex(self):
"""@brief Get the index in the input stream.

An index from 0..n-1 of the token object in the input stream.
This must be valid in order to use the ANTLRWorks debugger.
-
+
Using setter/getter methods is deprecated. Use o.index instead."""

raise NotImplementedError
-
+
def setTokenIndex(self, index):
"""@brief Set the index in the input stream.

@@ -173,11 +173,11 @@ class CommonToken(Token):
unnecessary copy operations.

"""
-
+
def __init__(self, type=None, channel=DEFAULT_CHANNEL, text=None,
input=None, start=None, stop=None, oldToken=None):
Token.__init__(self)
-
+
if oldToken is not None:
self.type = oldToken.type
self.line = oldToken.line
@@ -185,21 +185,21 @@ class CommonToken(Token):
self.channel = oldToken.channel
self.index = oldToken.index
self._text = oldToken._text
+            self.input = oldToken.input
if isinstance(oldToken, CommonToken):
-                self.input = oldToken.input
self.start = oldToken.start
self.stop = oldToken.stop
-
+
else:
self.type = type
self.input = input
self.charPositionInLine = -1 # set to invalid position
self.line = 0
self.channel = channel
-
+
#What token number is this from 0..n-1 tokens; < 0 implies invalid index
self.index = -1
-
+
# We need to be able to change the text once in a while.  If
# this is non-null, then getText should return this.  Note that
# start/stop are not affected by changing this.
@@ -219,8 +219,11 @@ class CommonToken(Token):

if self.input is None:
return None
-
-        return self.input.substring(self.start, self.stop)
+
+        if self.start < self.input.size() and self.stop < self.input.size():
+          return self.input.substring(self.start, self.stop)
+
+        return '<EOF>'


def setText(self, text):
@@ -248,28 +251,28 @@ class CommonToken(Token):

def getLine(self):
return self.line
-
+
def setLine(self, line):
self.line = line


def getCharPositionInLine(self):
return self.charPositionInLine
-
+
def setCharPositionInLine(self, pos):
self.charPositionInLine = pos


def getChannel(self):
return self.channel
-
+
def setChannel(self, channel):
self.channel = channel
-
+

def getTokenIndex(self):
return self.index
-
+
def setTokenIndex(self, index):
self.index = index

@@ -304,11 +307,11 @@ class CommonToken(Token):
self.typeName, channelStr,
self.line, self.charPositionInLine
)
-
+

class ClassicToken(Token):
"""@brief Alternative token implementation.
-
+
A Token object like we'd use in ANTLR 2.x; has an actual string created
and associated with this object.  These objects are needed for imaginary
tree nodes that have payload objects.  We need to create a Token object
@@ -321,14 +324,14 @@ class ClassicToken(Token):
oldToken=None
):
Token.__init__(self)
-
+
if oldToken is not None:
self.text = oldToken.text
self.type = oldToken.type
self.line = oldToken.line
self.charPositionInLine = oldToken.charPositionInLine
self.channel = oldToken.channel
-
+
self.text = text
self.type = type
self.line = None
@@ -345,36 +348,36 @@ class ClassicToken(Token):


def getType(self):
-        return self.type
+        return self.type

def setType(self, ttype):
self.type = ttype

-
+
def getLine(self):
return self.line
-
+
def setLine(self, line):
self.line = line


def getCharPositionInLine(self):
return self.charPositionInLine
-
+
def setCharPositionInLine(self, pos):
self.charPositionInLine = pos


def getChannel(self):
return self.channel
-
+
def setChannel(self, channel):
self.channel = channel
-
+

def getTokenIndex(self):
return self.index
-
+
def setTokenIndex(self, index):
self.index = index

@@ -390,7 +393,7 @@ class ClassicToken(Token):
channelStr = ""
if self.channel > 0:
channelStr = ",channel=" + str(self.channel)
-
+
txt = self.text
if txt is None:
txt = "<no text>"
@@ -402,15 +405,12 @@ class ClassicToken(Token):
self.line,
self.charPositionInLine
)
-
+

__str__ = toString
__repr__ = toString


-
-EOF_TOKEN = CommonToken(type=EOF)
-
INVALID_TOKEN = CommonToken(type=INVALID_TOKEN_TYPE)

# In an action, a lexer rule can set token to this SKIP_TOKEN and ANTLR
diff --git a/runtime/Python/antlr3/tree.py b/runtime/Python/antlr3/tree.py
index a85a317..86c796f 100644
--- a/runtime/Python/antlr3/tree.py
+++ b/runtime/Python/antlr3/tree.py
@@ -1106,6 +1106,9 @@ class BaseTreeAdaptor(TreeAdaptor):


def createFromToken(self, tokenType, fromToken, text=None):
+        if fromToken is None:
+            return self.createFromType(tokenType, text)
+
assert isinstance(tokenType, (int, long)), type(tokenType).__name__
assert isinstance(fromToken, Token), type(fromToken).__name__
assert text is None or isinstance(text, basestring), type(text).__name__
@@ -1856,6 +1859,10 @@ class CommonTreeNodeStream(TreeNodeStream):
self.calls = []


+    def __iter__(self):
+        return TreeIterator(self.root, self.adaptor)
+
+
def fillBuffer(self):
"""Walk tree with depth-first-search and fill nodes buffer.
Don't do DOWN, UP nodes if its a list (t is isNil).
@@ -1965,6 +1972,10 @@ class CommonTreeNodeStream(TreeNodeStream):
return self.nodes[self.p - k]


+    def isEOF(self, obj):
+        return self.adaptor.getType(obj) == EOF
+
+
def getTreeSource(self):
return self.root

@@ -2207,7 +2218,9 @@ class TreeParser(BaseRecognizer):

def getMissingSymbol(self, input, e, expectedTokenType, follow):
tokenText = "<missing " + self.tokenNames[expectedTokenType] + ">"
-        return CommonTree(CommonToken(type=expectedTokenType, text=tokenText))
+        adaptor = input.adaptor
+        return adaptor.createToken(
+            CommonToken(type=expectedTokenType, text=tokenText))


# precompiled regex used by inContext
@@ -2414,9 +2427,11 @@ class TreeVisitor(object):
# if rewritten, walk children of new t
t = pre_action(t)

-        for idx in xrange(self.adaptor.getChildCount(t)):
+        idx = 0
+        while idx < self.adaptor.getChildCount(t):
child = self.adaptor.getChild(t, idx)
self.visit(child, pre_action, post_action)
+            idx += 1

if post_action is not None and not isNil:
t = post_action(t)
@@ -2775,7 +2790,8 @@ class RewriteRuleSubtreeStream(RewriteRuleElementStream):

# test size above then fetch
el = self._next()
-        return el
+        # dup just the root (want node here)
+        return self.adaptor.dupNode(el)


def dup(self, el):
diff --git a/runtime/Python/tests/t051treeRewriteAST.py b/runtime/Python/tests/t051treeRewriteAST.py
index b27c4f5..39253b4 100644
--- a/runtime/Python/tests/t051treeRewriteAST.py
+++ b/runtime/Python/tests/t051treeRewriteAST.py
@@ -22,9 +22,9 @@ class T(testbase.ANTLRTest):
def recover(self, input, re):
# no error recovery yet, just crash!
raise
-
+
return TWalker
-
+

def execTreeParser(self, grammar, grammarEntry, treeGrammar, treeEntry, input):
lexerCls, parserCls = self.compileInlineGrammar(grammar)
@@ -44,7 +44,7 @@ class T(testbase.ANTLRTest):
return r.tree.toStringTree()

return ""
-
+

def testFlatList(self):
grammar = textwrap.dedent(
@@ -59,7 +59,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP1;
@@ -69,7 +69,7 @@ class T(testbase.ANTLRTest):
ASTLabelType=CommonTree;
tokenVocab=T1;
}
-
+
a : ID INT -> INT ID;
''')

@@ -95,7 +95,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP2;
@@ -130,7 +130,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP3;
@@ -174,7 +174,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP4;
@@ -209,7 +209,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP5;
@@ -244,7 +244,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP6;
@@ -279,7 +279,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP7;
@@ -318,7 +318,7 @@ class T(testbase.ANTLRTest):
r'''
tree grammar TP;
options {language=Python;output=AST; ASTLabelType=CommonTree; tokenVocab=T;}
-            a : ID .
+            a : ID .
;
''')

@@ -332,29 +332,29 @@ class T(testbase.ANTLRTest):
#     def testNoWildcardAsRootError(self):
#         ErrorQueue equeue = new ErrorQueue();
#         ErrorManager.setErrorListener(equeue);
-# >
+# >
#         String treeGrammar =
#             "tree grammar TP;\n"+
#             "options {language=Python;output=AST;}
-#             "a : ^(. INT)
+#             "a : ^(. INT)
#             "  ;\n";
-# >
+# >
#         Grammar g = new Grammar(treeGrammar);
#         Tool antlr = newTool();
#         antlr.setOutputDirectory(null); // write to /dev/null
#         CodeGenerator generator = new CodeGenerator(antlr, g, "Java");
#         g.setCodeGenerator(generator);
#         generator.genRecognizer();
-# >
+# >
#         assertEquals("unexpected errors: "+equeue, 1, equeue.errors.size());
-# >
+# >
#         int expectedMsgID = ErrorManager.MSG_WILDCARD_AS_ROOT;
#         Object expectedArg = null;
#         antlr.RecognitionException expectedExc = null;
#         GrammarSyntaxMessage expectedMessage =
#             new GrammarSyntaxMessage(expectedMsgID, g, null, expectedArg, expectedExc);
-# >
-#         checkError(equeue, expectedMessage);
+# >
+#         checkError(equeue, expectedMessage);
#     }

def testAutoWildcard2(self):
@@ -372,7 +372,7 @@ class T(testbase.ANTLRTest):
r'''
tree grammar TP;
options {language=Python;output=AST; ASTLabelType=CommonTree; tokenVocab=T;}
-            a : ^(ID .)
+            a : ^(ID .)
;
''')

@@ -393,12 +393,12 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP;
options {language=Python;output=AST; ASTLabelType=CommonTree; tokenVocab=T;}
-            a : ID c=.
+            a : ID c=.
;
''')

@@ -424,7 +424,7 @@ class T(testbase.ANTLRTest):
r'''
tree grammar TP;
options {language=Python;output=AST; ASTLabelType=CommonTree; tokenVocab=T;}
-            a : ID c+=.
+            a : ID c+=.
;
''')

@@ -448,7 +448,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP8;
@@ -484,7 +484,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP9;
@@ -520,7 +520,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP10;
@@ -556,7 +556,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP11;
@@ -592,7 +592,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP12;
@@ -628,7 +628,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP13;
@@ -664,7 +664,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP14;
@@ -701,7 +701,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP15;
@@ -737,7 +737,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP16;
@@ -747,7 +747,7 @@ class T(testbase.ANTLRTest):
ASTLabelType=CommonTree;
tokenVocab=T16;
}
-        a : ID ->
+        a : ID ->
;
''')

@@ -785,7 +785,7 @@ class T(testbase.ANTLRTest):
a : b INT;
b : ID | INT;
''')
-
+
found = self.execTreeParser(
grammar, 'a',
treeGrammar, 'a',
@@ -794,7 +794,7 @@ class T(testbase.ANTLRTest):

self.failUnlessEqual("abc 34", found)

-
+
def testSetOptionalMatchNoRewrite(self):
grammar = textwrap.dedent(
r'''
@@ -825,7 +825,7 @@ class T(testbase.ANTLRTest):
grammar, 'a',
treeGrammar, 'a',
"abc 34")
-
+
self.failUnlessEqual("abc 34", found)


@@ -854,7 +854,7 @@ class T(testbase.ANTLRTest):
}
a : ^(ID (ID | INT) ) ;
''')
-
+
found = self.execTreeParser(
grammar, 'a',
treeGrammar, 'a',
@@ -889,7 +889,7 @@ class T(testbase.ANTLRTest):
}
a : ^((ID | INT) INT) ;
''')
-
+
found = self.execTreeParser(
grammar, 'a',
treeGrammar, 'a',
@@ -914,7 +914,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP17;
@@ -961,7 +961,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
''')
-
+
treeGrammar = textwrap.dedent(
r'''
tree grammar TP18;
@@ -976,7 +976,7 @@ class T(testbase.ANTLRTest):
a : INT -> INT["1"]
;
''')
-
+
found = self.execTreeParser(
grammar, 'a',
treeGrammar, 's',
@@ -1167,7 +1167,7 @@ class T(testbase.ANTLRTest):
INT : '0'..'9'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
""")
-
+
treeGrammar = textwrap.dedent(
r"""
tree grammar TP;
@@ -1553,5 +1553,41 @@ class T(testbase.ANTLRTest):
self.assertEquals("(2 3) (2 3)", found)


+    def testRuleResultAsRoot(self):
+        grammar = textwrap.dedent(
+            r'''
+            grammar T;
+            options {
+                language=Python;
+                output=AST;
+            }
+            a : ID '=' INT -> ^('=' ID INT);
+            ID : 'a'..'z'+ ;
+            INT : '0'..'9'+;
+            COLON : ':' ;
+            WS : (' '|'\n') {$channel=HIDDEN;} ;
+            ''')
+
+        treeGrammar = textwrap.dedent(
+            r'''
+            tree grammar TP;
+            options {
+                language=Python;
+                output=AST;
+                rewrite=true;
+                ASTLabelType=CommonTree;
+                tokenVocab=T;
+            }
+            a : ^(eq e1=ID e2=.) -> ^(eq $e2 $e1) ;
+            eq : '=' | ':' {pass} ;  // bug in set match, doesn't add to tree!! booh. force nonset.
+            ''')
+
+        found = self.execTreeParser(
+            grammar, 'a',
+            treeGrammar, 'a',
+            "abc = 34")
+        self.assertEquals("(= 34 abc)", found)
+
+
if __name__ == '__main__':
unittest.main()
diff --git a/runtime/Python/tests/t052import.py b/runtime/Python/tests/t052import.py
index 7d7a183..8924462 100644
--- a/runtime/Python/tests/t052import.py
+++ b/runtime/Python/tests/t052import.py
@@ -38,9 +38,9 @@ class T(testbase.ANTLRTest):
def recover(self, input, re):
# no error recovery yet, just crash!
raise
-
+
return TParser
-
+

def lexerClass(self, base):
class TLexer(base):
@@ -65,9 +65,9 @@ class T(testbase.ANTLRTest):
def recover(self, input):
# no error recovery yet, just crash!
raise
-
+
return TLexer
-
+

def execParser(self, grammar, grammarEntry, slaves, input):
for slave in slaves:
@@ -78,7 +78,7 @@ class T(testbase.ANTLRTest):
del sys.modules[parserName+'Parser']
except KeyError:
pass
-
+
lexerCls, parserCls = self.compileInlineGrammar(grammar)

cStream = antlr3.StringStream(input)
@@ -88,7 +88,7 @@ class T(testbase.ANTLRTest):
getattr(parser, grammarEntry)()

return parser._output
-
+

def execLexer(self, grammar, slaves, input):
for slave in slaves:
@@ -99,7 +99,7 @@ class T(testbase.ANTLRTest):
del sys.modules[parserName+'Parser']
except KeyError:
pass
-
+
lexerCls = self.compileInlineGrammar(grammar)

cStream = antlr3.StringStream(input)
@@ -111,9 +111,20 @@ class T(testbase.ANTLRTest):
break

lexer._output += token.text
-
+
return lexer._output
-
+
+
+    # @Test public void testWildcardStillWorks() throws Exception {
+    #     ErrorQueue equeue = new ErrorQueue();
+    #     ErrorManager.setErrorListener(equeue);
+    #     String grammar =
+    #     "parser grammar S;\n" +
+    #     "a : B . C ;\n"; // not qualified ID
+    #     Grammar g = new Grammar(grammar);
+    #     assertEquals("unexpected errors: "+equeue, 0, equeue.errors.size());
+    #     }
+

def testDelegatorInvokesDelegateRule(self):
slave = textwrap.dedent(
@@ -127,7 +138,7 @@ class T(testbase.ANTLRTest):
self.gM1.capture(t)

}
-
+
a : B { self.capture("S.a") } ;
''')

@@ -152,6 +163,27 @@ class T(testbase.ANTLRTest):
self.failUnlessEqual("S.a", found)


+        # @Test public void testDelegatorInvokesDelegateRuleWithReturnStruct() throws Exception {
+        #     // must generate something like:
+        #          // public int a(int x) throws RecognitionException { return gS.a(x); }
+        #        // in M.
+        #        String slave =
+        #        "parser grammar S;\n" +
+        #        "a : B {System.out.print(\"S.a\");} ;\n";
+        #        mkdir(tmpdir);
+        #        writeFile(tmpdir, "S.g", slave);
+        #        String master =
+        #        "grammar M;\n" +
+        #        "import S;\n" +
+        #        "s : a {System.out.println($a.text);} ;\n" +
+        #        "B : 'b' ;" + // defines B from inherited token space
+        #        "WS : (' '|'\\n') {skip();} ;\n" ;
+        #        String found = execParser("M.g", master, "MParser", "MLexer",
+        #                                    "s", "b", debug);
+        #        assertEquals("S.ab\n", found);
+        #        }
+
+
def testDelegatorInvokesDelegateRuleWithArgs(self):
slave = textwrap.dedent(
r'''
@@ -325,6 +357,319 @@ class T(testbase.ANTLRTest):
self.failUnlessEqual("S.x T.y", found)


+        # @Test public void testDelegatesSeeSameTokenType2() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "parser grammar S;\n" + // A, B, C token type order
+        #                 "tokens { A; B; C; }\n" +
+        #                 "x : A {System.out.println(\"S.x\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+        #         String slave2 =
+        #                 "parser grammar T;\n" +
+        #                 "tokens { C; B; A; }\n" + // reverse order
+        #                 "y : A {System.out.println(\"T.y\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "T.g", slave2);
+
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S,T;\n" +
+        #                 "s : x y ;\n" + // matches AA, which should be "aa"
+        #                 "B : 'b' ;\n" + // another order: B, A, C
+        #                 "A : 'a' ;\n" +
+        #                 "C : 'c' ;\n" +
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         writeFile(tmpdir, "M.g", master);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+
+        #         String expectedTokenIDToTypeMap = "[A=4, B=5, C=6, WS=7]";
+        #         String expectedStringLiteralToTypeMap = "{}";
+        #         String expectedTypeToTokenList = "[A, B, C, WS]";
+
+        #         assertEquals(expectedTokenIDToTypeMap,
+        #                                  realElements(g.composite.tokenIDToTypeMap).toString());
+        #         assertEquals(expectedStringLiteralToTypeMap, g.composite.stringLiteralToTypeMap.toString());
+        #         assertEquals(expectedTypeToTokenList,
+        #                                  realElements(g.composite.typeToTokenList).toString());
+
+        #         assertEquals("unexpected errors: "+equeue, 0, equeue.errors.size());
+        # }
+
+        # @Test public void testCombinedImportsCombined() throws Exception {
+        #         // for now, we don't allow combined to import combined
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "grammar S;\n" + // A, B, C token type order
+        #                 "tokens { A; B; C; }\n" +
+        #                 "x : 'x' INT {System.out.println(\"S.x\");} ;\n" +
+        #                 "INT : '0'..'9'+ ;\n" +
+        #                 "WS : (' '|'\\n') {skip();} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S;\n" +
+        #                 "s : x INT ;\n";
+        #         writeFile(tmpdir, "M.g", master);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+
+        #         assertEquals("unexpected errors: "+equeue, 1, equeue.errors.size());
+        #         String expectedError = "error(161): "+tmpdir.toString().replaceFirst("\\-[0-9]+","")+"/M.g:2:8: combined grammar M cannot import combined grammar S";
+        #         assertEquals("unexpected errors: "+equeue, expectedError, equeue.errors.get(0).toString().replaceFirst("\\-[0-9]+",""));
+        # }
+
+        # @Test public void testSameStringTwoNames() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "parser grammar S;\n" +
+        #                 "tokens { A='a'; }\n" +
+        #                 "x : A {System.out.println(\"S.x\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+        #         String slave2 =
+        #                 "parser grammar T;\n" +
+        #                 "tokens { X='a'; }\n" +
+        #                 "y : X {System.out.println(\"T.y\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "T.g", slave2);
+
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S,T;\n" +
+        #                 "s : x y ;\n" +
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         writeFile(tmpdir, "M.g", master);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+
+        #         String expectedTokenIDToTypeMap = "[A=4, WS=6, X=5]";
+        #         String expectedStringLiteralToTypeMap = "{'a'=4}";
+        #         String expectedTypeToTokenList = "[A, X, WS]";
+
+        #         assertEquals(expectedTokenIDToTypeMap,
+        #                                  realElements(g.composite.tokenIDToTypeMap).toString());
+        #         assertEquals(expectedStringLiteralToTypeMap, g.composite.stringLiteralToTypeMap.toString());
+        #         assertEquals(expectedTypeToTokenList,
+        #                                  realElements(g.composite.typeToTokenList).toString());
+
+        #         Object expectedArg = "X='a'";
+        #         Object expectedArg2 = "A";
+        #         int expectedMsgID = ErrorManager.MSG_TOKEN_ALIAS_CONFLICT;
+        #         GrammarSemanticsMessage expectedMessage =
+        #                 new GrammarSemanticsMessage(expectedMsgID, g, null, expectedArg, expectedArg2);
+        #         checkGrammarSemanticsError(equeue, expectedMessage);
+
+        #         assertEquals("unexpected errors: "+equeue, 1, equeue.errors.size());
+
+        #         String expectedError =
+        #                 "error(158): T.g:2:10: cannot alias X='a'; string already assigned to A";
+        #         assertEquals(expectedError, equeue.errors.get(0).toString());
+        # }
+
+        # @Test public void testSameNameTwoStrings() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "parser grammar S;\n" +
+        #                 "tokens { A='a'; }\n" +
+        #                 "x : A {System.out.println(\"S.x\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+        #         String slave2 =
+        #                 "parser grammar T;\n" +
+        #                 "tokens { A='x'; }\n" +
+        #                 "y : A {System.out.println(\"T.y\");} ;\n";
+
+        #         writeFile(tmpdir, "T.g", slave2);
+
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S,T;\n" +
+        #                 "s : x y ;\n" +
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         writeFile(tmpdir, "M.g", master);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+
+        #         String expectedTokenIDToTypeMap = "[A=4, T__6=6, WS=5]";
+        #         String expectedStringLiteralToTypeMap = "{'a'=4, 'x'=6}";
+        #         String expectedTypeToTokenList = "[A, WS, T__6]";
+
+        #         assertEquals(expectedTokenIDToTypeMap,
+        #                                  realElements(g.composite.tokenIDToTypeMap).toString());
+        #         assertEquals(expectedStringLiteralToTypeMap, sortMapToString(g.composite.stringLiteralToTypeMap));
+        #         assertEquals(expectedTypeToTokenList,
+        #                                  realElements(g.composite.typeToTokenList).toString());
+
+        #         Object expectedArg = "A='x'";
+        #         Object expectedArg2 = "'a'";
+        #         int expectedMsgID = ErrorManager.MSG_TOKEN_ALIAS_REASSIGNMENT;
+        #         GrammarSemanticsMessage expectedMessage =
+        #                 new GrammarSemanticsMessage(expectedMsgID, g, null, expectedArg, expectedArg2);
+        #         checkGrammarSemanticsError(equeue, expectedMessage);
+
+        #         assertEquals("unexpected errors: "+equeue, 1, equeue.errors.size());
+
+        #         String expectedError =
+        #                 "error(159): T.g:2:10: cannot alias A='x'; token name already assigned to 'a'";
+        #         assertEquals(expectedError, equeue.errors.get(0).toString());
+        # }
+
+        # @Test public void testImportedTokenVocabIgnoredWithWarning() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "parser grammar S;\n" +
+        #                 "options {tokenVocab=whatever;}\n" +
+        #                 "tokens { A='a'; }\n" +
+        #                 "x : A {System.out.println(\"S.x\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S;\n" +
+        #                 "s : x ;\n" +
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         writeFile(tmpdir, "M.g", master);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+
+        #         Object expectedArg = "S";
+        #         int expectedMsgID = ErrorManager.MSG_TOKEN_VOCAB_IN_DELEGATE;
+        #         GrammarSemanticsMessage expectedMessage =
+        #                 new GrammarSemanticsMessage(expectedMsgID, g, null, expectedArg);
+        #         checkGrammarSemanticsWarning(equeue, expectedMessage);
+
+        #         assertEquals("unexpected errors: "+equeue, 0, equeue.errors.size());
+        #         assertEquals("unexpected errors: "+equeue, 1, equeue.warnings.size());
+
+        #         String expectedError =
+        #                 "warning(160): S.g:2:10: tokenVocab option ignored in imported grammar S";
+        #         assertEquals(expectedError, equeue.warnings.get(0).toString());
+        # }
+
+        # @Test public void testImportedTokenVocabWorksInRoot() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "parser grammar S;\n" +
+        #                 "tokens { A='a'; }\n" +
+        #                 "x : A {System.out.println(\"S.x\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+
+        #         String tokens =
+        #                 "A=99\n";
+        #         writeFile(tmpdir, "Test.tokens", tokens);
+
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "options {tokenVocab=Test;}\n" +
+        #                 "import S;\n" +
+        #                 "s : x ;\n" +
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         writeFile(tmpdir, "M.g", master);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+
+        #         String expectedTokenIDToTypeMap = "[A=99, WS=101]";
+        #         String expectedStringLiteralToTypeMap = "{'a'=100}";
+        #         String expectedTypeToTokenList = "[A, 'a', WS]";
+
+        #         assertEquals(expectedTokenIDToTypeMap,
+        #                                  realElements(g.composite.tokenIDToTypeMap).toString());
+        #         assertEquals(expectedStringLiteralToTypeMap, g.composite.stringLiteralToTypeMap.toString());
+        #         assertEquals(expectedTypeToTokenList,
+        #                                  realElements(g.composite.typeToTokenList).toString());
+
+        #         assertEquals("unexpected errors: "+equeue, 0, equeue.errors.size());
+        # }
+
+        # @Test public void testSyntaxErrorsInImportsNotThrownOut() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "parser grammar S;\n" +
+        #                 "options {toke\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S;\n" +
+        #                 "s : x ;\n" +
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         writeFile(tmpdir, "M.g", master);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+
+        #         // whole bunch of errors from bad S.g file
+        #         assertEquals("unexpected errors: "+equeue, 5, equeue.errors.size());
+        # }
+
+        # @Test public void testSyntaxErrorsInImportsNotThrownOut2() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "parser grammar S;\n" +
+        #                 ": A {System.out.println(\"S.x\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S;\n" +
+        #                 "s : x ;\n" +
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         writeFile(tmpdir, "M.g", master);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+
+        #         // whole bunch of errors from bad S.g file
+        #         assertEquals("unexpected errors: "+equeue, 3, equeue.errors.size());
+        # }
+
+
def testDelegatorRuleOverridesDelegate(self):
slave = textwrap.dedent(
r'''
@@ -360,6 +705,56 @@ class T(testbase.ANTLRTest):
self.failUnlessEqual("S.a", found)


+    #     @Test public void testDelegatorRuleOverridesLookaheadInDelegate() throws Exception {
+    #             String slave =
+    #                     "parser grammar JavaDecl;\n" +
+    #                     "type : 'int' ;\n" +
+    #                     "decl : type ID ';'\n" +
+    #                     "     | type ID init ';' {System.out.println(\"JavaDecl: \"+$decl.text);}\n" +
+    #                     "     ;\n" +
+    #                     "init : '=' INT ;\n" ;
+    #             mkdir(tmpdir);
+    #             writeFile(tmpdir, "JavaDecl.g", slave);
+    #             String master =
+    #                     "grammar Java;\n" +
+    #                     "import JavaDecl;\n" +
+    #                     "prog : decl ;\n" +
+    #                     "type : 'int' | 'float' ;\n" +
+    #                     "\n" +
+    #                     "ID  : 'a'..'z'+ ;\n" +
+    #                     "INT : '0'..'9'+ ;\n" +
+    #                     "WS : (' '|'\\n') {skip();} ;\n" ;
+    #             // for float to work in decl, type must be overridden
+    #             String found = execParser("Java.g", master, "JavaParser", "JavaLexer",
+    #                                                               "prog", "float x = 3;", debug);
+    #             assertEquals("JavaDecl: floatx=3;\n", found);
+    #     }
+
+    # @Test public void testDelegatorRuleOverridesDelegates() throws Exception {
+    #     String slave =
+    #         "parser grammar S;\n" +
+    #         "a : b {System.out.println(\"S.a\");} ;\n" +
+    #         "b : B ;\n" ;
+    #     mkdir(tmpdir);
+    #     writeFile(tmpdir, "S.g", slave);
+
+    #     String slave2 =
+    #         "parser grammar T;\n" +
+    #         "tokens { A='x'; }\n" +
+    #         "b : B {System.out.println(\"T.b\");} ;\n";
+    #     writeFile(tmpdir, "T.g", slave2);
+
+    #     String master =
+    #         "grammar M;\n" +
+    #         "import S, T;\n" +
+    #         "b : 'b'|'c' {System.out.println(\"M.b\");}|B|A ;\n" +
+    #         "WS : (' '|'\\n') {skip();} ;\n" ;
+    #     String found = execParser("M.g", master, "MParser", "MLexer",
+    #                               "a", "c", debug);
+    #     assertEquals("M.b\n" +
+    #                  "S.a\n", found);
+    # }
+
# LEXER INHERITANCE

def testLexerDelegatorInvokesDelegateRule(self):
@@ -430,6 +825,379 @@ class T(testbase.ANTLRTest):

self.failUnlessEqual("M.A a", found)

-
+        # @Test public void testLexerDelegatorRuleOverridesDelegateLeavingNoRules() throws Exception {
+        #         // M.Tokens has nothing to predict tokens from S.  Should
+        #         // not include S.Tokens alt in this case?
+        #         String slave =
+        #                 "lexer grammar S;\n" +
+        #                 "A : 'a' {System.out.println(\"S.A\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+        #         String master =
+        #                 "lexer grammar M;\n" +
+        #                 "import S;\n" +
+        #                 "A : 'a' {System.out.println(\"M.A\");} ;\n" +
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         writeFile(tmpdir, "/M.g", master);
+
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         composite.assignTokenTypes();
+        #         composite.defineGrammarSymbols();
+        #         composite.createNFAs();
+        #         g.createLookaheadDFAs(false);
+
+        #         // predict only alts from M not S
+        #         String expectingDFA =
+        #                 ".s0-'a'->.s1\n" +
+        #                 ".s0-{'\\n', ' '}->:s3=>2\n" +
+        #                 ".s1-<EOT>->:s2=>1\n";
+        #         org.antlr.analysis.DFA dfa = g.getLookaheadDFA(1);
+        #         FASerializer serializer = new FASerializer(g);
+        #         String result = serializer.serialize(dfa.startState);
+        #         assertEquals(expectingDFA, result);
+
+        #         // must not be a "unreachable alt: Tokens" error
+        #         assertEquals("unexpected errors: "+equeue, 0, equeue.errors.size());
+        # }
+
+        # @Test public void testInvalidImportMechanism() throws Exception {
+        #         // M.Tokens has nothing to predict tokens from S.  Should
+        #         // not include S.Tokens alt in this case?
+        #         String slave =
+        #                 "lexer grammar S;\n" +
+        #                 "A : 'a' {System.out.println(\"S.A\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+        #         String master =
+        #                 "tree grammar M;\n" +
+        #                 "import S;\n" +
+        #                 "a : A ;";
+        #         writeFile(tmpdir, "/M.g", master);
+
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+
+        #         assertEquals("unexpected errors: "+equeue, 1, equeue.errors.size());
+        #         assertEquals("unexpected errors: "+equeue, 0, equeue.warnings.size());
+
+        #         String expectedError =
+        #                 "error(161): "+tmpdir.toString().replaceFirst("\\-[0-9]+","")+"/M.g:2:8: tree grammar M cannot import lexer grammar S";
+        #         assertEquals(expectedError, equeue.errors.get(0).toString().replaceFirst("\\-[0-9]+",""));
+        # }
+
+        # @Test public void testSyntacticPredicateRulesAreNotInherited() throws Exception {
+        #         // if this compiles, it means that synpred1_S is defined in S.java
+        #         // but not MParser.java.  MParser has its own synpred1_M which must
+        #         // be separate to compile.
+        #         String slave =
+        #                 "parser grammar S;\n" +
+        #                 "a : 'a' {System.out.println(\"S.a1\");}\n" +
+        #                 "  | 'a' {System.out.println(\"S.a2\");}\n" +
+        #                 "  ;\n" +
+        #                 "b : 'x' | 'y' {;} ;\n"; // preds generated but not need in DFA here
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "options {backtrack=true;}\n" +
+        #                 "import S;\n" +
+        #                 "start : a b ;\n" +
+        #                 "nonsense : 'q' | 'q' {;} ;" + // forces def of preds here in M
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         String found = execParser("M.g", master, "MParser", "MLexer",
+        #                                                           "start", "ax", debug);
+        #         assertEquals("S.a1\n", found);
+        # }
+
+        # @Test public void testKeywordVSIDGivesNoWarning() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "lexer grammar S;\n" +
+        #                 "A : 'abc' {System.out.println(\"S.A\");} ;\n" +
+        #                 "ID : 'a'..'z'+ ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S;\n" +
+        #                 "a : A {System.out.println(\"M.a\");} ;\n" +
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         String found = execParser("M.g", master, "MParser", "MLexer",
+        #                                                           "a", "abc", debug);
+
+        #         assertEquals("unexpected errors: "+equeue, 0, equeue.errors.size());
+        #         assertEquals("unexpected warnings: "+equeue, 0, equeue.warnings.size());
+
+        #         assertEquals("S.A\nM.a\n", found);
+        # }
+
+        # @Test public void testWarningForUndefinedToken() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "lexer grammar S;\n" +
+        #                 "A : 'abc' {System.out.println(\"S.A\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S;\n" +
+        #                 "a : ABC A {System.out.println(\"M.a\");} ;\n" +
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         // A is defined in S but M should still see it and not give warning.
+        #         // only problem is ABC.
+
+        #         rawGenerateAndBuildRecognizer("M.g", master, "MParser", "MLexer", debug);
+
+        #         assertEquals("unexpected errors: "+equeue, 0, equeue.errors.size());
+        #         assertEquals("unexpected warnings: "+equeue, 1, equeue.warnings.size());
+
+        #         String expectedError =
+        #                 "warning(105): "+tmpdir.toString().replaceFirst("\\-[0-9]+","")+"/M.g:3:5: no lexer rule corresponding to token: ABC";
+        #         assertEquals(expectedError, equeue.warnings.get(0).toString().replaceFirst("\\-[0-9]+",""));
+        # }
+
+        # /** Make sure that M can import S that imports T. */
+        # @Test public void test3LevelImport() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "parser grammar T;\n" +
+        #                 "a : T ;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "T.g", slave);
+        #         String slave2 =
+        #                 "parser grammar S;\n" + // A, B, C token type order
+        #                 "import T;\n" +
+        #                 "a : S ;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave2);
+
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S;\n" +
+        #                 "a : M ;\n" ;
+        #         writeFile(tmpdir, "M.g", master);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+        #         g.composite.defineGrammarSymbols();
+
+        #         String expectedTokenIDToTypeMap = "[M=6, S=5, T=4]";
+        #         String expectedStringLiteralToTypeMap = "{}";
+        #         String expectedTypeToTokenList = "[T, S, M]";
+
+        #         assertEquals(expectedTokenIDToTypeMap,
+        #                                  realElements(g.composite.tokenIDToTypeMap).toString());
+        #         assertEquals(expectedStringLiteralToTypeMap, g.composite.stringLiteralToTypeMap.toString());
+        #         assertEquals(expectedTypeToTokenList,
+        #                                  realElements(g.composite.typeToTokenList).toString());
+
+        #         assertEquals("unexpected errors: "+equeue, 0, equeue.errors.size());
+
+        #         boolean ok =
+        #                 rawGenerateAndBuildRecognizer("M.g", master, "MParser", null, false);
+        #         boolean expecting = true; // should be ok
+        #         assertEquals(expecting, ok);
+        # }
+
+        # @Test public void testBigTreeOfImports() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "parser grammar T;\n" +
+        #                 "x : T ;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "T.g", slave);
+        #         slave =
+        #                 "parser grammar S;\n" +
+        #                 "import T;\n" +
+        #                 "y : S ;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+
+        #         slave =
+        #                 "parser grammar C;\n" +
+        #                 "i : C ;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "C.g", slave);
+        #         slave =
+        #                 "parser grammar B;\n" +
+        #                 "j : B ;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "B.g", slave);
+        #         slave =
+        #                 "parser grammar A;\n" +
+        #                 "import B,C;\n" +
+        #                 "k : A ;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "A.g", slave);
+
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S,A;\n" +
+        #                 "a : M ;\n" ;
+        #         writeFile(tmpdir, "M.g", master);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+        #         g.composite.defineGrammarSymbols();
+
+        #         String expectedTokenIDToTypeMap = "[A=8, B=6, C=7, M=9, S=5, T=4]";
+        #         String expectedStringLiteralToTypeMap = "{}";
+        #         String expectedTypeToTokenList = "[T, S, B, C, A, M]";
+
+        #         assertEquals(expectedTokenIDToTypeMap,
+        #                                  realElements(g.composite.tokenIDToTypeMap).toString());
+        #         assertEquals(expectedStringLiteralToTypeMap, g.composite.stringLiteralToTypeMap.toString());
+        #         assertEquals(expectedTypeToTokenList,
+        #                                  realElements(g.composite.typeToTokenList).toString());
+
+        #         assertEquals("unexpected errors: "+equeue, 0, equeue.errors.size());
+
+        #         boolean ok =
+        #                 rawGenerateAndBuildRecognizer("M.g", master, "MParser", null, false);
+        #         boolean expecting = true; // should be ok
+        #         assertEquals(expecting, ok);
+        # }
+
+        # @Test public void testRulesVisibleThroughMultilevelImport() throws Exception {
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String slave =
+        #                 "parser grammar T;\n" +
+        #                 "x : T ;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "T.g", slave);
+        #         String slave2 =
+        #                 "parser grammar S;\n" + // A, B, C token type order
+        #                 "import T;\n" +
+        #                 "a : S ;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave2);
+
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S;\n" +
+        #                 "a : M x ;\n" ; // x MUST BE VISIBLE TO M
+        #         writeFile(tmpdir, "M.g", master);
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/M.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+        #         g.composite.defineGrammarSymbols();
+
+        #         String expectedTokenIDToTypeMap = "[M=6, S=5, T=4]";
+        #         String expectedStringLiteralToTypeMap = "{}";
+        #         String expectedTypeToTokenList = "[T, S, M]";
+
+        #         assertEquals(expectedTokenIDToTypeMap,
+        #                                  realElements(g.composite.tokenIDToTypeMap).toString());
+        #         assertEquals(expectedStringLiteralToTypeMap, g.composite.stringLiteralToTypeMap.toString());
+        #         assertEquals(expectedTypeToTokenList,
+        #                                  realElements(g.composite.typeToTokenList).toString());
+
+        #         assertEquals("unexpected errors: "+equeue, 0, equeue.errors.size());
+        # }
+
+        # @Test public void testNestedComposite() throws Exception {
+        #         // Wasn't compiling. http://www.antlr.org/jira/browse/ANTLR-438
+        #         ErrorQueue equeue = new ErrorQueue();
+        #         ErrorManager.setErrorListener(equeue);
+        #         String gstr =
+        #                 "lexer grammar L;\n" +
+        #                 "T1: '1';\n" +
+        #                 "T2: '2';\n" +
+        #                 "T3: '3';\n" +
+        #                 "T4: '4';\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "L.g", gstr);
+        #         gstr =
+        #                 "parser grammar G1;\n" +
+        #                 "s: a | b;\n" +
+        #                 "a: T1;\n" +
+        #                 "b: T2;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "G1.g", gstr);
+
+        #         gstr =
+        #                 "parser grammar G2;\n" +
+        #                 "import G1;\n" +
+        #                 "a: T3;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "G2.g", gstr);
+        #         String G3str =
+        #                 "grammar G3;\n" +
+        #                 "import G2;\n" +
+        #                 "b: T4;\n" ;
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "G3.g", G3str);
+
+        #         Tool antlr = newTool(new String[] {"-lib", tmpdir});
+        #         CompositeGrammar composite = new CompositeGrammar();
+        #         Grammar g = new Grammar(antlr,tmpdir+"/G3.g",composite);
+        #         composite.setDelegationRoot(g);
+        #         g.parseAndBuildAST();
+        #         g.composite.assignTokenTypes();
+        #         g.composite.defineGrammarSymbols();
+
+        #         String expectedTokenIDToTypeMap = "[T1=4, T2=5, T3=6, T4=7]";
+        #         String expectedStringLiteralToTypeMap = "{}";
+        #         String expectedTypeToTokenList = "[T1, T2, T3, T4]";
+
+        #         assertEquals(expectedTokenIDToTypeMap,
+        #                                  realElements(g.composite.tokenIDToTypeMap).toString());
+        #         assertEquals(expectedStringLiteralToTypeMap, g.composite.stringLiteralToTypeMap.toString());
+        #         assertEquals(expectedTypeToTokenList,
+        #                                  realElements(g.composite.typeToTokenList).toString());
+
+        #         assertEquals("unexpected errors: "+equeue, 0, equeue.errors.size());
+
+        #         boolean ok =
+        #                 rawGenerateAndBuildRecognizer("G3.g", G3str, "G3Parser", null, false);
+        #         boolean expecting = true; // should be ok
+        #         assertEquals(expecting, ok);
+        # }
+
+        # @Test public void testHeadersPropogatedCorrectlyToImportedGrammars() throws Exception {
+        #         String slave =
+        #                 "parser grammar S;\n" +
+        #                 "a : B {System.out.print(\"S.a\");} ;\n";
+        #         mkdir(tmpdir);
+        #         writeFile(tmpdir, "S.g", slave);
+        #         String master =
+        #                 "grammar M;\n" +
+        #                 "import S;\n" +
+        #                 "@header{package mypackage;}\n" +
+        #                 "@lexer::header{package mypackage;}\n" +
+        #                 "s : a ;\n" +
+        #                 "B : 'b' ;" + // defines B from inherited token space
+        #                 "WS : (' '|'\\n') {skip();} ;\n" ;
+        #         boolean ok = antlr("M.g", "M.g", master, debug);
+        #         boolean expecting = true; // should be ok
+        #         assertEquals(expecting, ok);
+        # }
+
+
if __name__ == '__main__':
unittest.main()
diff --git a/runtime/Python/tests/t055templates.py b/runtime/Python/tests/t055templates.py
index 86d3ef0..5090b01 100644
--- a/runtime/Python/tests/t055templates.py
+++ b/runtime/Python/tests/t055templates.py
@@ -22,7 +22,7 @@ class T(testbase.ANTLRTest):
if result.st is not None:
return result.st.toString()
return None
-
+

def testInlineTemplate(self):
grammar = textwrap.dedent(
@@ -35,7 +35,7 @@ class T(testbase.ANTLRTest):
-> template(id={$ID.text}, int={$INT.text})
"id=<id>, int=<int>"
;
-
+
ID : 'a'..'z'+;
INT : '0'..'9'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
@@ -64,7 +64,7 @@ class T(testbase.ANTLRTest):
file=StringIO(templates),
lexer='angle-bracket'
)
-
+
grammar = textwrap.dedent(
r'''grammar T2;
options {
@@ -75,7 +75,7 @@ class T(testbase.ANTLRTest):
-> expr(op={$OP.text}, args={$r})
;
arg: ID -> template(t={$ID.text}) "<t>";
-
+
ID : 'a'..'z'+;
OP: '+';
WS : (' '|'\n') {$channel=HIDDEN;} ;
@@ -99,9 +99,9 @@ class T(testbase.ANTLRTest):
output=template;
}
a : ID INT
-              ->
+              ->
;
-
+
ID : 'a'..'z'+;
INT : '0'..'9'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
@@ -131,7 +131,7 @@ class T(testbase.ANTLRTest):
b: ID
-> template(t={$ID.text}) "<t>"
;
-
+
ID : 'a'..'z'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
'''
@@ -155,7 +155,7 @@ class T(testbase.ANTLRTest):
a: ID
-> { stringtemplate3.StringTemplate("hello") }
;
-
+
ID : 'a'..'z'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
'''
@@ -179,7 +179,7 @@ class T(testbase.ANTLRTest):
a: ID
{ $st = %{"hello"} }
;
-
+
ID : 'a'..'z'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
'''
@@ -207,7 +207,7 @@ class T(testbase.ANTLRTest):
}
-> { res }
;
-
+
ID : 'a'..'z'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
'''
@@ -235,7 +235,7 @@ class T(testbase.ANTLRTest):
file=StringIO(templates),
lexer='angle-bracket'
)
-
+
grammar = textwrap.dedent(
r'''grammar T;
options {
@@ -247,7 +247,7 @@ class T(testbase.ANTLRTest):
$st = %({"expr"})(args={[1, 2, 3]}, op={"+"})
}
;
-
+
ID : 'a'..'z'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
'''
@@ -277,7 +277,7 @@ class T(testbase.ANTLRTest):
->                  template(int={$INT.text})
"C: <int>"
;
-
+
ID : 'a'..'z'+;
INT : '0'..'9'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
@@ -304,7 +304,7 @@ class T(testbase.ANTLRTest):
-> template(id={$ID.text}, int={$INT.text})
"id=<id>, int=<int>"
;
-
+
ID : 'a'..'z'+;
INT : '0'..'9'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
@@ -341,12 +341,12 @@ class T(testbase.ANTLRTest):
: expr
-> template(t={$text}) <<boom(<t>)>>
;
-
+
expr
: ID
| INT
;
-
+
ID:  'a'..'z'+;
INT: '0'..'9'+;
WS: (' '|'\n')+ {$channel=HIDDEN;} ;
@@ -365,7 +365,7 @@ class T(testbase.ANTLRTest):
return 12;
'''
)
-
+
lexerCls, parserCls = self.compileInlineGrammar(grammar)

cStream = antlr3.StringStream(input)
@@ -375,7 +375,7 @@ class T(testbase.ANTLRTest):
result = parser.prog()

found = tStream.toString()
-
+
expected = textwrap.dedent(
'''\
if ( foo ) {
@@ -387,7 +387,7 @@ class T(testbase.ANTLRTest):
return boom(12);
'''
)
-
+
self.failUnlessEqual(expected, found)


@@ -403,20 +403,20 @@ class T(testbase.ANTLRTest):
BLOCK;
ASSIGN;
}
-
+
prog: stat+;

stat
: IF '(' e=expr ')' s=stat
-> ^(IF $e $s)
| RETURN expr ';'
-                  -> ^(RETURN expr)
+                  -> ^(RETURN expr)
| '{' stat* '}'
-                  -> ^(BLOCK stat*)
+                  -> ^(BLOCK stat*)
| ID '=' expr ';'
-> ^(ASSIGN ID expr)
;
-
+
expr
: ID
| INT
@@ -445,8 +445,8 @@ class T(testbase.ANTLRTest):

stat
: ^(IF expr stat)
-                | ^(RETURN return_expr)
-                | ^(BLOCK stat*)
+                | ^(RETURN return_expr)
+                | ^(BLOCK stat*)
| ^(ASSIGN ID expr)
;

@@ -454,7 +454,7 @@ class T(testbase.ANTLRTest):
: expr
-> template(t={$text}) <<boom(<t>)>>
;
-
+
expr
: ID
| INT
@@ -473,10 +473,10 @@ class T(testbase.ANTLRTest):
return 12;
'''
)
-
+
lexerCls, parserCls = self.compileInlineGrammar(grammar)
walkerCls = self.compileInlineGrammar(treeGrammar)
-
+
cStream = antlr3.StringStream(input)
lexer = lexerCls(cStream)
tStream = antlr3.TokenRewriteStream(lexer)
@@ -486,9 +486,9 @@ class T(testbase.ANTLRTest):
nodes.setTokenStream(tStream)
walker = walkerCls(nodes)
walker.prog()
-
+
found = tStream.toString()
-
+
expected = textwrap.dedent(
'''\
if ( foo ) {
@@ -500,9 +500,9 @@ class T(testbase.ANTLRTest):
return boom(12);
'''
)
-
+
self.failUnlessEqual(expected, found)

-
+
if __name__ == '__main__':
unittest.main()
diff --git a/runtime/Python/tests/t057autoAST.py b/runtime/Python/tests/t057autoAST.py
index e95469a..e5c1d35 100644
--- a/runtime/Python/tests/t057autoAST.py
+++ b/runtime/Python/tests/t057autoAST.py
@@ -901,7 +901,7 @@ class TestAutoAST(testbase.ANTLRTest):
''')

found, errors = self.execParser(grammar, "a", "abc", expectErrors=True)
-        self.assertEquals(["line 0:-1 missing INT at '<EOF>'"], errors)
+        self.assertEquals(["line 1:3 missing INT at '<EOF>'"], errors)
self.assertEquals("abc <missing INT>", found)


@@ -918,7 +918,7 @@ class TestAutoAST(testbase.ANTLRTest):
''')

found, errors = self.execParser(grammar, "a", "abc", expectErrors=True)
-        self.assertEquals(["line 0:-1 mismatched input '<EOF>' expecting INT"], errors)
+        self.assertEquals(["line 1:3 mismatched input '<EOF>' expecting INT"], errors)
self.assertEquals("<mismatched token: <EOF>, resync=abc>", found)


diff --git a/runtime/Python/tests/t058rewriteAST.py b/runtime/Python/tests/t058rewriteAST.py
index 2ebc5cb..15036f4 100644
--- a/runtime/Python/tests/t058rewriteAST.py
+++ b/runtime/Python/tests/t058rewriteAST.py
@@ -30,9 +30,9 @@ class TestRewriteAST(testbase.ANTLRTest):
def emitErrorMessage(self, msg):
self._errors.append(msg)

-
+
return TParser
-
+

def lexerClass(self, base):
class TLexer(base):
@@ -57,9 +57,9 @@ class TestRewriteAST(testbase.ANTLRTest):
def recover(self, input, re):
# no error recovery yet, just crash!
raise
-
+
return TLexer
-
+

def execParser(self, grammar, grammarEntry, input, expectErrors=False):
lexerCls, parserCls = self.compileInlineGrammar(grammar)
@@ -87,7 +87,7 @@ class TestRewriteAST(testbase.ANTLRTest):

else:
return result, parser._errors
-
+

def execTreeParser(self, grammar, grammarEntry, treeGrammar, treeEntry, input):
lexerCls, parserCls = self.compileInlineGrammar(grammar)
@@ -470,7 +470,7 @@ class TestRewriteAST(testbase.ANTLRTest):
options {language=Python;output=AST;}
a : ID INT -> {False}? ID
-> {True}? INT
-                       ->
+                       ->
;
ID : 'a'..'z'+ ;
INT : '0'..'9'+;
@@ -1176,7 +1176,7 @@ class TestRewriteAST(testbase.ANTLRTest):
grammar = textwrap.dedent(
r'''
grammar T;
-            options {language=Python;output=AST;}
+            options {language=Python;output=AST;}
a: (INT|ID)+ -> INT+ ID+ ;
INT: '0'..'9'+;
ID : 'a'..'z'+;
@@ -1191,7 +1191,7 @@ class TestRewriteAST(testbase.ANTLRTest):
grammar = textwrap.dedent(
r'''
grammar T;
-            options {language=Python;output=AST;}
+            options {language=Python;output=AST;}
a: (INT|ID) -> INT? ID? ;
INT: '0'..'9'+;
ID : 'a'..'z'+;
@@ -1208,7 +1208,7 @@ class TestRewriteAST(testbase.ANTLRTest):
grammar = textwrap.dedent(
r'''
grammar T;
-            options {language=Python;output=AST;}
+            options {language=Python;output=AST;}
a : x=(INT|ID) -> $x ;
INT: '0'..'9'+;
ID : 'a'..'z'+;
@@ -1222,14 +1222,14 @@ class TestRewriteAST(testbase.ANTLRTest):
def testRewriteAction(self):
grammar = textwrap.dedent(
r'''
-            grammar T;
+            grammar T;
options {language=Python;output=AST;}
tokens { FLOAT; }
r
-                : INT -> {CommonTree(CommonToken(type=FLOAT, text=$INT.text+".0"))}
-                ;
-            INT : '0'..'9'+;
-            WS: (' ' | '\n' | '\t')+ {$channel = HIDDEN;};
+                : INT -> {CommonTree(CommonToken(type=FLOAT, text=$INT.text+".0"))}
+                ;
+            INT : '0'..'9'+;
+            WS: (' ' | '\n' | '\t')+ {$channel = HIDDEN;};
''')

found = self.execParser(grammar, "r", "25")
@@ -1242,13 +1242,13 @@ class TestRewriteAST(testbase.ANTLRTest):
grammar = textwrap.dedent(
r"""
grammar T;
-            options {language=Python;output=AST;}
-            tokens {PARMS;}
-
-            modulo
-             : 'modulo' ID ('(' parms+ ')')? -> ^('modulo' ID ^(PARMS parms+)?)
-             ;
-            parms : '#'|ID;
+            options {language=Python;output=AST;}
+            tokens {PARMS;}
+
+            modulo
+             : 'modulo' ID ('(' parms+ ')')? -> ^('modulo' ID ^(PARMS parms+)?)
+             ;
+            parms : '#'|ID;
ID : ('a'..'z' | 'A'..'Z')+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
""")
@@ -1267,7 +1267,7 @@ class TestRewriteAST(testbase.ANTLRTest):
tokens {BLOCK;}
a : ID ID INT INT INT -> (ID INT)+;
ID : 'a'..'z'+ ;
-            INT : '0'..'9'+;
+            INT : '0'..'9'+;
WS : (' '|'\n') {$channel=HIDDEN;} ;
''')

@@ -1412,7 +1412,6 @@ class TestRewriteAST(testbase.ANTLRTest):
self.assertEquals("(EXPR <error: x> x 1)", found) # tree gets invented ID token


-    #@testbase.broken("FIXME", AssertionError)
def testMissingTokenGivesErrorNode(self):
grammar = textwrap.dedent(
r'''
@@ -1426,7 +1425,7 @@ class TestRewriteAST(testbase.ANTLRTest):

found, errors = self.execParser(grammar, "a", "abc",
expectErrors=True)
-        self.assertEquals(["line 0:-1 missing INT at '<EOF>'"], errors)
+        self.assertEquals(["line 1:3 missing INT at '<EOF>'"], errors)
# doesn't do in-line recovery for sets (yet?)
self.assertEquals("abc <missing INT>", found)

diff --git a/runtime/Python/tests/t059debug.py b/runtime/Python/tests/t059debug.py
index 0e15fd8..2097b66 100644
--- a/runtime/Python/tests/t059debug.py
+++ b/runtime/Python/tests/t059debug.py
@@ -78,7 +78,7 @@ class T(testbase.ANTLRTest):
tStream = antlr3.CommonTokenStream(lexer)
parser = parserCls(tStream, dbg=listener, port=port, **parser_args)
getattr(parser, grammarEntry)()
-
+
finally:
if listener is None:
debugger.join()
@@ -119,7 +119,7 @@ class T(testbase.ANTLRTest):
"exitRule a"]
found = [event for event in listener.events
if not event.startswith("LT ")]
-        self.assertEqual(found, expected)
+        self.assertListEqual(found, expected)

def testSocketProxy(self):
grammar = textwrap.dedent(
@@ -147,14 +147,14 @@ class T(testbase.ANTLRTest):
['LT', '1', '0', '4', '0', '1', '0', '"a'],
['consumeToken', '0', '4', '0', '1', '0', '"a'],
['location', '6', '8'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['consumeToken', '-1', '-1', '0', '0', '-1', '"'],
+                    ['LT', '1', '-1', '-1', '0', '1', '1', '"<EOF>'],
+                    ['LT', '1', '-1', '-1', '0', '1', '1', '"<EOF>'],
+                    ['consumeToken', '-1', '-1', '0', '1', '1', '"<EOF>'],
['location', '6', '11'],
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)

def testRecognitionException(self):
grammar = textwrap.dedent(
@@ -185,20 +185,20 @@ class T(testbase.ANTLRTest):
['location', '6', '8'],
['LT', '1', '2', '4', '0', '1', '2', '"b'],
['LT', '1', '2', '4', '0', '1', '2', '"b'],
-                    ['LT', '2', '-1', '-1', '0', '0', '-1', '"'],
+                    ['LT', '2', '-1', '-1', '0', '1', '3', '"<EOF>'],
['LT', '1', '2', '4', '0', '1', '2', '"b'],
['LT', '1', '2', '4', '0', '1', '2', '"b'],
['beginResync'],
['consumeToken', '2', '4', '0', '1', '2', '"b'],
['endResync'],
['exception', 'UnwantedTokenException', '2', '1', '2'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['consumeToken', '-1', '-1', '0', '0', '-1', '"'],
+                    ['LT', '1', '-1', '-1', '0', '1', '3', '"<EOF>'],
+                    ['consumeToken', '-1', '-1', '0', '1', '3', '"<EOF>'],
['location', '6', '11'],
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)


def testSemPred(self):
@@ -229,14 +229,14 @@ class T(testbase.ANTLRTest):
['LT', '1', '0', '4', '0', '1', '0', '"a'],
['consumeToken', '0', '4', '0', '1', '0', '"a'],
['location', '6', '16'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['consumeToken', '-1', '-1', '0', '0', '-1', '"'],
+                    ['LT', '1', '-1', '-1', '0', '1', '1', '"<EOF>'],
+                    ['LT', '1', '-1', '-1', '0', '1', '1', '"<EOF>'],
+                    ['consumeToken', '-1', '-1', '0', '1', '1', '"<EOF>'],
['location', '6', '19'],
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)


def testPositiveClosureBlock(self):
@@ -300,18 +300,18 @@ class T(testbase.ANTLRTest):
['LT', '1', '8', '5', '0', '1', '8', '"3'],
['consumeToken', '8', '5', '0', '1', '8', '"3'],
['enterDecision', '1'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
+                    ['LT', '1', '-1', '-1', '0', '1', '9', '"<EOF>'],
['exitDecision', '1'],
['exitSubRule', '1'],
['location', '6', '22'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['consumeToken', '-1', '-1', '0', '0', '-1', '"'],
+                    ['LT', '1', '-1', '-1', '0', '1', '9', '"<EOF>'],
+                    ['LT', '1', '-1', '-1', '0', '1', '9', '"<EOF>'],
+                    ['consumeToken', '-1', '-1', '0', '1', '9', '"<EOF>'],
['location', '6', '25'],
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)


def testClosureBlock(self):
@@ -375,18 +375,18 @@ class T(testbase.ANTLRTest):
['LT', '1', '8', '5', '0', '1', '8', '"3'],
['consumeToken', '8', '5', '0', '1', '8', '"3'],
['enterDecision', '1'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
+                    ['LT', '1', '-1', '-1', '0', '1', '9', '"<EOF>'],
['exitDecision', '1'],
['exitSubRule', '1'],
['location', '6', '22'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['consumeToken', '-1', '-1', '0', '0', '-1', '"'],
+                    ['LT', '1', '-1', '-1', '0', '1', '9', '"<EOF>'],
+                    ['LT', '1', '-1', '-1', '0', '1', '9', '"<EOF>'],
+                    ['consumeToken', '-1', '-1', '0', '1', '9', '"<EOF>'],
['location', '6', '25'],
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)


def testMismatchedSetException(self):
@@ -416,19 +416,19 @@ class T(testbase.ANTLRTest):
['LT', '1', '0', '4', '0', '1', '0', '"a'],
['consumeToken', '0', '4', '0', '1', '0', '"a'],
['location', '6', '8'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['exception', 'MismatchedSetException', '1', '0', '-1'],
-                    ['exception', 'MismatchedSetException', '1', '0', '-1'],
+                    ['LT', '1', '-1', '-1', '0', '1', '1', '"<EOF>'],
+                    ['LT', '1', '-1', '-1', '0', '1', '1', '"<EOF>'],
+                    ['LT', '1', '-1', '-1', '0', '1', '1', '"<EOF>'],
+                    ['exception', 'MismatchedSetException', '1', '1', '1'],
+                    ['exception', 'MismatchedSetException', '1', '1', '1'],
['beginResync'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
+                    ['LT', '1', '-1', '-1', '0', '1', '1', '"<EOF>'],
['endResync'],
['location', '6', '24'],
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)


def testBlock(self):
@@ -478,14 +478,14 @@ class T(testbase.ANTLRTest):
['exitRule', 'T.g', 'c'],
['exitSubRule', '1'],
['location', '6', '18'],
-                     ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                     ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                     ['consumeToken', '-1', '-1', '0', '0', '-1', '"'],
+                     ['LT', '1', '-1', '-1', '0', '1', '3', '"<EOF>'],
+                     ['LT', '1', '-1', '-1', '0', '1', '3', '"<EOF>'],
+                     ['consumeToken', '-1', '-1', '0', '1', '3', '"<EOF>'],
['location', '6', '21'],
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)


def testNoViableAlt(self):
@@ -531,13 +531,13 @@ class T(testbase.ANTLRTest):
['beginResync'],
['LT', '1', '2', '6', '0', '1', '2', '"!'],
['consumeToken', '2', '6', '0', '1', '2', '"!'],
-                     ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
+                     ['LT', '1', '-1', '-1', '0', '1', '3', '"<EOF>'],
['endResync'],
['location', '6', '21'],
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)


def testRuleBlock(self):
@@ -581,7 +581,7 @@ class T(testbase.ANTLRTest):
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)


def testRuleBlockSingleAlt(self):
@@ -621,7 +621,7 @@ class T(testbase.ANTLRTest):
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)


def testBlockSingleAlt(self):
@@ -663,7 +663,7 @@ class T(testbase.ANTLRTest):
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)


def testDFA(self):
@@ -728,14 +728,14 @@ class T(testbase.ANTLRTest):
['exitRule', 'T.g', 'c'],
['exitSubRule', '1'],
['location', '6', '15'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['LT', '1', '-1', '-1', '0', '0', '-1', '"'],
-                    ['consumeToken', '-1', '-1', '0', '0', '-1', '"'],
+                    ['LT', '1', '-1', '-1', '0', '1', '2', '"<EOF>'],
+                    ['LT', '1', '-1', '-1', '0', '1', '2', '"<EOF>'],
+                    ['consumeToken', '-1', '-1', '0', '1', '2', '"<EOF>'],
['location', '6', '18'],
['exitRule', 'T.g', 'a'],
['terminate']]

-        self.assertEqual(debugger.events, expected)
+        self.assertListEqual(debugger.events, expected)


def testBasicAST(self):
diff --git a/runtime/Python/tests/testbase.py b/runtime/Python/tests/testbase.py
index 19d1a12..b358f2f 100644
--- a/runtime/Python/tests/testbase.py
+++ b/runtime/Python/tests/testbase.py
@@ -114,6 +114,17 @@ class ANTLRTest(unittest.TestCase):
self.grammarType = None


+    def assertListEqual(self, a, b):
+        if a == b:
+            return
+
+        import difflib
+        a = [str(l) + '\n' for l in a]
+        b = [str(l) + '\n' for l in b]
+
+        raise AssertionError(''.join(difflib.unified_diff(a, b)))
+
+
@property
def baseDir(self):
if self._baseDir is None:
@@ -240,7 +251,7 @@ class ANTLRTest(unittest.TestCase):
)

# add dependencies to my .stg files
-                templateDir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'src', 'org', 'antlr', 'codegen', 'templates', 'Python'))
+                templateDir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'tool', 'src', 'main', 'resources', 'org', 'antlr', 'codegen', 'templates', 'Python'))
templates = glob.glob(os.path.join(templateDir, '*.stg'))

for dst, src in dependencies:
diff --git a/runtime/Python/unittests/teststreams.py b/runtime/Python/unittests/teststreams.py
index fe28750..f8430ba 100644
--- a/runtime/Python/unittests/teststreams.py
+++ b/runtime/Python/unittests/teststreams.py
@@ -16,15 +16,15 @@ class TestStringStream(unittest.TestCase):

self.failUnlessEqual(stream.size(), 3)

-
+
def testIndex(self):
"""StringStream.index()"""

stream = antlr3.StringStream('foo')

self.failUnlessEqual(stream.index(), 0)
-
-
+
+
def testConsume(self):
"""StringStream.consume()"""

@@ -74,8 +74,8 @@ class TestStringStream(unittest.TestCase):
self.failUnlessEqual(stream.index(), 7)
self.failUnlessEqual(stream.charPositionInLine, 3)
self.failUnlessEqual(stream.line, 2)
-
-
+
+
def testReset(self):
"""StringStream.reset()"""

@@ -89,8 +89,8 @@ class TestStringStream(unittest.TestCase):
self.failUnlessEqual(stream.line, 1)
self.failUnlessEqual(stream.charPositionInLine, 0)
self.failUnlessEqual(stream.LT(1), 'f')
-
-
+
+
def testLA(self):
"""StringStream.LA()"""

@@ -99,7 +99,7 @@ class TestStringStream(unittest.TestCase):
self.failUnlessEqual(stream.LT(1), 'f')
self.failUnlessEqual(stream.LT(2), 'o')
self.failUnlessEqual(stream.LT(3), 'o')
-
+
stream.consume()
stream.consume()

@@ -112,20 +112,20 @@ class TestStringStream(unittest.TestCase):
"""StringStream.substring()"""

stream = antlr3.StringStream('foobar')
-
+
self.failUnlessEqual(stream.substring(0, 0), 'f')
self.failUnlessEqual(stream.substring(0, 1), 'fo')
self.failUnlessEqual(stream.substring(0, 5), 'foobar')
self.failUnlessEqual(stream.substring(3, 5), 'bar')
-
-
+
+
def testSeekForward(self):
"""StringStream.seek(): forward"""

stream = antlr3.StringStream('foo\nbar')

stream.seek(4)
-
+
self.failUnlessEqual(stream.index(), 4)
self.failUnlessEqual(stream.line, 2)
self.failUnlessEqual(stream.charPositionInLine, 0)
@@ -140,7 +140,7 @@ class TestStringStream(unittest.TestCase):

##         stream.seek(4)
##         stream.seek(1)
-
+
##         self.failUnlessEqual(stream.index(), 1)
##         self.failUnlessEqual(stream.line, 1)
##         self.failUnlessEqual(stream.charPositionInLine, 1)
@@ -162,7 +162,7 @@ class TestStringStream(unittest.TestCase):
marker = stream.mark()
self.failUnlessEqual(marker, 2)
self.failUnlessEqual(stream.markDepth, 2)
-
+

def testReleaseLast(self):
"""StringStream.release(): last marker"""
@@ -171,7 +171,7 @@ class TestStringStream(unittest.TestCase):

stream.seek(4)
marker1 = stream.mark()
-
+
stream.consume()
marker2 = stream.mark()

@@ -181,7 +181,7 @@ class TestStringStream(unittest.TestCase):
# release same marker again, nothing has changed
stream.release()
self.failUnlessEqual(stream.markDepth, 1)
-
+

def testReleaseNested(self):
"""StringStream.release(): nested"""
@@ -190,16 +190,16 @@ class TestStringStream(unittest.TestCase):

stream.seek(4)
marker1 = stream.mark()
-
+
stream.consume()
marker2 = stream.mark()
-
+
stream.consume()
marker3 = stream.mark()

stream.release(marker2)
self.failUnlessEqual(stream.markDepth, 1)
-
+

def testRewindLast(self):
"""StringStream.rewind(): last marker"""
@@ -218,7 +218,7 @@ class TestStringStream(unittest.TestCase):
self.failUnlessEqual(stream.line, 2)
self.failUnlessEqual(stream.charPositionInLine, 0)
self.failUnlessEqual(stream.LT(1), 'b')
-
+

def testRewindNested(self):
"""StringStream.rewind(): nested"""
@@ -227,10 +227,10 @@ class TestStringStream(unittest.TestCase):

stream.seek(4)
marker1 = stream.mark()
-
+
stream.consume()
marker2 = stream.mark()
-
+
stream.consume()
marker3 = stream.mark()

@@ -241,22 +241,22 @@ class TestStringStream(unittest.TestCase):
self.failUnlessEqual(stream.charPositionInLine, 1)
self.failUnlessEqual(stream.LT(1), 'a')

-
+
class TestFileStream(unittest.TestCase):
"""Test case for the FileStream class."""


def testNoEncoding(self):
path = os.path.join(os.path.dirname(__file__), 'teststreams.input1')
-
+
stream = antlr3.FileStream(path)

stream.seek(4)
marker1 = stream.mark()
-
+
stream.consume()
marker2 = stream.mark()
-
+
stream.consume()
marker3 = stream.mark()

@@ -271,15 +271,15 @@ class TestFileStream(unittest.TestCase):

def testEncoded(self):
path = os.path.join(os.path.dirname(__file__), 'teststreams.input2')
-
+
stream = antlr3.FileStream(path, 'utf-8')

stream.seek(4)
marker1 = stream.mark()
-
+
stream.consume()
marker2 = stream.mark()
-
+
stream.consume()
marker3 = stream.mark()

@@ -291,22 +291,22 @@ class TestFileStream(unittest.TestCase):
self.failUnlessEqual(stream.LT(1), u'ä')
self.failUnlessEqual(stream.LA(1), ord(u'ä'))

-
+

class TestInputStream(unittest.TestCase):
"""Test case for the InputStream class."""

def testNoEncoding(self):
file = StringIO('foo\nbar')
-
+
stream = antlr3.InputStream(file)

stream.seek(4)
marker1 = stream.mark()
-
+
stream.consume()
marker2 = stream.mark()
-
+
stream.consume()
marker3 = stream.mark()

@@ -321,15 +321,15 @@ class TestInputStream(unittest.TestCase):

def testEncoded(self):
file = StringIO(u'foo\nbär'.encode('utf-8'))
-
+
stream = antlr3.InputStream(file, 'utf-8')

stream.seek(4)
marker1 = stream.mark()
-
+
stream.consume()
marker2 = stream.mark()
-
+
stream.consume()
marker3 = stream.mark()

@@ -341,7 +341,7 @@ class TestInputStream(unittest.TestCase):
self.failUnlessEqual(stream.LT(1), u'ä')
self.failUnlessEqual(stream.LA(1), ord(u'ä'))

-
+
class TestCommonTokenStream(unittest.TestCase):
"""Test case for the StringStream class."""

@@ -357,22 +357,25 @@ class TestCommonTokenStream(unittest.TestCase):
def __init__(self):
self.tokens = []

+            def makeEOFToken(self):
+                return antlr3.CommonToken(type=antlr3.EOF)
+
def nextToken(self):
try:
return self.tokens.pop(0)
except IndexError:
return None
-
+
self.source = MockSource()
-
-
+
+
def testInit(self):
"""CommonTokenStream.__init__()"""

stream = antlr3.CommonTokenStream(self.source)
self.failUnlessEqual(stream.index(), -1)
-
-
+
+
def testSetTokenSource(self):
"""CommonTokenStream.setTokenSource()"""

@@ -389,7 +392,7 @@ class TestCommonTokenStream(unittest.TestCase):

lt1 = stream.LT(1)
self.failUnlessEqual(lt1.type, antlr3.EOF)
-
+

def testLT1(self):
"""CommonTokenStream.LT(1)"""
@@ -397,12 +400,12 @@ class TestCommonTokenStream(unittest.TestCase):
self.source.tokens.append(
antlr3.CommonToken(type=12)
)
-
+
stream = antlr3.CommonTokenStream(self.source)

lt1 = stream.LT(1)
self.failUnlessEqual(lt1.type, 12)
-
+

def testLT1WithHidden(self):
"""CommonTokenStream.LT(1): with hidden tokens"""
@@ -410,16 +413,16 @@ class TestCommonTokenStream(unittest.TestCase):
self.source.tokens.append(
antlr3.CommonToken(type=12, channel=antlr3.HIDDEN_CHANNEL)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=13)
)
-
+
stream = antlr3.CommonTokenStream(self.source)

lt1 = stream.LT(1)
self.failUnlessEqual(lt1.type, 13)
-
+

def testLT2BeyondEnd(self):
"""CommonTokenStream.LT(2): beyond end"""
@@ -427,16 +430,16 @@ class TestCommonTokenStream(unittest.TestCase):
self.source.tokens.append(
antlr3.CommonToken(type=12)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=13, channel=antlr3.HIDDEN_CHANNEL)
)
-
+
stream = antlr3.CommonTokenStream(self.source)

lt1 = stream.LT(2)
self.failUnlessEqual(lt1.type, antlr3.EOF)
-
+

# not yet implemented
def testLTNegative(self):
@@ -445,15 +448,15 @@ class TestCommonTokenStream(unittest.TestCase):
self.source.tokens.append(
antlr3.CommonToken(type=12)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=13)
)
-
+
stream = antlr3.CommonTokenStream(self.source)
stream.fillBuffer()
stream.consume()
-
+
lt1 = stream.LT(-1)
self.failUnlessEqual(lt1.type, 12)

@@ -464,17 +467,17 @@ class TestCommonTokenStream(unittest.TestCase):
self.source.tokens.append(
antlr3.CommonToken(type=12)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=13)
)
-
+
stream = antlr3.CommonTokenStream(self.source)
stream.fillBuffer()
stream.consume()
-
+
self.failUnlessEqual(stream.LB(1).type, 12)
-
+

def testLTZero(self):
"""CommonTokenStream.LT(0)"""
@@ -482,16 +485,16 @@ class TestCommonTokenStream(unittest.TestCase):
self.source.tokens.append(
antlr3.CommonToken(type=12)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=13)
)
-
+
stream = antlr3.CommonTokenStream(self.source)

lt1 = stream.LT(0)
self.failUnless(lt1 is None)
-
+

def testLBBeyondBegin(self):
"""CommonTokenStream.LB(-1): beyond begin"""
@@ -499,26 +502,26 @@ class TestCommonTokenStream(unittest.TestCase):
self.source.tokens.append(
antlr3.CommonToken(type=12)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=12, channel=antlr3.HIDDEN_CHANNEL)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=12, channel=antlr3.HIDDEN_CHANNEL)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=13)
)
-
+
stream = antlr3.CommonTokenStream(self.source)
self.failUnless(stream.LB(1) is None)

stream.consume()
stream.consume()
self.failUnless(stream.LB(3) is None)
-
+

def testFillBuffer(self):
"""CommonTokenStream.fillBuffer()"""
@@ -526,19 +529,19 @@ class TestCommonTokenStream(unittest.TestCase):
self.source.tokens.append(
antlr3.CommonToken(type=12)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=13)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=14)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=antlr3.EOF)
)
-
+
stream = antlr3.CommonTokenStream(self.source)
stream.fillBuffer()

@@ -546,23 +549,23 @@ class TestCommonTokenStream(unittest.TestCase):
self.failUnlessEqual(stream.tokens[0].type, 12)
self.failUnlessEqual(stream.tokens[1].type, 13)
self.failUnlessEqual(stream.tokens[2].type, 14)
-
-
+
+
def testConsume(self):
"""CommonTokenStream.consume()"""

self.source.tokens.append(
antlr3.CommonToken(type=12)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=13)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=antlr3.EOF)
)
-
+
stream = antlr3.CommonTokenStream(self.source)
self.failUnlessEqual(stream.LA(1), 12)

@@ -574,23 +577,23 @@ class TestCommonTokenStream(unittest.TestCase):

stream.consume()
self.failUnlessEqual(stream.LA(1), antlr3.EOF)
-
-
+
+
def testSeek(self):
"""CommonTokenStream.seek()"""

self.source.tokens.append(
antlr3.CommonToken(type=12)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=13)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=antlr3.EOF)
)
-
+
stream = antlr3.CommonTokenStream(self.source)
self.failUnlessEqual(stream.LA(1), 12)

@@ -599,60 +602,60 @@ class TestCommonTokenStream(unittest.TestCase):

stream.seek(0)
self.failUnlessEqual(stream.LA(1), 12)
-
-
+
+
def testMarkRewind(self):
"""CommonTokenStream.mark()/rewind()"""

self.source.tokens.append(
antlr3.CommonToken(type=12)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=13)
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=antlr3.EOF)
)
-
+
stream = antlr3.CommonTokenStream(self.source)
stream.fillBuffer()
-
+
stream.consume()
marker = stream.mark()
-
+
stream.consume()
stream.rewind(marker)
-
+
self.failUnlessEqual(stream.LA(1), 13)


def testToString(self):
"""CommonTokenStream.toString()"""
-
+
self.source.tokens.append(
antlr3.CommonToken(type=12, text="foo")
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=13, text="bar")
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=14, text="gnurz")
)
-
+
self.source.tokens.append(
antlr3.CommonToken(type=15, text="blarz")
)
-
+
stream = antlr3.CommonTokenStream(self.source)

assert stream.toString() == "foobargnurzblarz"
assert stream.toString(1, 2) == "bargnurz"
assert stream.toString(stream.tokens[1], stream.tokens[-2]) == "bargnurz"
-
+

if __name__ == "__main__":
unittest.main(testRunner=unittest.TextTestRunner(verbosity=2))
diff --git a/runtime/Python/unittests/testtree.py b/runtime/Python/unittests/testtree.py
index dc7b6aa..1f4e36f 100644
--- a/runtime/Python/unittests/testtree.py
+++ b/runtime/Python/unittests/testtree.py
@@ -194,7 +194,7 @@ class TestTreeNodeStream(unittest.TestCase):
stream.consume()

self.failUnlessEqual(EOF, stream.LT(1).getType())
-        self.failUnlessEqual(UP, stream.LT(-1).getType())
+        self.failUnlessEqual(UP, stream.LT(-1).getType())  #TODO: remove?
stream.rewind(m)      # REWIND

# consume til end again :)
@@ -203,7 +203,7 @@ class TestTreeNodeStream(unittest.TestCase):
stream.consume()

self.failUnlessEqual(EOF, stream.LT(1).getType())
-        self.failUnlessEqual(UP, stream.LT(-1).getType())
+        self.failUnlessEqual(UP, stream.LT(-1).getType())  #TODO: remove?


def testMarkRewindInMiddle(self):
@@ -359,6 +359,25 @@ class TestTreeNodeStream(unittest.TestCase):
self.assertEquals(v1, v2)


+    def testIterator(self):
+        r0 = CommonTree(CommonToken(101))
+        r1 = CommonTree(CommonToken(102))
+        r0.addChild(r1)
+        r1.addChild(CommonTree(CommonToken(103)))
+        r2 = CommonTree(CommonToken(106))
+        r2.addChild(CommonTree(CommonToken(107)))
+        r1.addChild(r2)
+        r0.addChild(CommonTree(CommonToken(104)))
+        r0.addChild(CommonTree(CommonToken(105)))
+
+        stream = CommonTreeNodeStream(r0)
+
+        expecting = [
+            101, DOWN, 102, DOWN, 103, 106, DOWN, 107, UP, UP, 104, 105, UP]
+        found = [t.type for t in stream]
+        self.assertEqual(expecting, found)
+
+
def toNodesOnlyString(self, nodes):
buf = []
for i in range(nodes.size()):
diff --git a/runtime/Python/unittests/testtreewizard.py b/runtime/Python/unittests/testtreewizard.py
index 18537f3..2ad99be 100644
--- a/runtime/Python/unittests/testtreewizard.py
+++ b/runtime/Python/unittests/testtreewizard.py
@@ -42,7 +42,7 @@ class TestTreePatternLexer(unittest.TestCase):
self.failUnlessEqual(lexer.sval, '')
self.failUnlessEqual(lexer.error, False)

-
+
def testEnd(self):
"""TreePatternLexer(): ')'"""

@@ -52,7 +52,7 @@ class TestTreePatternLexer(unittest.TestCase):
self.failUnlessEqual(lexer.sval, '')
self.failUnlessEqual(lexer.error, False)

-
+
def testPercent(self):
"""TreePatternLexer(): '%'"""

@@ -62,7 +62,7 @@ class TestTreePatternLexer(unittest.TestCase):
self.failUnlessEqual(lexer.sval, '')
self.failUnlessEqual(lexer.error, False)

-
+
def testDot(self):
"""TreePatternLexer(): '.'"""

@@ -72,7 +72,7 @@ class TestTreePatternLexer(unittest.TestCase):
self.failUnlessEqual(lexer.sval, '')
self.failUnlessEqual(lexer.error, False)

-
+
def testColon(self):
"""TreePatternLexer(): ':'"""

@@ -82,7 +82,7 @@ class TestTreePatternLexer(unittest.TestCase):
self.failUnlessEqual(lexer.sval, '')
self.failUnlessEqual(lexer.error, False)

-
+
def testEOF(self):
"""TreePatternLexer(): EOF"""

@@ -92,7 +92,7 @@ class TestTreePatternLexer(unittest.TestCase):
self.failUnlessEqual(lexer.sval, '')
self.failUnlessEqual(lexer.error, False)

-
+
def testID(self):
"""TreePatternLexer(): ID"""

@@ -102,7 +102,7 @@ class TestTreePatternLexer(unittest.TestCase):
self.failUnlessEqual(lexer.sval, '_foo12_bar')
self.failUnlessEqual(lexer.error, False)

-
+
def testARG(self):
"""TreePatternLexer(): ARG"""

@@ -131,7 +131,7 @@ class TestTreePatternParser(unittest.TestCase):

We need a tree adaptor, use CommonTreeAdaptor.
And a constant list of token names.
-
+
"""

self.adaptor = CommonTreeAdaptor()
@@ -172,7 +172,7 @@ class TestTreePatternParser(unittest.TestCase):
self.failUnlessEqual(tree.getChildCount(), 1)
self.failUnlessEqual(tree.getChild(0).getType(), 6)
self.failUnlessEqual(tree.getChild(0).getText(), 'B')
-
+

def testNil(self):
"""TreePatternParser: 'nil'"""
@@ -182,7 +182,7 @@ class TestTreePatternParser(unittest.TestCase):
self.failUnless(isinstance(tree, CommonTree))
self.failUnlessEqual(tree.getType(), 0)
self.failUnlessEqual(tree.getText(), None)
-
+

def testWildcard(self):
"""TreePatternParser: '(.)'"""
@@ -190,7 +190,7 @@ class TestTreePatternParser(unittest.TestCase):
parser = TreePatternParser(lexer, self.wizard, self.adaptor)
tree = parser.pattern()
self.failUnless(isinstance(tree, WildcardTreePattern))
-
+

def testLabel(self):
"""TreePatternParser: '(%a:A)'"""
@@ -199,7 +199,7 @@ class TestTreePatternParser(unittest.TestCase):
tree = parser.pattern()
self.failUnless(isinstance(tree, TreePattern))
self.failUnlessEqual(tree.label, 'a')
-
+

def testError1(self):
"""TreePatternParser: ')'"""
@@ -207,7 +207,7 @@ class TestTreePatternParser(unittest.TestCase):
parser = TreePatternParser(lexer, self.wizard, self.adaptor)
tree = parser.pattern()
self.failUnless(tree is None)
-
+

def testError2(self):
"""TreePatternParser: '()'"""
@@ -215,7 +215,7 @@ class TestTreePatternParser(unittest.TestCase):
parser = TreePatternParser(lexer, self.wizard, self.adaptor)
tree = parser.pattern()
self.failUnless(tree is None)
-
+

def testError3(self):
"""TreePatternParser: '(A ])'"""
@@ -223,7 +223,7 @@ class TestTreePatternParser(unittest.TestCase):
parser = TreePatternParser(lexer, self.wizard, self.adaptor)
tree = parser.pattern()
self.failUnless(tree is None)
-
+

class TestTreeWizard(unittest.TestCase):
"""Test case for the TreeWizard class."""
@@ -233,7 +233,7 @@ class TestTreeWizard(unittest.TestCase):

We need a tree adaptor, use CommonTreeAdaptor.
And a constant list of token names.
-
+
"""

self.adaptor = CommonTreeAdaptor()
@@ -269,12 +269,12 @@ class TestTreeWizard(unittest.TestCase):
wiz.getTokenType('A'),
5
)
-
+
self.failUnlessEqual(
wiz.getTokenType('VAR'),
11
)
-
+
self.failUnlessEqual(
wiz.getTokenType('invalid'),
INVALID_TOKEN_TYPE
@@ -339,7 +339,7 @@ class TestTreeWizard(unittest.TestCase):
(ttype, [str(node) for node in nodes])
for ttype, nodes in indexMap.items()
)
-
+
def testSingleNodeIndex(self):
wiz = TreeWizard(self.adaptor, self.tokens)
tree = wiz.create("ID")
@@ -374,9 +374,9 @@ class TestTreeWizard(unittest.TestCase):
elements = []
def visitor(node, parent, childIndex, labels):
elements.append(str(node))
-
+
wiz.visit(tree, wiz.getTokenType("B"), visitor)
-
+
expecting = ['B']
self.failUnlessEqual(expecting, elements)

@@ -388,9 +388,9 @@ class TestTreeWizard(unittest.TestCase):
elements = []
def visitor(node, parent, childIndex, labels):
elements.append(str(node))
-
+
wiz.visit(tree, wiz.getTokenType("C"), visitor)
-
+
expecting = ['C']
self.failUnlessEqual(expecting, elements)

@@ -402,9 +402,9 @@ class TestTreeWizard(unittest.TestCase):
elements = []
def visitor(node, parent, childIndex, labels):
elements.append(str(node))
-
+
wiz.visit(tree, wiz.getTokenType("B"), visitor)
-
+
expecting = ['B', 'B', 'B']
self.failUnlessEqual(expecting, elements)

@@ -416,9 +416,9 @@ class TestTreeWizard(unittest.TestCase):
elements = []
def visitor(node, parent, childIndex, labels):
elements.append(str(node))
-
+
wiz.visit(tree, wiz.getTokenType("A"), visitor)
-
+
expecting = ['A', 'A']
self.failUnlessEqual(expecting, elements)

@@ -430,9 +430,9 @@ class TestTreeWizard(unittest.TestCase):
elements = []
def visitor(node, parent, childIndex, labels):
elements.append('%s@%s[%d]' % (node, parent, childIndex))
-
+
wiz.visit(tree, wiz.getTokenType("B"), visitor)
-
+
expecting = ['B@A[0]', 'B@A[1]', 'B@A[2]']
self.failUnlessEqual(expecting, elements)

@@ -447,9 +447,9 @@ class TestTreeWizard(unittest.TestCase):
'%s@%s[%d]'
% (node, ['nil', parent][parent is not None], childIndex)
)
-
+
wiz.visit(tree, wiz.getTokenType("A"), visitor)
-
+
expecting = ['A@nil[0]', 'A@A[1]']
self.failUnlessEqual(expecting, elements)

@@ -463,9 +463,9 @@ class TestTreeWizard(unittest.TestCase):
elements.append(
str(node)
)
-
+
wiz.visit(tree, '(A B)', visitor)
-
+
expecting = ['A'] # shouldn't match overall root, just (A B)
self.failUnlessEqual(expecting, elements)

@@ -480,9 +480,9 @@ class TestTreeWizard(unittest.TestCase):
'%s@%s[%d]'
% (node, ['nil', parent][parent is not None], childIndex)
)
-
+
wiz.visit(tree, '(A B)', visitor)
-
+
expecting = ['A@A[2]', 'A@D[0]']
self.failUnlessEqual(expecting, elements)

@@ -502,9 +502,9 @@ class TestTreeWizard(unittest.TestCase):
labels['b'],
)
)
-
+
wiz.visit(tree, '(%a:A %b:B)', visitor)
-
+
expecting = ['foo@A[2]foo&bar', 'big@D[0]big&dog']
self.failUnlessEqual(expecting, elements)

@@ -567,6 +567,15 @@ class TestTreeWizard(unittest.TestCase):
self.failUnless(valid)


+    def testParseWithText2(self):
+        wiz = TreeWizard(self.adaptor, self.tokens)
+        t = wiz.create("(A B[T__32] (C (D E[a])))")
+        # C pattern has no text arg so despite [bar] in t, no need
+        # to match text--check structure only.
+        valid = wiz.parse(t, "(A B[foo] C)")
+        self.assertEquals("(A T__32 (C (D a)))", t.toStringTree())
+
+
def testParseWithTextFails(self):
wiz = TreeWizard(self.adaptor, self.tokens)
t = wiz.create("(A B C)")
@@ -634,7 +643,7 @@ class TestTreeWizard(unittest.TestCase):
same = wiz.equals(t1, t2)
self.failUnless(same)

-
+
def testEqualsWithMismatchedText(self):
wiz = TreeWizard(self.adaptor, self.tokens)
t1 = wiz.create("(A B[foo] C)")
diff --git a/tool/src/main/resources/org/antlr/codegen/templates/Python/Dbg.stg b/tool/src/main/resources/org/antlr/codegen/templates/Python/Dbg.stg
index 454e464..a16b054 100644
--- a/tool/src/main/resources/org/antlr/codegen/templates/Python/Dbg.stg
+++ b/tool/src/main/resources/org/antlr/codegen/templates/Python/Dbg.stg
@@ -66,6 +66,10 @@ ruleNames = [
"invalidRule", <grammar.allImportedRules:{rST | "<rST.name>"}; wrap="\n    ", separator=", ">
]<\n>
<endif>
+decisionCanBacktrack = [
+    False, # invalid decision
+    <grammar.decisions:{d | <if(d.dfa.hasSynPred)>True<else>False<endif>}; wrap="\n    ", separator=", ">
+    ]
<if(grammar.grammarIsRoot)> <! grammar imports other grammar(s) !>
def getRuleLevel(self):
return self.ruleLevel
@@ -229,7 +233,8 @@ finally:

@blockBody.decision() ::= <<
try:
-    self._dbg.enterDecision(<decisionNumber>)
+    self._dbg.enterDecision(
+        <decisionNumber>, self.decisionCanBacktrack[<decisionNumber>])
<@super.decision()>
finally:
self._dbg.exitDecision(<decisionNumber>)
@@ -237,7 +242,8 @@ finally:

@ruleBlock.decision() ::= <<
try:
-    self._dbg.enterDecision(<decisionNumber>)
+    self._dbg.enterDecision(
+        <decisionNumber>, self.decisionCanBacktrack[<decisionNumber>])
<@super.decision()>
finally:
self._dbg.exitDecision(<decisionNumber>)
@@ -257,7 +263,8 @@ finally:

@positiveClosureBlockLoop.decisionBody() ::= <<
try:
-    self._dbg.enterDecision(<decisionNumber>)
+    self._dbg.enterDecision(
+        <decisionNumber>, self.decisionCanBacktrack[<decisionNumber>])
<@super.decisionBody()>
finally:
self._dbg.exitDecision(<decisionNumber>)
@@ -276,7 +283,8 @@ finally:

@closureBlockLoop.decisionBody() ::= <<
try:
-    self._dbg.enterDecision(<decisionNumber>)
+    self._dbg.enterDecision(
+        <decisionNumber>, self.decisionCanBacktrack[<decisionNumber>])
<@super.decisionBody()>
finally:
self._dbg.exitDecision(<decisionNumber>)
diff --git a/tool/src/main/resources/org/antlr/codegen/templates/Python/Python.stg b/tool/src/main/resources/org/antlr/codegen/templates/Python/Python.stg
index ad3b17c..319ece3 100644
--- a/tool/src/main/resources/org/antlr/codegen/templates/Python/Python.stg
+++ b/tool/src/main/resources/org/antlr/codegen/templates/Python/Python.stg
@@ -96,7 +96,7 @@ if __name__ == '__main__':

>>

-lexer(grammar, name, tokens, scopes, rules, numRules, labelType="Token",
+lexer(grammar, name, tokens, scopes, rules, numRules, labelType="CommonToken",
filterMode, superClass="Lexer") ::= <<
<grammar.directDelegates:
{g|from <g.recognizerName> import <g.recognizerName>}; separator="\n">
@@ -155,7 +155,7 @@ filteringNextToken() ::= <<
def nextToken(self):
while True:
if self.input.LA(1) == EOF:
-            return EOF_TOKEN
+            return self.makeEOFToken()

self._state.token = None
self._state.channel = DEFAULT_CHANNEL
@@ -814,11 +814,15 @@ matchSetAndListLabel(s,label,elementIndex,postmatchCode) ::= <<
>>

/** Match a string literal */
-lexerStringRef(string,label,elementIndex) ::= <<
+lexerStringRef(string,label,elementIndex="0") ::= <<
<if(label)>
<label>Start = self.getCharIndex()
self.match(<string>)
+<label>StartLine<elementIndex> = self.getLine()
+<label>StartCharPos<elementIndex> = self.getCharPositionInLine()
<label> = <labelType>(input=self.input, type=INVALID_TOKEN_TYPE, channel=DEFAULT_CHANNEL, start=<label>Start, stop=self.getCharIndex()-1)
+<label>.setLine(<label>StartLine<elementIndex>)
+<label>.setCharPositionInLine(<label>StartCharPos<elementIndex>)
<else>
self.match(<string>)
<endif>
@@ -876,13 +880,16 @@ lexerRuleRef(rule,label,args,elementIndex,scope) ::= <<
<if(label)>
<label>Start<elementIndex> = self.getCharIndex()
self.<if(scope)><scope:delegateName()>.<endif>m<rule.name>(<args; separator=", ">)
+<label>StartLine<elementIndex> = self.getLine()
+<label>StartCharPos<elementIndex> = self.getCharPositionInLine()
<label> = <labelType>(
input=self.input,
type=INVALID_TOKEN_TYPE,
channel=DEFAULT_CHANNEL,
start=<label>Start<elementIndex>,
-    stop=self.getCharIndex()-1
-    )
+    stop=self.getCharIndex()-1)
+<label>.setLine(<label>StartLine<elementIndex>)
+<label>.setCharPositionInLine(<label>StartCharPos<elementIndex>)
<else>
self.<if(scope)><scope:delegateName()>.<endif>m<rule.name>(<args; separator=", ">)
<endif>
@@ -898,8 +905,12 @@ lexerRuleRefAndListLabel(rule,label,args,elementIndex,scope) ::= <<
lexerMatchEOF(label,elementIndex) ::= <<
<if(label)>
<label>Start<elementIndex> = self.getCharIndex()
+<label>StartLine<elementIndex> = self.getLine()
+<label>StartCharPos<elementIndex> = self.getCharPositionInLine()
self.match(EOF)
<label> = <labelType>(input=self.input, type=EOF, channel=DEFAULT_CHANNEL, start=<label>Start<elementIndex>, stop=self.getCharIndex()-1)
+<label>.setLine(<label>StartLine<elementIndex>)
+<label>.setCharPositionInLine(<label>StartCharPos<elementIndex>)
<else>
self.match(EOF)
<endif>

