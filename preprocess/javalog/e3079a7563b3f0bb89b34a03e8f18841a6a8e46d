commit e3079a7563b3f0bb89b34a03e8f18841a6a8e46d
Author:     Zannick <jokeserver@gmail.com>
AuthorDate: Sat Jun 16 19:19:17 2012 -0700
Commit:     Zannick <jokeserver@gmail.com>
CommitDate: Mon Jun 25 00:56:37 2012 -0700

Rewrite the Python target to support Python3.

Functional changes in the runtime:
- (2to3) Use __next__ for iterators.
- (2to3) Use "except Exception as e" syntax.
- (2to3) Replace basestring and unicode with str, long with int,
and chr with unichr.
- (2to3) Replace StringIO module with io.
- Use relative imports.
- Move compatible_api_versions to constants to avoid circular
dependency.
- main.py: Replace optparse with argparse.
(This makes us require Python 3.2.)
- Nuke compat.py and everything relating to stringtemplate3.
- Rearrange some loops and conditionals to be simpler.
- Stop using exceptions for control flow.
- Replace a handful of deprecated setters and getters with properties.
Change many things to use those instead (mostly for Token objects).
- Use a with statement for opening a file.
- Don't use exceptions for control flow.
- Put common properties of Token subclasses in the Token class itself.
- Remove an unnecessary argument from a matchAny function.
- Fix a newline that was meant to be two characters.
- Axe the use of encodings entirely (which will result in all files
being opened as UTF-8, which is fine because all strings are unicode
in Python 3), because bytes are no longer the same as strings.

Cosmetic changes in the runtime:
- Use super() instead of super(Class, self) or
SuperClass.function(self).
- Fix some spelling errors in comments and such.
- Replace tabs with spaces and correct some off-by-one indenting.
- Remove commented-out print statements.
- dfa.py: Remove a comment asking for a bitwise op to check equality
against 0xFFFF. This can be done with (((a + 1) & 0xFFFF) - 1), but it
is substantially slower than the conditional.
- Use decorators.
- Use the implied string concatenator instead of putting '+' on each
line.
- Use single-quotes instead of escaping double-quotes.
- Replace all '%' based string formatting with the new .format syntax.
- Add spacing between some binary operands.
- Use raw strings instead of doubling backslashes.
- Remove unnecessary semicolons.
- Remove unused tokenNames argument for some error handling functions
that use self.tokenNames anyway.

Template changes:
- Remove likely unnecessary C# booleanLiteral rule.
- Use super() instead of super(Class, self).
- Use relative imports if the generated code is being used as a library,
absolute imports otherwise.
- Insert comment about circular dependencies within the generated
parser.
- Don't pass arguments to matchAny().
- Don't need to say u"" anymore.
- Remove all hard tabs. *Correctly* remove all hard tabs, that is.
- Access Tokens' properties directly. But not recognizers and trees.
- Remove an unnecessary semicolon.

diff --git a/runtime/Python3/antlr3/__init__.py b/runtime/Python3/antlr3/__init__.py
index 4068559..b2458f2 100644
--- a/runtime/Python3/antlr3/__init__.py
+++ b/runtime/Python3/antlr3/__init__.py
@@ -6,7 +6,7 @@ generated by ANTLR3.

@mainpage

-\note Please be warned that the line numbers in the API documentation do not
+\\note Please be warned that the line numbers in the API documentation do not
match the real locations in the source code of the package. This is an
unintended artifact of doxygen, which I could only convince to use the
correct module names by concatenating all files from the package into a single
@@ -140,13 +140,13 @@ bug in your grammar, it can only be detected at runtime.

__version__ = '3.4'

-# This runtime is compatible with generated parsers using the following
-# API versions. 'HEAD' is only used by unittests.
-compatible_api_versions = ['HEAD', 1]
+# This runtime is compatible with generated parsers using the
+# API versions listed in constants.compatible_api_versions.
+# 'HEAD' is only used by unittests.

-from constants import *
-from dfa import *
-from exceptions import *
-from recognizers import *
-from streams import *
-from tokens import *
+from .constants import *
+from .dfa import *
+from .exceptions import *
+from .recognizers import *
+from .streams import *
+from .tokens import *
diff --git a/runtime/Python3/antlr3/compat.py b/runtime/Python3/antlr3/compat.py
deleted file mode 100644
index b29afca..0000000
--- a/runtime/Python3/antlr3/compat.py
+++ /dev/null
@@ -1,48 +0,0 @@
-"""Compatibility stuff"""
-
-# begin[licence]
-#
-# [The "BSD licence"]
-# Copyright (c) 2005-2008 Terence Parr
-# All rights reserved.
-#
-# Redistribution and use in source and binary forms, with or without
-# modification, are permitted provided that the following conditions
-# are met:
-# 1. Redistributions of source code must retain the above copyright
-#    notice, this list of conditions and the following disclaimer.
-# 2. Redistributions in binary form must reproduce the above copyright
-#    notice, this list of conditions and the following disclaimer in the
-#    documentation and/or other materials provided with the distribution.
-# 3. The name of the author may not be used to endorse or promote products
-#    derived from this software without specific prior written permission.
-#
-# THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
-# IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-# OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
-# IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
-# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
-# NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
-# THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-#
-# end[licence]
-
-try:
-    set = set
-    frozenset = frozenset
-except NameError:
-    from sets import Set as set, ImmutableSet as frozenset
-
-
-try:
-    reversed = reversed
-except NameError:
-    def reversed(l):
-        l = l[:]
-        l.reverse()
-        return l
-
-
diff --git a/runtime/Python3/antlr3/constants.py b/runtime/Python3/antlr3/constants.py
index bf4a47a..913c5ce 100644
--- a/runtime/Python3/antlr3/constants.py
+++ b/runtime/Python3/antlr3/constants.py
@@ -30,6 +30,8 @@
#
# end[licence]

+compatible_api_versions = ['HEAD', 1]
+
EOF = -1

## All tokens go to the parser (unless skip() is called in that rule)
@@ -51,7 +53,7 @@ DOWN = 2
#imaginary tree navigation type; finish with a child list
UP = 3

-MIN_TOKEN_TYPE = UP+1
+MIN_TOKEN_TYPE = UP + 1

INVALID_TOKEN_TYPE = 0

diff --git a/runtime/Python3/antlr3/debug.py b/runtime/Python3/antlr3/debug.py
index 6668fa5..1008515 100644
--- a/runtime/Python3/antlr3/debug.py
+++ b/runtime/Python3/antlr3/debug.py
@@ -29,8 +29,11 @@
# end[licence]

import socket
-from antlr3 import Parser, TokenStream, RecognitionException, Token
-from antlr3.tree import CommonTreeAdaptor, TreeAdaptor, Tree
+from .exceptions import RecognitionException
+from .recognizers import Parser
+from .streams import TokenStream
+from .tokens import Token
+from .tree import CommonTreeAdaptor, TreeAdaptor, Tree

class DebugParser(Parser):
def __init__(self, stream, state=None, dbg=None, *args, **kwargs):
@@ -38,7 +41,7 @@ class DebugParser(Parser):
if not isinstance(stream, DebugTokenStream):
stream = DebugTokenStream(stream, dbg)

-        super(DebugParser, self).__init__(stream, state, *args, **kwargs)
+        super().__init__(stream, state, *args, **kwargs)

# Who to notify when events in the parser occur.
self._dbg = None
@@ -47,9 +50,9 @@ class DebugParser(Parser):


def setDebugListener(self, dbg):
-	"""Provide a new debug event listener for this parser.  Notify the
+        """Provide a new debug event listener for this parser.  Notify the
input stream too that it should send events to this listener.
-	"""
+        """

if hasattr(self.input, 'dbg'):
self.input.dbg = dbg
@@ -119,10 +122,10 @@ class DebugTokenStream(TokenStream):
b = self.input.index()
self._dbg.consumeToken(t)

-        if b > a+1:
+        if b > a + 1:
# then we consumed more than one token; must be off channel tokens
-            for idx in range(a+1, b):
-                self._dbg.consumeHiddenToken(self.input.get(idx));
+            for idx in range(a + 1, b):
+                self._dbg.consumeHiddenToken(self.input.get(idx))


def consumeInitialHiddenTokens(self):
@@ -218,9 +221,9 @@ class DebugTreeAdaptor(TreeAdaptor):


def createWithPayload(self, payload):
-        if payload.getTokenIndex() < 0:
+        if payload.index < 0:
# could be token conjured up during error recovery
-            return self.createFromType(payload.getType(), payload.getText())
+            return self.createFromType(payload.type, payload.text)

node = self.adaptor.createWithPayload(payload)
self.dbg.createNode(node, payload)
@@ -255,7 +258,7 @@ class DebugTreeAdaptor(TreeAdaptor):


def simulateTreeConstruction(self, t):
-	"""^(A B C): emit create A, create B, add child, ..."""
+        """^(A B C): emit create A, create B, add child, ..."""
self.dbg.createNode(t)
for i in range(self.adaptor.getChildCount(t)):
child = self.adaptor.getChild(t, i)
@@ -328,10 +331,9 @@ class DebugTreeAdaptor(TreeAdaptor):

def setTokenBoundaries(self, t, startToken, stopToken):
self.adaptor.setTokenBoundaries(t, startToken, stopToken)
-        if t is not None and startToken is not None and stopToken is not None:
+        if t and startToken and stopToken:
self.dbg.setTokenBoundaries(
-                t, startToken.getTokenIndex(),
-                stopToken.getTokenIndex())
+                t, startToken.index, stopToken.index)


def getTokenStartIndex(self, t):
@@ -411,7 +413,7 @@ class DebugEventListener(object):
PROTOCOL_VERSION = "2"

def enterRule(self, grammarFileName, ruleName):
-	"""The parser has just entered a rule. No decision has been made about
+        """The parser has just entered a rule. No decision has been made about
which alt is predicted.  This is fired AFTER init actions have been
executed.  Attributes are defined and available etc...
The grammarFileName allows composite grammars to jump around among
@@ -422,25 +424,25 @@ class DebugEventListener(object):


def enterAlt(self, alt):
-	"""Because rules can have lots of alternatives, it is very useful to
+        """Because rules can have lots of alternatives, it is very useful to
know which alt you are entering.  This is 1..n for n alts.
"""
pass


def exitRule(self, grammarFileName, ruleName):
-	"""This is the last thing executed before leaving a rule.  It is
+        """This is the last thing executed before leaving a rule.  It is
executed even if an exception is thrown.  This is triggered after
error reporting and recovery have occurred (unless the exception is
not caught in this rule).  This implies an "exitAlt" event.
The grammarFileName allows composite grammars to jump around among
multiple grammar files.
-	"""
+        """
pass


def enterSubRule(self, decisionNumber):
-	"""Track entry into any (...) subrule other EBNF construct"""
+        """Track entry into any (...) subrule other EBNF construct"""
pass


@@ -449,7 +451,7 @@ class DebugEventListener(object):


def enterDecision(self, decisionNumber, couldBacktrack):
-	"""Every decision, fixed k or arbitrary, has an enter/exit event
+        """Every decision, fixed k or arbitrary, has an enter/exit event
so that a GUI can easily track what LT/consume events are
associated with prediction.  You will see a single enter/exit
subrule but multiple enter/exit decision events, one for each
@@ -463,40 +465,40 @@ class DebugEventListener(object):


def consumeToken(self, t):
-	"""An input token was consumed; matched by any kind of element.
+        """An input token was consumed; matched by any kind of element.
Trigger after the token was matched by things like match(), matchAny().
-	"""
+        """
pass


def consumeHiddenToken(self, t):
-	"""An off-channel input token was consumed.
+        """An off-channel input token was consumed.
Trigger after the token was matched by things like match(), matchAny().
(unless of course the hidden token is first stuff in the input stream).
-	"""
+        """
pass


def LT(self, i, t):
-	"""Somebody (anybody) looked ahead.  Note that this actually gets
+        """Somebody (anybody) looked ahead.  Note that this actually gets
triggered by both LA and LT calls.  The debugger will want to know
which Token object was examined.  Like consumeToken, this indicates
what token was seen at that depth.  A remote debugger cannot look
ahead into a file it doesn't have so LT events must pass the token
even if the info is redundant.
-	"""
+        """
pass


def mark(self, marker):
-	"""The parser is going to look arbitrarily ahead; mark this location,
+        """The parser is going to look arbitrarily ahead; mark this location,
the token stream's marker is sent in case you need it.
-	"""
+        """
pass


def rewind(self, marker=None):
-	"""After an arbitrairly long lookahead as with a cyclic DFA (or with
+        """After an arbitrairly long lookahead as with a cyclic DFA (or with
any backtrack), this informs the debugger that stream should be
rewound to the position associated with marker.

@@ -513,19 +515,19 @@ class DebugEventListener(object):


def location(self, line, pos):
-	"""To watch a parser move through the grammar, the parser needs to
+        """To watch a parser move through the grammar, the parser needs to
inform the debugger what line/charPos it is passing in the grammar.
For now, this does not know how to switch from one grammar to the
other and back for island grammars etc...

This should also allow breakpoints because the debugger can stop
the parser whenever it hits this line/pos.
-	"""
+        """
pass


def recognitionException(self, e):
-	"""A recognition exception occurred such as NoViableAltException.  I made
+        """A recognition exception occurred such as NoViableAltException.  I made
this a generic event so that I can alter the exception hierachy later
without having to alter all the debug objects.

@@ -553,39 +555,39 @@ class DebugEventListener(object):
The sequence for this rule (with no viable alt in the subrule) for
input 'c c' (there are 3 tokens) is:

-		commence
-		LT(1)
-		enterRule b
-		location 7 1
-		enter decision 3
-		LT(1)
-		exit decision 3
-		enterAlt1
-		location 7 5
-		LT(1)
-		consumeToken [c/<4>,1:0]
-		location 7 7
-		enterSubRule 2
-		enter decision 2
-		LT(1)
-		LT(1)
-		recognitionException NoViableAltException 2 1 2
-		exit decision 2
-		exitSubRule 2
-		beginResync
-		LT(1)
-		consumeToken [c/<4>,1:1]
-		LT(1)
-		endResync
-		LT(-1)
-		exitRule b
-		terminate
-	"""
+                commence
+                LT(1)
+                enterRule b
+                location 7 1
+                enter decision 3
+                LT(1)
+                exit decision 3
+                enterAlt1
+                location 7 5
+                LT(1)
+                consumeToken [c/<4>,1:0]
+                location 7 7
+                enterSubRule 2
+                enter decision 2
+                LT(1)
+                LT(1)
+                recognitionException NoViableAltException 2 1 2
+                exit decision 2
+                exitSubRule 2
+                beginResync
+                LT(1)
+                consumeToken [c/<4>,1:1]
+                LT(1)
+                endResync
+                LT(-1)
+                exitRule b
+                terminate
+        """
pass


def beginResync(self):
-	"""Indicates the recognizer is about to consume tokens to resynchronize
+        """Indicates the recognizer is about to consume tokens to resynchronize
the parser.  Any consume events from here until the recovered event
are not part of the parse--they are dead tokens.
"""
@@ -593,30 +595,30 @@ class DebugEventListener(object):


def endResync(self):
-	"""Indicates that the recognizer has finished consuming tokens in order
+        """Indicates that the recognizer has finished consuming tokens in order
to resychronize.  There may be multiple beginResync/endResync pairs
before the recognizer comes out of errorRecovery mode (in which
multiple errors are suppressed).  This will be useful
in a gui where you want to probably grey out tokens that are consumed
but not matched to anything in grammar.  Anything between
a beginResync/endResync pair was tossed out by the parser.
-	"""
+        """
pass


def semanticPredicate(self, result, predicate):
-	"""A semantic predicate was evaluate with this result and action text"""
+        """A semantic predicate was evaluate with this result and action text"""
pass


def commence(self):
-	"""Announce that parsing has begun.  Not technically useful except for
+        """Announce that parsing has begun.  Not technically useful except for
sending events over a socket.  A GUI for example will launch a thread
to connect and communicate with a remote parser.  The thread will want
to notify the GUI when a connection is made.  ANTLR parsers
trigger this upon entry to the first rule (the ruleLevel is used to
figure this out).
-	"""
+        """
pass


@@ -625,7 +627,7 @@ class DebugEventListener(object):
remote debugging listeners that it's time to quit.  When the rule
invocation level goes to zero at the end of a rule, we are done
parsing.
-	"""
+        """
pass


@@ -644,10 +646,10 @@ class DebugEventListener(object):


def LT(self, i, t):
-	"""The tree parser lookedahead.  If the type is UP or DOWN,
+        """The tree parser lookedahead.  If the type is UP or DOWN,
then the ID is not really meaningful as it's fixed--there is
just one UP node and one DOWN navigation node.
-	"""
+        """
pass


@@ -655,7 +657,7 @@ class DebugEventListener(object):
## A S T  E v e n t s

def nilNode(self, t):
-	"""A nil was created (even nil nodes have a unique ID...
+        """A nil was created (even nil nodes have a unique ID...
they are not "null" per se).  As of 4/28/2006, this
seems to be uniquely triggered when starting a new subtree
such as when entering a subrule in automatic mode and when
@@ -663,29 +665,29 @@ class DebugEventListener(object):

If you are receiving this event over a socket via
RemoteDebugEventSocketListener then only t.ID is set.
-	"""
+        """
pass


def errorNode(self, t):
-	"""Upon syntax error, recognizers bracket the error with an error node
+        """Upon syntax error, recognizers bracket the error with an error node
if they are building ASTs.
"""
pass


def createNode(self, node, token=None):
-	"""Announce a new node built from token elements such as type etc...
+        """Announce a new node built from token elements such as type etc...

If you are receiving this event over a socket via
RemoteDebugEventSocketListener then only t.ID, type, text are
set.
-	"""
+        """
pass


def becomeRoot(self, newRoot, oldRoot):
-	"""Make a node the new root of an existing root.
+        """Make a node the new root of an existing root.

Note: the newRootID parameter is possibly different
than the TreeAdaptor.becomeRoot() newRoot parameter.
@@ -700,12 +702,12 @@ class DebugEventListener(object):
RemoteDebugEventSocketListener then only IDs are set.

@see antlr3.tree.TreeAdaptor.becomeRoot()
-	"""
+        """
pass


def addChild(self, root, child):
-	"""Make childID a child of rootID.
+        """Make childID a child of rootID.

If you are receiving this event over a socket via
RemoteDebugEventSocketListener then only IDs are set.
@@ -716,11 +718,11 @@ class DebugEventListener(object):


def setTokenBoundaries(self, t, tokenStartIndex, tokenStopIndex):
-	"""Set the token start/stop token index for a subtree root or node.
+        """Set the token start/stop token index for a subtree root or node.

If you are receiving this event over a socket via
RemoteDebugEventSocketListener then only t.ID is set.
-	"""
+        """
pass


@@ -745,7 +747,7 @@ class TraceDebugEventListener(DebugEventListener):
"""

def __init__(self, adaptor=None):
-        super(TraceDebugEventListener, self).__init__()
+        super().__init__()

if adaptor is None:
adaptor = CommonTreeAdaptor()
@@ -755,10 +757,10 @@ class TraceDebugEventListener(DebugEventListener):
sys.stdout.write(event + '\n')

def enterRule(self, grammarFileName, ruleName):
-        self.record("enterRule "+ruleName)
+        self.record("enterRule " + ruleName)

def exitRule(self, grammarFileName, ruleName):
-        self.record("exitRule "+ruleName)
+        self.record("exitRule " + ruleName)

def enterSubRule(self, decisionNumber):
self.record("enterSubRule")
@@ -767,18 +769,18 @@ class TraceDebugEventListener(DebugEventListener):
self.record("exitSubRule")

def location(self, line, pos):
-        self.record("location %s:%s" % (line, pos))
+        self.record("location {}:{}".format(line, pos))

## Tree parsing stuff

def consumeNode(self, t):
-        self.record("consumeNode %s %s %s" % (
+        self.record("consumeNode {} {} {}".format(
self.adaptor.getUniqueID(t),
self.adaptor.getText(t),
self.adaptor.getType(t)))

def LT(self, i, t):
-        self.record("LT %s %s %s %s" % (
+        self.record("LT {} {} {} {}".format(
i,
self.adaptor.getUniqueID(t),
self.adaptor.getText(t),
@@ -787,32 +789,32 @@ class TraceDebugEventListener(DebugEventListener):

## AST stuff
def nilNode(self, t):
-        self.record("nilNode %s" % self.adaptor.getUniqueID(t))
+        self.record("nilNode {}".format(self.adaptor.getUniqueID(t)))

def createNode(self, t, token=None):
if token is None:
-            self.record("create %s: %s, %s" % (
+            self.record("create {}: {}, {}".format(
self.adaptor.getUniqueID(t),
self.adaptor.getText(t),
self.adaptor.getType(t)))

else:
-            self.record("create %s: %s" % (
+            self.record("create {}: {}".format(
self.adaptor.getUniqueID(t),
-                    token.getTokenIndex()))
+                    token.index))

def becomeRoot(self, newRoot, oldRoot):
-        self.record("becomeRoot %s, %s" % (
+        self.record("becomeRoot {}, {}".format(
self.adaptor.getUniqueID(newRoot),
self.adaptor.getUniqueID(oldRoot)))

def addChild(self, root, child):
-        self.record("addChild %s, %s" % (
+        self.record("addChild {}, {}".format(
self.adaptor.getUniqueID(root),
self.adaptor.getUniqueID(child)))

def setTokenBoundaries(self, t, tokenStartIndex, tokenStopIndex):
-        self.record("setTokenBoundaries %s, %s, %s" % (
+        self.record("setTokenBoundaries {}, {}, {}".format(
self.adaptor.getUniqueID(t),
tokenStartIndex, tokenStopIndex))

@@ -821,7 +823,7 @@ class RecordDebugEventListener(TraceDebugEventListener):
"""A listener that records events as strings in an array."""

def __init__(self, adaptor=None):
-        super(RecordDebugEventListener, self).__init__(adaptor)
+        super().__init__(adaptor)

self.events = []

@@ -839,13 +841,12 @@ class DebugEventSocketProxy(DebugEventListener):

DEFAULT_DEBUGGER_PORT = 49100

-    def __init__(self, recognizer, adaptor=None, port=None,
-                 debug=None):
-        super(DebugEventSocketProxy, self).__init__()
+    def __init__(self, recognizer, adaptor=None, port=None, debug=None):
+        super().__init__()

self.grammarFileName = recognizer.getGrammarFileName()

-	# Almost certainly the recognizer will have adaptor set, but
+        # Almost certainly the recognizer will have adaptor set, but
# we don't know how to cast it (Parser or TreeParser) to get
# the adaptor field.  Must be set with a constructor. :(
self.adaptor = adaptor
@@ -861,7 +862,7 @@ class DebugEventSocketProxy(DebugEventListener):


def log(self, msg):
-        if self.debug is not None:
+        if self.debug:
self.debug.write(msg + '\n')


@@ -872,33 +873,32 @@ class DebugEventSocketProxy(DebugEventListener):
self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
self.socket.bind(('', self.port))
self.socket.listen(1)
-            self.log("Waiting for incoming connection on port %d" % self.port)
+            self.log("Waiting for incoming connection on port {}".format(self.port))

# wait for an incoming connection
self.connection, addr = self.socket.accept()
-            self.log("Accepted connection from %s:%d" % addr)
+            self.log("Accepted connection from {}:{}".format(addr[0], addr[1]))

self.connection.setblocking(1)
self.connection.setsockopt(socket.SOL_TCP, socket.TCP_NODELAY, 1)

-            # FIXME(pink): wrap into utf8 encoding stream
self.output = self.connection.makefile('w', 0)
self.input = self.connection.makefile('r', 0)

-            self.write("ANTLR %s" % self.PROTOCOL_VERSION)
-            self.write("grammar \"%s" % self.grammarFileName)
+            self.write("ANTLR {}".format(self.PROTOCOL_VERSION))
+            self.write('grammar "{}"'.format(self.grammarFileName))
self.ack()


def write(self, msg):
-        self.log("> %s" % msg)
-        self.output.write("%s\n" % msg)
+        self.log("> {}".format(msg))
+        self.output.write("{}\n".format(msg))
self.output.flush()


def ack(self):
t = self.input.readline()
-        self.log("< %s" % t.rstrip())
+        self.log("< {}".format(t.rstrip()))


def transmit(self, event):
@@ -920,40 +920,40 @@ class DebugEventSocketProxy(DebugEventListener):


def enterRule(self, grammarFileName, ruleName):
-        self.transmit("enterRule\t%s\t%s" % (grammarFileName, ruleName))
+        self.transmit("enterRule\t{}\t{}".format(grammarFileName, ruleName))


def enterAlt(self, alt):
-        self.transmit("enterAlt\t%d" % alt)
+        self.transmit("enterAlt\t{}".format(alt))


def exitRule(self, grammarFileName, ruleName):
-        self.transmit("exitRule\t%s\t%s" % (grammarFileName, ruleName))
+        self.transmit("exitRule\t{}\t{}".format(grammarFileName, ruleName))


def enterSubRule(self, decisionNumber):
-        self.transmit("enterSubRule\t%d" % decisionNumber)
+        self.transmit("enterSubRule\t{}".format(decisionNumber))


def exitSubRule(self, decisionNumber):
-        self.transmit("exitSubRule\t%d" % decisionNumber)
+        self.transmit("exitSubRule\t{}".format(decisionNumber))


def enterDecision(self, decisionNumber, couldBacktrack):
self.transmit(
-            "enterDecision\t%d\t%d" % (decisionNumber, couldBacktrack))
+            "enterDecision\t{}\t{}".format(decisionNumber, couldBacktrack))


def exitDecision(self, decisionNumber):
-        self.transmit("exitDecision\t%d" % decisionNumber)
+        self.transmit("exitDecision\t{}".format(decisionNumber))


def consumeToken(self, t):
-        self.transmit("consumeToken\t%s" % self.serializeToken(t))
+        self.transmit("consumeToken\t{}".format(self.serializeToken(t)))


def consumeHiddenToken(self, t):
-        self.transmit("consumeHiddenToken\t%s" % self.serializeToken(t))
+        self.transmit("consumeHiddenToken\t{}".format(self.serializeToken(t)))


def LT(self, i, o):
@@ -964,31 +964,31 @@ class DebugEventSocketProxy(DebugEventListener):

def LT_token(self, i, t):
if t is not None:
-            self.transmit("LT\t%d\t%s" % (i, self.serializeToken(t)))
+            self.transmit("LT\t{}\t{}".format(i, self.serializeToken(t)))


def mark(self, i):
-        self.transmit("mark\t%d" % i)
+        self.transmit("mark\t{}".format(i))


def rewind(self, i=None):
if i is not None:
-            self.transmit("rewind\t%d" % i)
+            self.transmit("rewind\t{}".format(i))
else:
self.transmit("rewind")


def beginBacktrack(self, level):
-        self.transmit("beginBacktrack\t%d" % level)
+        self.transmit("beginBacktrack\t{}".format(level))


def endBacktrack(self, level, successful):
-        self.transmit("endBacktrack\t%d\t%s" % (
-                level, ['0', '1'][bool(successful)]))
+        self.transmit("endBacktrack\t{}\t{}".format(
+                level, '1' if successful else '0'))


def location(self, line, pos):
-        self.transmit("location\t%d\t%d" % (line, pos))
+        self.transmit("location\t{}\t{}".format(line, pos))


def recognitionException(self, exc):
@@ -1065,11 +1065,11 @@ class DebugEventSocketProxy(DebugEventListener):
## A S T  E v e n t s

def nilNode(self, t):
-        self.transmit("nilNode\t%d" % self.adaptor.getUniqueID(t))
+        self.transmit("nilNode\t{}".format(self.adaptor.getUniqueID(t)))


def errorNode(self, t):
-        self.transmit("errorNode\t%d\t%d\t\"%s" % (
+        self.transmit('errorNode\t{}\t{}\t"{}'.format(
self.adaptor.getUniqueID(t),
Token.INVALID_TOKEN_TYPE,
self.escapeNewlines(t.toString())))
@@ -1078,31 +1078,31 @@ class DebugEventSocketProxy(DebugEventListener):

def createNode(self, node, token=None):
if token is not None:
-            self.transmit("createNode\t%d\t%d" % (
+            self.transmit("createNode\t{}\t{}".format(
self.adaptor.getUniqueID(node),
-                    token.getTokenIndex()))
+                    token.index))

else:
-            self.transmit("createNodeFromTokenElements\t%d\t%d\t\"%s" % (
+            self.transmit('createNodeFromTokenElements\t{}\t{}\t"{}'.format(
self.adaptor.getUniqueID(node),
self.adaptor.getType(node),
self.adaptor.getText(node)))


def becomeRoot(self, newRoot, oldRoot):
-        self.transmit("becomeRoot\t%d\t%d" % (
+        self.transmit("becomeRoot\t{}\t{}".format(
self.adaptor.getUniqueID(newRoot),
self.adaptor.getUniqueID(oldRoot)))


def addChild(self, root, child):
-        self.transmit("addChild\t%d\t%d" % (
+        self.transmit("addChild\t{}\t{}".format(
self.adaptor.getUniqueID(root),
self.adaptor.getUniqueID(child)))


def setTokenBoundaries(self, t, tokenStartIndex, tokenStopIndex):
-        self.transmit("setTokenBoundaries\t%d\t%d\t%d" % (
+        self.transmit("setTokenBoundaries\t{}\t{}\t{}".format(
self.adaptor.getUniqueID(t),
tokenStartIndex, tokenStopIndex))

@@ -1118,12 +1118,12 @@ class DebugEventSocketProxy(DebugEventListener):


def serializeToken(self, t):
-        buf = [str(int(t.getTokenIndex())),
-               str(int(t.getType())),
-               str(int(t.getChannel())),
-               str(int(t.getLine() or 0)),
-               str(int(t.getCharPositionInLine() or 0)),
-               '\"' + self.escapeNewlines(t.getText())]
+        buf = [str(int(t.index)),
+               str(int(t.type)),
+               str(int(t.channel)),
+               str(int(t.line or 0)),
+               str(int(t.charPositionInLine or 0)),
+               '"' + self.escapeNewlines(t.text)]
return '\t'.join(buf)


diff --git a/runtime/Python3/antlr3/dfa.py b/runtime/Python3/antlr3/dfa.py
index ff93761..07fdd5b 100644
--- a/runtime/Python3/antlr3/dfa.py
+++ b/runtime/Python3/antlr3/dfa.py
@@ -28,10 +28,10 @@
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
# THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
-# end[licensc]
+# end[licence]

-from antlr3.constants import EOF
-from antlr3.exceptions import NoViableAltException, BacktrackingFailed
+from .constants import EOF
+from .exceptions import NoViableAltException, BacktrackingFailed


class DFA(object):
@@ -64,19 +64,16 @@ class DFA(object):
def predict(self, input):
"""
From the input stream, predict what alternative will succeed
-	using this DFA (representing the covering regular approximation
-	to the underlying CFL).  Return an alternative number 1..n.  Throw
-	 an exception upon error.
-	"""
+        using this DFA (representing the covering regular approximation
+        to the underlying CFL).  Return an alternative number 1..n.  Throw
+        an exception upon error.
+        """
mark = input.mark()
s = 0 # we always start at s0
try:
-            for _ in xrange(50000):
-                #print "***Current state = %d" % s
-
+            for _ in range(50000):
specialState = self.special[s]
if specialState >= 0:
-                    #print "is special"
s = self.specialStateTransition(specialState, input)
if s == -1:
self.noViableAlt(s, input)
@@ -85,29 +82,21 @@ class DFA(object):
continue

if self.accept[s] >= 1:
-                    #print "accept state for alt %d" % self.accept[s]
return self.accept[s]

# look for a normal char transition
c = input.LA(1)

-                #print "LA = %d (%r)" % (c, unichr(c) if c >= 0 else 'EOF')
-                #print "range = %d..%d" % (self.min[s], self.max[s])
-
if c >= self.min[s] and c <= self.max[s]:
# move to next state
snext = self.transition[s][c-self.min[s]]
-                    #print "in range, next state = %d" % snext

if snext < 0:
-                        #print "not a normal transition"
# was in range but not a normal transition
# must check EOT, which is like the else clause.
# eot[s]>=0 indicates that an EOT edge goes to another
# state.
if self.eot[s] >= 0: # EOT Transition to accept state?
-                            #print "EOT trans to accept state %d" % self.eot[s]
-
s = self.eot[s]
input.consume()
# TODO: I had this as return accept[eot[s]]
@@ -117,7 +106,6 @@ class DFA(object):
# target?
continue

-                        #print "no viable alt"
self.noViableAlt(s, input)
return 0

@@ -126,16 +114,12 @@ class DFA(object):
continue

if self.eot[s] >= 0:
-                    #print "EOT to %d" % self.eot[s]
-
s = self.eot[s]
input.consume()
continue

# EOF Transition to accept state?
if c == EOF and self.eof[s] >= 0:
-                    #print "EOF Transition to accept state %d" \
-                    #  % self.accept[self.eof[s]]
return self.accept[self.eof[s]]

# not in range and not EOF/EOT, must be invalid symbol
@@ -181,6 +165,7 @@ class DFA(object):
##         return 0


+    @classmethod
def unpack(cls, string):
"""@brief Unpack the runlength encoded table data.

@@ -199,15 +184,12 @@ class DFA(object):
"""

ret = []
-        for i in range(len(string) / 2):
-            (n, v) = ord(string[i*2]), ord(string[i*2+1])
+        for i in range(0, len(string) - 1, 2):
+            (n, v) = ord(string[i]), ord(string[i + 1])

-            # Is there a bitwise operation to do this?
if v == 0xFFFF:
v = -1

ret += [v] * n

return ret
-
-    unpack = classmethod(unpack)
diff --git a/runtime/Python3/antlr3/dottreegen.py b/runtime/Python3/antlr3/dottreegen.py
deleted file mode 100644
index 827d4ec..0000000
--- a/runtime/Python3/antlr3/dottreegen.py
+++ /dev/null
@@ -1,210 +0,0 @@
-""" @package antlr3.dottreegenerator
-@brief ANTLR3 runtime package, tree module
-
-This module contains all support classes for AST construction and tree parsers.
-
-"""
-
-# begin[licence]
-#
-# [The "BSD licence"]
-# Copyright (c) 2005-2008 Terence Parr
-# All rights reserved.
-#
-# Redistribution and use in source and binary forms, with or without
-# modification, are permitted provided that the following conditions
-# are met:
-# 1. Redistributions of source code must retain the above copyright
-#    notice, this list of conditions and the following disclaimer.
-# 2. Redistributions in binary form must reproduce the above copyright
-#    notice, this list of conditions and the following disclaimer in the
-#    documentation and/or other materials provided with the distribution.
-# 3. The name of the author may not be used to endorse or promote products
-#    derived from this software without specific prior written permission.
-#
-# THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
-# IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-# OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
-# IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
-# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
-# NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
-# THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-#
-# end[licence]
-
-# lot's of docstrings are missing, don't complain for now...
-# pylint: disable-msg=C0111
-
-from antlr3.tree import CommonTreeAdaptor
-import stringtemplate3
-
-class DOTTreeGenerator(object):
-    """
-    A utility class to generate DOT diagrams (graphviz) from
-    arbitrary trees.  You can pass in your own templates and
-    can pass in any kind of tree or use Tree interface method.
-    """
-
-    _treeST = stringtemplate3.StringTemplate(
-        template=(
-        "digraph {\n" +
-        "  ordering=out;\n" +
-        "  ranksep=.4;\n" +
-        "  node [shape=plaintext, fixedsize=true, fontsize=11, fontname=\"Courier\",\n" +
-        "        width=.25, height=.25];\n" +
-        "  edge [arrowsize=.5]\n" +
-        "  $nodes$\n" +
-        "  $edges$\n" +
-        "}\n")
-        )
-
-    _nodeST = stringtemplate3.StringTemplate(
-        template="$name$ [label=\"$text$\"];\n"
-        )
-
-    _edgeST = stringtemplate3.StringTemplate(
-        template="$parent$ -> $child$ // \"$parentText$\" -> \"$childText$\"\n"
-        )
-
-    def __init__(self):
-        ## Track node to number mapping so we can get proper node name back
-        self.nodeToNumberMap = {}
-
-        ## Track node number so we can get unique node names
-        self.nodeNumber = 0
-
-
-    def toDOT(self, tree, adaptor=None, treeST=_treeST, edgeST=_edgeST):
-        if adaptor is None:
-            adaptor = CommonTreeAdaptor()
-
-        treeST = treeST.getInstanceOf()
-
-        self.nodeNumber = 0
-        self.toDOTDefineNodes(tree, adaptor, treeST)
-
-        self.nodeNumber = 0
-        self.toDOTDefineEdges(tree, adaptor, treeST, edgeST)
-        return treeST
-
-
-    def toDOTDefineNodes(self, tree, adaptor, treeST, knownNodes=None):
-        if knownNodes is None:
-            knownNodes = set()
-
-        if tree is None:
-            return
-
-        n = adaptor.getChildCount(tree)
-        if n == 0:
-            # must have already dumped as child from previous
-            # invocation; do nothing
-            return
-
-        # define parent node
-        number = self.getNodeNumber(tree)
-        if number not in knownNodes:
-            parentNodeST = self.getNodeST(adaptor, tree)
-            treeST.setAttribute("nodes", parentNodeST)
-            knownNodes.add(number)
-
-        # for each child, do a "<unique-name> [label=text]" node def
-        for i in range(n):
-            child = adaptor.getChild(tree, i)
-
-            number = self.getNodeNumber(child)
-            if number not in knownNodes:
-                nodeST = self.getNodeST(adaptor, child)
-                treeST.setAttribute("nodes", nodeST)
-                knownNodes.add(number)
-
-            self.toDOTDefineNodes(child, adaptor, treeST, knownNodes)
-
-
-    def toDOTDefineEdges(self, tree, adaptor, treeST, edgeST):
-        if tree is None:
-            return
-
-        n = adaptor.getChildCount(tree)
-        if n == 0:
-            # must have already dumped as child from previous
-            # invocation; do nothing
-            return
-
-        parentName = "n%d" % self.getNodeNumber(tree)
-
-        # for each child, do a parent -> child edge using unique node names
-        parentText = adaptor.getText(tree)
-        for i in range(n):
-            child = adaptor.getChild(tree, i)
-            childText = adaptor.getText(child)
-            childName = "n%d" % self.getNodeNumber(child)
-            edgeST = edgeST.getInstanceOf()
-            edgeST.setAttribute("parent", parentName)
-            edgeST.setAttribute("child", childName)
-            edgeST.setAttribute("parentText", parentText)
-            edgeST.setAttribute("childText", childText)
-            treeST.setAttribute("edges", edgeST)
-            self.toDOTDefineEdges(child, adaptor, treeST, edgeST)
-
-
-    def getNodeST(self, adaptor, t):
-        text = adaptor.getText(t)
-        nodeST = self._nodeST.getInstanceOf()
-        uniqueName = "n%d" % self.getNodeNumber(t)
-        nodeST.setAttribute("name", uniqueName)
-        if text is not None:
-            text = text.replace('"', r'\\"')
-        nodeST.setAttribute("text", text)
-        return nodeST
-
-
-    def getNodeNumber(self, t):
-        try:
-            return self.nodeToNumberMap[t]
-        except KeyError:
-            self.nodeToNumberMap[t] = self.nodeNumber
-            self.nodeNumber += 1
-            return self.nodeNumber - 1
-
-
-def toDOT(tree, adaptor=None, treeST=DOTTreeGenerator._treeST, edgeST=DOTTreeGenerator._edgeST):
-    """
-    Generate DOT (graphviz) for a whole tree not just a node.
-    For example, 3+4*5 should generate:
-
-    digraph {
-        node [shape=plaintext, fixedsize=true, fontsize=11, fontname="Courier",
-            width=.4, height=.2];
-        edge [arrowsize=.7]
-        "+"->3
-        "+"->"*"
-        "*"->4
-        "*"->5
-    }
-
-    Return the ST not a string in case people want to alter.
-
-    Takes a Tree interface object.
-
-    Example of invokation:
-
-        import antlr3
-        import antlr3.extras
-
-        input = antlr3.ANTLRInputStream(sys.stdin)
-        lex = TLexer(input)
-        tokens = antlr3.CommonTokenStream(lex)
-        parser = TParser(tokens)
-        tree = parser.e().tree
-        print tree.toStringTree()
-        st = antlr3.extras.toDOT(t)
-        print st
-
-    """
-
-    gen = DOTTreeGenerator()
-    return gen.toDOT(tree, adaptor, treeST, edgeST)
diff --git a/runtime/Python3/antlr3/exceptions.py b/runtime/Python3/antlr3/exceptions.py
index 97b1074..901b6e4 100644
--- a/runtime/Python3/antlr3/exceptions.py
+++ b/runtime/Python3/antlr3/exceptions.py
@@ -30,7 +30,7 @@
#
# end[licence]

-from antlr3.constants import INVALID_TOKEN_TYPE
+from .constants import INVALID_TOKEN_TYPE


class BacktrackingFailed(Exception):
@@ -73,29 +73,29 @@ class RecognitionException(Exception):
"""

def __init__(self, input=None):
-        Exception.__init__(self)
+        super().__init__()

-	# What input stream did the error occur in?
+        # What input stream did the error occur in?
self.input = None

# What is index of token/char were we looking at when the error
# occurred?
self.index = None

-	# The current Token when an error occurred.  Since not all streams
-	# can retrieve the ith Token, we have to track the Token object.
-	# For parsers.  Even when it's a tree parser, token might be set.
+        # The current Token when an error occurred.  Since not all streams
+        # can retrieve the ith Token, we have to track the Token object.
+        # For parsers.  Even when it's a tree parser, token might be set.
self.token = None

-	# If this is a tree parser exception, node is set to the node with
-	# the problem.
+        # If this is a tree parser exception, node is set to the node with
+        # the problem.
self.node = None

-	# The current char when an error occurred. For lexers.
+        # The current char when an error occurred. For lexers.
self.c = None

-	# Track the line at which the error occurred in case this is
-	# generated from a lexer.  We need to track this since the
+        # Track the line at which the error occurred in case this is
+        # generated from a lexer.  We need to track this since the
# unexpected char doesn't carry the line info.
self.line = None

@@ -108,13 +108,13 @@ class RecognitionException(Exception):
self.approximateLineInfo = False


-        if input is not None:
+        if input:
self.input = input
self.index = input.index()

# late import to avoid cyclic dependencies
-            from antlr3.streams import TokenStream, CharStream
-            from antlr3.tree import TreeNodeStream
+            from .streams import TokenStream, CharStream
+            from .tree import TreeNodeStream

if isinstance(self.input, TokenStream):
self.token = self.input.LT(1)
@@ -134,21 +134,21 @@ class RecognitionException(Exception):
self.c = self.input.LA(1)

def extractInformationFromTreeNodeStream(self, nodes):
-        from antlr3.tree import Tree, CommonTree
-        from antlr3.tokens import CommonToken
+        from .tree import Tree, CommonTree
+        from .tokens import CommonToken

self.node = nodes.LT(1)
adaptor = nodes.adaptor
payload = adaptor.getToken(self.node)
-        if payload is not None:
+        if payload:
self.token = payload
if payload.line <= 0:
# imaginary node; no line/pos info; scan backwards
i = -1
priorNode = nodes.LT(i)
-                while priorNode is not None:
+                while priorNode:
priorPayload = adaptor.getToken(priorNode)
-                    if priorPayload is not None and priorPayload.line > 0:
+                    if priorPayload and priorPayload.line > 0:
# we found the most recent real line / pos info
self.line = priorPayload.line
self.charPositionInLine = priorPayload.charPositionInLine
@@ -177,8 +177,8 @@ class RecognitionException(Exception):
def getUnexpectedType(self):
"""Return the token type or char of the unexpected input element"""

-        from antlr3.streams import TokenStream
-        from antlr3.tree import TreeNodeStream
+        from .streams import TokenStream
+        from .tree import TreeNodeStream

if isinstance(self.input, TokenStream):
return self.token.type
@@ -197,13 +197,12 @@ class MismatchedTokenException(RecognitionException):
"""@brief A mismatched char or Token or tree node."""

def __init__(self, expecting, input):
-        RecognitionException.__init__(self, input)
+        super().__init__(input)
self.expecting = expecting


def __str__(self):
-        #return "MismatchedTokenException("+self.expecting+")"
-        return "MismatchedTokenException(%r!=%r)" % (
+        return "MismatchedTokenException({!r}!={!r})".format(
self.getUnexpectedType(), self.expecting
)
__repr__ = __str__
@@ -217,14 +216,14 @@ class UnwantedTokenException(MismatchedTokenException):


def __str__(self):
-        exp = ", expected %s" % self.expecting
+        exp = ", expected {}".format(self.expecting)
if self.expecting == INVALID_TOKEN_TYPE:
exp = ""

-        if self.token is None:
-            return "UnwantedTokenException(found=%s%s)" % (None, exp)
+        if not self.token:
+            return "UnwantedTokenException(found={}{})".format(None, exp)

-        return "UnwantedTokenException(found=%s%s)" % (self.token.text, exp)
+        return "UnwantedTokenException(found={}{})".format(self.token.text, exp)
__repr__ = __str__


@@ -235,7 +234,7 @@ class MissingTokenException(MismatchedTokenException):
"""

def __init__(self, expecting, input, inserted):
-        MismatchedTokenException.__init__(self, expecting, input)
+        super().__init__(expecting, input)

self.inserted = inserted

@@ -245,12 +244,12 @@ class MissingTokenException(MismatchedTokenException):


def __str__(self):
-        if self.inserted is not None and self.token is not None:
-            return "MissingTokenException(inserted %r at %r)" % (
-                self.inserted, self.token.text)
+        if self.token:
+            if self.inserted:
+                return "MissingTokenException(inserted {!r} at {!r})".format(
+                    self.inserted, self.token.text)

-        if self.token is not None:
-            return "MissingTokenException(at %r)" % self.token.text
+            return "MissingTokenException(at {!r})".format(self.token.text)

return "MissingTokenException"
__repr__ = __str__
@@ -260,14 +259,14 @@ class MismatchedRangeException(RecognitionException):
"""@brief The next token does not match a range of expected types."""

def __init__(self, a, b, input):
-        RecognitionException.__init__(self, input)
+        super().__init__(input)

self.a = a
self.b = b


def __str__(self):
-        return "MismatchedRangeException(%r not in [%r..%r])" % (
+        return "MismatchedRangeException({!r} not in [{!r}..{!r}])".format(
self.getUnexpectedType(), self.a, self.b
)
__repr__ = __str__
@@ -277,13 +276,13 @@ class MismatchedSetException(RecognitionException):
"""@brief The next token does not match a set of expected types."""

def __init__(self, expecting, input):
-        RecognitionException.__init__(self, input)
+        super().__init__(input)

self.expecting = expecting


def __str__(self):
-        return "MismatchedSetException(%r not in %r)" % (
+        return "MismatchedSetException({!r} not in {!r})".format(
self.getUnexpectedType(), self.expecting
)
__repr__ = __str__
@@ -293,7 +292,7 @@ class MismatchedNotSetException(MismatchedSetException):
"""@brief Used for remote debugger deserialization"""

def __str__(self):
-        return "MismatchedNotSetException(%r!=%r)" % (
+        return "MismatchedNotSetException({!r}!={!r})".format(
self.getUnexpectedType(), self.expecting
)
__repr__ = __str__
@@ -305,7 +304,7 @@ class NoViableAltException(RecognitionException):
def __init__(
self, grammarDecisionDescription, decisionNumber, stateNumber, input
):
-        RecognitionException.__init__(self, input)
+        super().__init__(input)

self.grammarDecisionDescription = grammarDecisionDescription
self.decisionNumber = decisionNumber
@@ -313,7 +312,7 @@ class NoViableAltException(RecognitionException):


def __str__(self):
-        return "NoViableAltException(%r!=[%r])" % (
+        return "NoViableAltException({!r}!=[{!r}])".format(
self.unexpectedType, self.grammarDecisionDescription
)
__repr__ = __str__
@@ -323,7 +322,7 @@ class EarlyExitException(RecognitionException):
"""@brief The recognizer did not match anything for a (..)+ loop."""

def __init__(self, decisionNumber, input):
-        RecognitionException.__init__(self, input)
+        super().__init__(input)

self.decisionNumber = decisionNumber

@@ -338,14 +337,15 @@ class FailedPredicateException(RecognitionException):
"""

def __init__(self, input, ruleName, predicateText):
-        RecognitionException.__init__(self, input)
+        super().__init__(input)

self.ruleName = ruleName
self.predicateText = predicateText


def __str__(self):
-        return "FailedPredicateException("+self.ruleName+",{"+self.predicateText+"}?)"
+        return "FailedPredicateException({},{{{}}}?)".format(
+            self.ruleName, self.predicateText)
__repr__ = __str__


@@ -353,12 +353,12 @@ class MismatchedTreeNodeException(RecognitionException):
"""@brief The next tree mode does not match the expected type."""

def __init__(self, expecting, input):
-        RecognitionException.__init__(self, input)
+        super().__init__(input)

self.expecting = expecting

def __str__(self):
-        return "MismatchedTreeNodeException(%r!=%r)" % (
+        return "MismatchedTreeNodeException({!r}!={!r})".format(
self.getUnexpectedType(), self.expecting
)
__repr__ = __str__
diff --git a/runtime/Python3/antlr3/extras.py b/runtime/Python3/antlr3/extras.py
deleted file mode 100644
index 9155cda..0000000
--- a/runtime/Python3/antlr3/extras.py
+++ /dev/null
@@ -1,47 +0,0 @@
-""" @package antlr3.dottreegenerator
-@brief ANTLR3 runtime package, tree module
-
-This module contains all support classes for AST construction and tree parsers.
-
-"""
-
-# begin[licence]
-#
-# [The "BSD licence"]
-# Copyright (c) 2005-2008 Terence Parr
-# All rights reserved.
-#
-# Redistribution and use in source and binary forms, with or without
-# modification, are permitted provided that the following conditions
-# are met:
-# 1. Redistributions of source code must retain the above copyright
-#    notice, this list of conditions and the following disclaimer.
-# 2. Redistributions in binary form must reproduce the above copyright
-#    notice, this list of conditions and the following disclaimer in the
-#    documentation and/or other materials provided with the distribution.
-# 3. The name of the author may not be used to endorse or promote products
-#    derived from this software without specific prior written permission.
-#
-# THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
-# IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-# OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
-# IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
-# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
-# NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
-# THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-#
-# end[licence]
-
-# lot's of docstrings are missing, don't complain for now...
-# pylint: disable-msg=C0111
-
-from treewizard import TreeWizard
-
-try:
-    from antlr3.dottreegen import toDOT
-except ImportError, exc:
-    def toDOT(*args, **kwargs):
-        raise exc
diff --git a/runtime/Python3/antlr3/main.py b/runtime/Python3/antlr3/main.py
index ae3906f..d474836 100644
--- a/runtime/Python3/antlr3/main.py
+++ b/runtime/Python3/antlr3/main.py
@@ -32,9 +32,11 @@


import sys
-import optparse
+import argparse

-import antlr3
+from .streams import ANTLRStringStream, ANTLRFileStream, \
+     ANTLRInputStream, CommonTokenStream
+from .tree import CommonTreeNodeStream


class _Main(object):
@@ -44,100 +46,60 @@ class _Main(object):
self.stderr = sys.stderr


-    def parseOptions(self, argv):
-        optParser = optparse.OptionParser()
-        optParser.add_option(
-            "--encoding",
-            action="store",
-            type="string",
-            dest="encoding"
-            )
-        optParser.add_option(
-            "--input",
-            action="store",
-            type="string",
-            dest="input"
-            )
-        optParser.add_option(
-            "--interactive", "-i",
-            action="store_true",
-            dest="interactive"
-            )
-        optParser.add_option(
-            "--no-output",
-            action="store_true",
-            dest="no_output"
-            )
-        optParser.add_option(
-            "--profile",
-            action="store_true",
-            dest="profile"
-            )
-        optParser.add_option(
-            "--hotshot",
-            action="store_true",
-            dest="hotshot"
-            )
-        optParser.add_option(
-            "--port",
-            type="int",
-            dest="port",
-            default=None
-            )
-        optParser.add_option(
-            "--debug-socket",
-            action='store_true',
-            dest="debug_socket",
-            default=None
-            )
-
-        self.setupOptions(optParser)
-
-        return optParser.parse_args(argv[1:])
-
-
-    def setupOptions(self, optParser):
+    def parseArgs(self, argv):
+        argParser = argparse.ArgumentParser()
+        argParser.add_argument("--input")
+        argParser.add_argument("--interactive", "-i", action="store_true")
+        argParser.add_argument("--no-output", action="store_true")
+        argParser.add_argument("--profile", action="store_true")
+        argParser.add_argument("--hotshot", action="store_true")
+        argParser.add_argument("--port", type=int)
+        argParser.add_argument("--debug-socket", action='store_true')
+        argParser.add_argument("file", nargs='?')
+
+        self.setupArgs(argParser)
+
+        return argParser.parse_args(argv[1:])
+
+
+    def setupArgs(self, argParser):
pass


def execute(self, argv):
-        options, args = self.parseOptions(argv)
+        args = self.parseArgs(argv)

-        self.setUp(options)
+        self.setUp(args)

-        if options.interactive:
+        if args.interactive:
while True:
try:
-                    input = raw_input(">>> ")
+                    input_str = input(">>> ")
except (EOFError, KeyboardInterrupt):
self.stdout.write("\nBye.\n")
break

-                inStream = antlr3.ANTLRStringStream(input)
-                self.parseStream(options, inStream)
+                inStream = ANTLRStringStream(input_str)
+                self.parseStream(args, inStream)

else:
-            if options.input is not None:
-                inStream = antlr3.ANTLRStringStream(options.input)
+            if args.input:
+                inStream = ANTLRStringStream(args.input)

-            elif len(args) == 1 and args[0] != '-':
-                inStream = antlr3.ANTLRFileStream(
-                    args[0], encoding=options.encoding
-                    )
+            elif args.file and args.file != '-':
+                inStream = ANTLRFileStream(args.file)

else:
-                inStream = antlr3.ANTLRInputStream(
-                    self.stdin, encoding=options.encoding
-                    )
+                inStream = ANTLRInputStream(self.stdin)

-            if options.profile:
+            if args.profile:
try:
import cProfile as profile
except ImportError:
import profile

profile.runctx(
-                    'self.parseStream(options, inStream)',
+                    'self.parseStream(args, inStream)',
globals(),
locals(),
'profile.dat'
@@ -149,157 +111,124 @@ class _Main(object):
stats.sort_stats('time')
stats.print_stats(100)

-            elif options.hotshot:
+            elif args.hotshot:
import hotshot

profiler = hotshot.Profile('hotshot.dat')
profiler.runctx(
-                    'self.parseStream(options, inStream)',
+                    'self.parseStream(args, inStream)',
globals(),
locals()
)

else:
-                self.parseStream(options, inStream)
+                self.parseStream(args, inStream)


-    def setUp(self, options):
+    def setUp(self, args):
pass


-    def parseStream(self, options, inStream):
+    def parseStream(self, args, inStream):
raise NotImplementedError


-    def write(self, options, text):
-        if not options.no_output:
+    def write(self, args, text):
+        if not args.no_output:
self.stdout.write(text)


-    def writeln(self, options, text):
-        self.write(options, text + '\n')
+    def writeln(self, args, text):
+        self.write(args, text + '\n')


class LexerMain(_Main):
def __init__(self, lexerClass):
-        _Main.__init__(self)
+        super().__init__()

self.lexerClass = lexerClass


-    def parseStream(self, options, inStream):
+    def parseStream(self, args, inStream):
lexer = self.lexerClass(inStream)
for token in lexer:
-            self.writeln(options, str(token))
+            self.writeln(args, str(token))


class ParserMain(_Main):
def __init__(self, lexerClassName, parserClass):
-        _Main.__init__(self)
+        super().__init__()

self.lexerClassName = lexerClassName
self.lexerClass = None
self.parserClass = parserClass


-    def setupOptions(self, optParser):
-        optParser.add_option(
-            "--lexer",
-            action="store",
-            type="string",
-            dest="lexerClass",
-            default=self.lexerClassName
-            )
-        optParser.add_option(
-            "--rule",
-            action="store",
-            type="string",
-            dest="parserRule"
-            )
+    def setupArgs(self, argParser):
+        argParser.add_argument("--lexer", dest="lexerClass",
+                               default=self.lexerClassName)
+        argParser.add_argument("--rule", dest="parserRule")


-    def setUp(self, options):
-        lexerMod = __import__(options.lexerClass)
-        self.lexerClass = getattr(lexerMod, options.lexerClass)
+    def setUp(self, args):
+        lexerMod = __import__(args.lexerClass)
+        self.lexerClass = getattr(lexerMod, args.lexerClass)


-    def parseStream(self, options, inStream):
+    def parseStream(self, args, inStream):
kwargs = {}
-        if options.port is not None:
-            kwargs['port'] = options.port
-        if options.debug_socket is not None:
+        if args.port is not None:
+            kwargs['port'] = args.port
+        if args.debug_socket:
kwargs['debug_socket'] = sys.stderr

lexer = self.lexerClass(inStream)
-        tokenStream = antlr3.CommonTokenStream(lexer)
+        tokenStream = CommonTokenStream(lexer)
parser = self.parserClass(tokenStream, **kwargs)
-        result = getattr(parser, options.parserRule)()
-        if result is not None:
-            if hasattr(result, 'tree') and result.tree is not None:
-                self.writeln(options, result.tree.toStringTree())
+        result = getattr(parser, args.parserRule)()
+        if result:
+            if hasattr(result, 'tree') and result.tree:
+                self.writeln(args, result.tree.toStringTree())
else:
-                self.writeln(options, repr(result))
+                self.writeln(args, repr(result))


class WalkerMain(_Main):
def __init__(self, walkerClass):
-        _Main.__init__(self)
+        super().__init__()

self.lexerClass = None
self.parserClass = None
self.walkerClass = walkerClass


-    def setupOptions(self, optParser):
-        optParser.add_option(
-            "--lexer",
-            action="store",
-            type="string",
-            dest="lexerClass",
-            default=None
-            )
-        optParser.add_option(
-            "--parser",
-            action="store",
-            type="string",
-            dest="parserClass",
-            default=None
-            )
-        optParser.add_option(
-            "--parser-rule",
-            action="store",
-            type="string",
-            dest="parserRule",
-            default=None
-            )
-        optParser.add_option(
-            "--rule",
-            action="store",
-            type="string",
-            dest="walkerRule"
-            )
-
-
-    def setUp(self, options):
-        lexerMod = __import__(options.lexerClass)
-        self.lexerClass = getattr(lexerMod, options.lexerClass)
-        parserMod = __import__(options.parserClass)
-        self.parserClass = getattr(parserMod, options.parserClass)
-
-
-    def parseStream(self, options, inStream):
+    def setupArgs(self, argParser):
+        argParser.add_argument("--lexer", dest="lexerClass")
+        argParser.add_argument("--parser", dest="parserClass")
+        argParser.add_argument("--parser-rule", dest="parserRule")
+        argParser.add_argument("--rule", dest="walkerRule")
+
+
+    def setUp(self, args):
+        lexerMod = __import__(args.lexerClass)
+        self.lexerClass = getattr(lexerMod, args.lexerClass)
+        parserMod = __import__(args.parserClass)
+        self.parserClass = getattr(parserMod, args.parserClass)
+
+
+    def parseStream(self, args, inStream):
lexer = self.lexerClass(inStream)
-        tokenStream = antlr3.CommonTokenStream(lexer)
+        tokenStream = CommonTokenStream(lexer)
parser = self.parserClass(tokenStream)
-        result = getattr(parser, options.parserRule)()
-        if result is not None:
+        result = getattr(parser, args.parserRule)()
+        if result:
assert hasattr(result, 'tree'), "Parser did not return an AST"
-            nodeStream = antlr3.tree.CommonTreeNodeStream(result.tree)
+            nodeStream = CommonTreeNodeStream(result.tree)
nodeStream.setTokenStream(tokenStream)
walker = self.walkerClass(nodeStream)
-            result = getattr(walker, options.walkerRule)()
-            if result is not None:
+            result = getattr(walker, args.walkerRule)()
+            if result:
if hasattr(result, 'tree'):
-                    self.writeln(options, result.tree.toStringTree())
+                    self.writeln(args, result.tree.toStringTree())
else:
-                    self.writeln(options, repr(result))
+                    self.writeln(args, repr(result))
diff --git a/runtime/Python3/antlr3/recognizers.py b/runtime/Python3/antlr3/recognizers.py
index d48280a..d9bc761 100644
--- a/runtime/Python3/antlr3/recognizers.py
+++ b/runtime/Python3/antlr3/recognizers.py
@@ -33,16 +33,14 @@
import sys
import inspect

-from antlr3 import compatible_api_versions
-from antlr3.constants import DEFAULT_CHANNEL, HIDDEN_CHANNEL, EOF, \
-     EOR_TOKEN_TYPE, INVALID_TOKEN_TYPE
-from antlr3.exceptions import RecognitionException, MismatchedTokenException, \
+from .constants import compatible_api_versions, DEFAULT_CHANNEL, \
+     HIDDEN_CHANNEL, EOF, EOR_TOKEN_TYPE, INVALID_TOKEN_TYPE
+from .exceptions import RecognitionException, MismatchedTokenException, \
MismatchedRangeException, MismatchedTreeNodeException, \
NoViableAltException, EarlyExitException, MismatchedSetException, \
MismatchedNotSetException, FailedPredicateException, \
BacktrackingFailed, UnwantedTokenException, MissingTokenException
-from antlr3.tokens import CommonToken, SKIP_TOKEN
-from antlr3.compat import set, frozenset, reversed
+from .tokens import CommonToken, SKIP_TOKEN


class RecognizerSharedState(object):
@@ -76,7 +74,7 @@ class RecognizerSharedState(object):
# If >0 then it's the level of backtracking.
self.backtracking = 0

-        # An array[size num rules] of Map<Integer,Integer> that tracks
+        # An array[size num rules] of (int -> int) dicts that tracks
# the stop token index for each rule.  ruleMemo[ruleIndex] is
# the memoization table for ruleIndex.  For key ruleStartIndex, you
# get back the stop token for associated rule or MEMO_RULE_FAILED.
@@ -92,7 +90,7 @@ class RecognizerSharedState(object):
# constantly in generated code and Lexer object) :(


-	## The goal of all lexer rules/methods is to create a token object.
+        ## The goal of all lexer rules/methods is to create a token object.
# This is an instance variable as multiple rules may collaborate to
# create a single token.  nextToken will return this object after
# matching lexer rule(s).  If you subclass to allow multiple token
@@ -164,10 +162,10 @@ class BaseRecognizer(object):

if self.api_version not in compatible_api_versions:
raise RuntimeError(
-                ("ANTLR version mismatch: "
-                 "The recognizer has been generated with API V%s, "
-                 "but this runtime does not support this.")
-                % self.api_version)
+                "ANTLR version mismatch: "
+                "The recognizer has been generated with API V{}, "
+                "but this runtime does not support this."
+                .format(self.api_version))

# this one only exists to shut up pylint :(
def setInput(self, input):
@@ -176,7 +174,7 @@ class BaseRecognizer(object):

def reset(self):
"""
-        reset the parser's state; subclasses must rewinds the input stream
+        reset the parser's state; subclasses must rewind the input stream
"""

# wack everything related to error recovery
@@ -222,7 +220,7 @@ class BaseRecognizer(object):
return matchedSymbol


-    def matchAny(self, input):
+    def matchAny(self):
"""Match the wildcard: in a symbol"""

self._state.errorRecovery = False
@@ -242,11 +240,11 @@ class BaseRecognizer(object):
# compute what can follow this grammar element reference
if EOR_TOKEN_TYPE in follow:
viableTokensFollowingThisRule = self.computeContextSensitiveRuleFOLLOW()
-            follow = follow | viableTokensFollowingThisRule
+            follow |= viableTokensFollowingThisRule

if len(self._state.following) > 0:
# remove EOR if we're not the start symbol
-                follow = follow - set([EOR_TOKEN_TYPE])
+                follow -= {EOR_TOKEN_TYPE}

# if current token is consistent with what could come after set
# then we know we're missing a token; error recovery is free to
@@ -267,7 +265,7 @@ class BaseRecognizer(object):

1. error occurs
2. enter recovery mode, report error
-        3. consume until token found in resynch set
+        3. consume until token found in resync set
4. try to resume parsing
5. next match() will reset errorRecovery mode

@@ -284,16 +282,16 @@ class BaseRecognizer(object):
self._state.syntaxErrors += 1 # don't count spurious
self._state.errorRecovery = True

-        self.displayRecognitionError(self.tokenNames, e)
+        self.displayRecognitionError(e)


-    def displayRecognitionError(self, tokenNames, e):
+    def displayRecognitionError(self, e):
hdr = self.getErrorHeader(e)
-        msg = self.getErrorMessage(e, tokenNames)
-        self.emitErrorMessage(hdr+" "+msg)
+        msg = self.getErrorMessage(e)
+        self.emitErrorMessage(hdr + " " + msg)


-    def getErrorMessage(self, e, tokenNames):
+    def getErrorMessage(self, e):
"""
What error message should be generated for the various
exception types?
@@ -319,78 +317,71 @@ class BaseRecognizer(object):
"""

if isinstance(e, UnwantedTokenException):
-            tokenName = "<unknown>"
if e.expecting == EOF:
tokenName = "EOF"
-
else:
tokenName = self.tokenNames[e.expecting]

-            msg = "extraneous input %s expecting %s" % (
+            msg = "extraneous input {} expecting {}".format(
self.getTokenErrorDisplay(e.getUnexpectedToken()),
tokenName
)

elif isinstance(e, MissingTokenException):
-            tokenName = "<unknown>"
if e.expecting == EOF:
tokenName = "EOF"
-
else:
tokenName = self.tokenNames[e.expecting]

-            msg = "missing %s at %s" % (
+            msg = "missing {} at {}".format(
tokenName, self.getTokenErrorDisplay(e.token)
)

elif isinstance(e, MismatchedTokenException):
-            tokenName = "<unknown>"
if e.expecting == EOF:
tokenName = "EOF"
else:
tokenName = self.tokenNames[e.expecting]

-            msg = "mismatched input " \
-                  + self.getTokenErrorDisplay(e.token) \
-                  + " expecting " \
-                  + tokenName
+            msg = "mismatched input {} expecting {}".format(
+                self.getTokenErrorDisplay(e.token),
+                tokenName
+                )

elif isinstance(e, MismatchedTreeNodeException):
-            tokenName = "<unknown>"
if e.expecting == EOF:
tokenName = "EOF"
else:
tokenName = self.tokenNames[e.expecting]

-            msg = "mismatched tree node: %s expecting %s" \
-                  % (e.node, tokenName)
+            msg = "mismatched tree node: {} expecting {}".format(
+                e.node, tokenName)

elif isinstance(e, NoViableAltException):
-            msg = "no viable alternative at input " \
-                  + self.getTokenErrorDisplay(e.token)
+            msg = "no viable alternative at input {}".format(
+                self.getTokenErrorDisplay(e.token))

elif isinstance(e, EarlyExitException):
-            msg = "required (...)+ loop did not match anything at input " \
-                  + self.getTokenErrorDisplay(e.token)
+            msg = "required (...)+ loop did not match anything at input {}".format(
+                self.getTokenErrorDisplay(e.token))

elif isinstance(e, MismatchedSetException):
-            msg = "mismatched input " \
-                  + self.getTokenErrorDisplay(e.token) \
-                  + " expecting set " \
-                  + repr(e.expecting)
+            msg = "mismatched input {} expecting set {!r}".format(
+                self.getTokenErrorDisplay(e.token),
+                e.expecting
+                )

elif isinstance(e, MismatchedNotSetException):
-            msg = "mismatched input " \
-                  + self.getTokenErrorDisplay(e.token) \
-                  + " expecting set " \
-                  + repr(e.expecting)
+            msg = "mismatched input {} expecting set {!r}".format(
+                self.getTokenErrorDisplay(e.token),
+                e.expecting
+                )

elif isinstance(e, FailedPredicateException):
-            msg = "rule " \
-                  + e.ruleName \
-                  + " failed predicate: {" \
-                  + e.predicateText \
-                  + "}?"
+            msg = "rule {} failed predicate: {{{}}}?".format(
+                e.ruleName,
+                e.predicateText
+                )

else:
msg = str(e)
@@ -403,10 +394,10 @@ class BaseRecognizer(object):
Get number of recognition errors (lexer, parser, tree parser).  Each
recognizer tracks its own number.  So parser and lexer each have
separate count.  Does not count the spurious errors found between
-        an error and next valid token match
+        an error and next valid token match.

-        See also reportError()
-	"""
+        See also reportError().
+        """
return self._state.syntaxErrors


@@ -417,8 +408,8 @@ class BaseRecognizer(object):

source_name = self.getSourceName()
if source_name is not None:
-            return "%s line %d:%d" % (source_name, e.line, e.charPositionInLine)
-        return "line %d:%d" % (e.line, e.charPositionInLine)
+            return "{} line {}:{}".format(source_name, e.line, e.charPositionInLine)
+        return "line {}:{}".format(e.line, e.charPositionInLine)


def getTokenErrorDisplay(self, t):
@@ -437,7 +428,7 @@ class BaseRecognizer(object):
if t.type == EOF:
s = "<EOF>"
else:
-                s = "<"+t.type+">"
+                s = "<{}>".format(t.typeName)

return repr(s)

@@ -619,7 +610,7 @@ class BaseRecognizer(object):

The FOLLOW sets are all inclusive whereas context-sensitive
FOLLOW sets are precisely what could follow a rule reference.
-        For input input "i=(3);", here is the derivation:
+        For input "i=(3);", here is the derivation:

stat => ID '=' expr ';'
=> ID '=' atom ('+' atom)* ';'
@@ -781,22 +772,6 @@ class BaseRecognizer(object):
return None


-##     def recoverFromMissingElement(self, input, e, follow):
-##         """
-##         This code is factored out from mismatched token and mismatched set
-##         recovery.  It handles "single token insertion" error recovery for
-##         both.  No tokens are consumed to recover from insertions.  Return
-##         true if recovery was possible else return false.
-##         """
-
-##         if self.mismatchIsMissingToken(input, follow):
-##             self.reportError(e)
-##             return True
-
-##         # nothing to do; throw exception
-##         return False
-
-
def consumeUntil(self, input, tokenTypes):
"""
Consume tokens until one matches the given token or token set
@@ -839,6 +814,7 @@ class BaseRecognizer(object):
return self._getRuleInvocationStack(self.__module__)


+    @classmethod
def _getRuleInvocationStack(cls, module):
"""
A more general version of getRuleInvocationStack where you can
@@ -873,8 +849,6 @@ class BaseRecognizer(object):

return rules

-    _getRuleInvocationStack = classmethod(_getRuleInvocationStack)
-

def getBacktrackingLevel(self):
return self._state.backtracking
@@ -899,7 +873,7 @@ class BaseRecognizer(object):
def toStrings(self, tokens):
"""A convenience method for use most often with template rewrites.

-        Convert a List<Token> to List<String>
+        Convert a Token list to a str list.
"""

if tokens is None:
@@ -966,19 +940,19 @@ class BaseRecognizer(object):


def traceIn(self, ruleName, ruleIndex, inputSymbol):
-        sys.stdout.write("enter %s %s" % (ruleName, inputSymbol))
+        sys.stdout.write("enter {} {}".format(ruleName, inputSymbol))

if self._state.backtracking > 0:
-            sys.stdout.write(" backtracking=%s" % self._state.backtracking)
+            sys.stdout.write(" backtracking={}".format(self._state.backtracking))

sys.stdout.write('\n')


def traceOut(self, ruleName, ruleIndex, inputSymbol):
-        sys.stdout.write("exit %s %s" % (ruleName, inputSymbol))
+        sys.stdout.write("exit {} {}".format(ruleName, inputSymbol))

if self._state.backtracking > 0:
-            sys.stdout.write(" backtracking=%s" % self._state.backtracking)
+            sys.stdout.write(" backtracking={}".format(self._state.backtracking))

# mmmm... we use BacktrackingFailed exceptions now. So how could we
# get that information here?
@@ -1022,14 +996,14 @@ class TokenSource(object):
"""The TokenSource is an interator.

The iteration will not include the final EOF token, see also the note
-        for the next() method.
+        for the __next__() method.

"""

return self


-    def next(self):
+    def __next__(self):
"""Return next token or raise StopIteration.

Note that this will raise StopIteration when hitting the EOF token,
@@ -1062,7 +1036,7 @@ class Lexer(BaseRecognizer, TokenSource):


def reset(self):
-        BaseRecognizer.reset(self) # reset all recognizer state variables
+        super().reset() # reset all recognizer state variables

if self.input is not None:
# rewind the input
@@ -1118,11 +1092,11 @@ class Lexer(BaseRecognizer, TokenSource):

return self._state.token

-            except NoViableAltException, re:
+            except NoViableAltException as re:
self.reportError(re)
self.recover(re) # throw out current char and try again

-            except RecognitionException, re:
+            except RecognitionException as re:
self.reportError(re)
# match() routine has already called recover()

@@ -1187,7 +1161,7 @@ class Lexer(BaseRecognizer, TokenSource):


def match(self, s):
-        if isinstance(s, basestring):
+        if isinstance(s, str):
for c in s:
if self.input.LA(1) != ord(c):
if self._state.backtracking > 0:
@@ -1204,7 +1178,7 @@ class Lexer(BaseRecognizer, TokenSource):
if self._state.backtracking > 0:
raise BacktrackingFailed

-                mte = MismatchedTokenException(unichr(s), self.input)
+                mte = MismatchedTokenException(chr(s), self.input)
self.recover(mte) # don't really recover; just consume in lexer
raise mte

@@ -1220,7 +1194,7 @@ class Lexer(BaseRecognizer, TokenSource):
if self._state.backtracking > 0:
raise BacktrackingFailed

-            mre = MismatchedRangeException(unichr(a), unichr(b), self.input)
+            mre = MismatchedRangeException(chr(a), chr(b), self.input)
self.recover(mre)
raise mre

@@ -1272,53 +1246,47 @@ class Lexer(BaseRecognizer, TokenSource):
## # if we've already reported an error and have not matched a token
## # yet successfully, don't report any errors.
## if self.errorRecovery:
-        ##     #System.err.print("[SPURIOUS] ");
-        ##     return;
+        ##     return
##
## self.errorRecovery = True

-        self.displayRecognitionError(self.tokenNames, e)
+        self.displayRecognitionError(e)


-    def getErrorMessage(self, e, tokenNames):
+    def getErrorMessage(self, e):
msg = None

if isinstance(e, MismatchedTokenException):
-            msg = "mismatched character " \
-                  + self.getCharErrorDisplay(e.c) \
-                  + " expecting " \
-                  + self.getCharErrorDisplay(e.expecting)
+            msg = "mismatched character {} expecting {}".format(
+                self.getCharErrorDisplay(e.c),
+                self.getCharErrorDisplay(e.expecting))

elif isinstance(e, NoViableAltException):
-            msg = "no viable alternative at character " \
-                  + self.getCharErrorDisplay(e.c)
+            msg = "no viable alternative at character {}".format(
+                self.getCharErrorDisplay(e.c))

elif isinstance(e, EarlyExitException):
-            msg = "required (...)+ loop did not match anything at character " \
-                  + self.getCharErrorDisplay(e.c)
+            msg = "required (...)+ loop did not match anything at character {}".format(
+                self.getCharErrorDisplay(e.c))

elif isinstance(e, MismatchedNotSetException):
-            msg = "mismatched character " \
-                  + self.getCharErrorDisplay(e.c) \
-                  + " expecting set " \
-                  + repr(e.expecting)
+            msg = "mismatched character {} expecting set {!r}".format(
+                self.getCharErrorDisplay(e.c),
+                e.expecting)

elif isinstance(e, MismatchedSetException):
-            msg = "mismatched character " \
-                  + self.getCharErrorDisplay(e.c) \
-                  + " expecting set " \
-                  + repr(e.expecting)
+            msg = "mismatched character {} expecting set {!r}".format(
+                self.getCharErrorDisplay(e.c),
+                e.expecting)

elif isinstance(e, MismatchedRangeException):
-            msg = "mismatched character " \
-                  + self.getCharErrorDisplay(e.c) \
-                  + " expecting set " \
-                  + self.getCharErrorDisplay(e.a) \
-                  + ".." \
-                  + self.getCharErrorDisplay(e.b)
+            msg = "mismatched character {} expecting set {}..{}".format(
+                self.getCharErrorDisplay(e.c),
+                self.getCharErrorDisplay(e.a),
+                self.getCharErrorDisplay(e.b))

else:
-            msg = BaseRecognizer.getErrorMessage(self, e, tokenNames)
+            msg = super().getErrorMessage(e)

return msg

@@ -1341,21 +1309,21 @@ class Lexer(BaseRecognizer, TokenSource):


def traceIn(self, ruleName, ruleIndex):
-        inputSymbol = "%s line=%d:%s" % (self.input.LT(1),
-                                         self.getLine(),
-                                         self.getCharPositionInLine()
-                                         )
+        inputSymbol = "{} line={}:{}".format(self.input.LT(1),
+                                             self.getLine(),
+                                             self.getCharPositionInLine()
+                                             )

-        BaseRecognizer.traceIn(self, ruleName, ruleIndex, inputSymbol)
+        super().traceIn(ruleName, ruleIndex, inputSymbol)


def traceOut(self, ruleName, ruleIndex):
-        inputSymbol = "%s line=%d:%s" % (self.input.LT(1),
-                                         self.getLine(),
-                                         self.getCharPositionInLine()
-                                         )
+        inputSymbol = "{} line={}:{}".format(self.input.LT(1),
+                                             self.getLine(),
+                                             self.getCharPositionInLine()
+                                             )

-        BaseRecognizer.traceOut(self, ruleName, ruleIndex, inputSymbol)
+        super().traceOut(ruleName, ruleIndex, inputSymbol)



@@ -1365,13 +1333,13 @@ class Parser(BaseRecognizer):
"""

def __init__(self, lexer, state=None):
-        BaseRecognizer.__init__(self, state)
+        super().__init__(state)

self.input = lexer


def reset(self):
-        BaseRecognizer.reset(self) # reset all recognizer state variables
+        super().reset() # reset all recognizer state variables
if self.input is not None:
self.input.seek(0) # rewind the input

@@ -1384,7 +1352,7 @@ class Parser(BaseRecognizer):
if expectedTokenType == EOF:
tokenText = "<missing EOF>"
else:
-            tokenText = "<missing " + self.tokenNames[expectedTokenType] + ">"
+            tokenText = "<missing {}>".format(self.tokenNames[expectedTokenType])
t = CommonToken(type=expectedTokenType, text=tokenText)
current = input.LT(1)
if current.type == EOF:
@@ -1414,11 +1382,11 @@ class Parser(BaseRecognizer):


def traceIn(self, ruleName, ruleIndex):
-        BaseRecognizer.traceIn(self, ruleName, ruleIndex, self.input.LT(1))
+        super().traceIn(ruleName, ruleIndex, self.input.LT(1))


def traceOut(self, ruleName, ruleIndex):
-        BaseRecognizer.traceOut(self, ruleName, ruleIndex, self.input.LT(1))
+        super().traceOut(ruleName, ruleIndex, self.input.LT(1))


class RuleReturnScope(object):
diff --git a/runtime/Python3/antlr3/streams.py b/runtime/Python3/antlr3/streams.py
index 84016bd..2cfd77c 100644
--- a/runtime/Python3/antlr3/streams.py
+++ b/runtime/Python3/antlr3/streams.py
@@ -30,11 +30,10 @@
#
# end[licence]

-import codecs
-from StringIO import StringIO
+from io import StringIO

-from antlr3.constants import DEFAULT_CHANNEL, EOF
-from antlr3.tokens import Token, CommonToken
+from .constants import DEFAULT_CHANNEL, EOF
+from .tokens import Token, CommonToken


############################################################################
@@ -64,9 +63,9 @@ class IntStream(object):
"""Get int at current input pointer + i ahead where i=1 is next int.

Negative indexes are allowed.  LA(-1) is previous token (token
-	just matched).  LA(-i) where i is before first token should
-	yield -1, invalid char / EOF.
-	"""
+        just matched).  LA(-i) where i is before first token should
+        yield -1, invalid char / EOF.
+        """

raise NotImplementedError

@@ -113,7 +112,7 @@ class IntStream(object):
and rewind(i) should balance still. It is
like invoking rewind(last marker) but it should not "pop"
the marker off.  It's like seek(last marker's input position).
-	"""
+        """

raise NotImplementedError

@@ -127,7 +126,7 @@ class IntStream(object):
This must throw away resources for all markers back to the marker
argument.  So if you're nested 5 levels of mark(), and then release(2)
you have to release resources for depths 2..5.
-	"""
+        """

raise NotImplementedError

@@ -164,7 +163,7 @@ class IntStream(object):
Only makes sense for streams that buffer everything up probably, but
might be useful to display the entire stream or for testing.  This
value includes a single EOF.
-	"""
+        """

raise NotImplementedError

@@ -259,7 +258,7 @@ class TokenStream(IntStream):
two tokens ago. LT(0) is undefined.  For i>=n, return Token.EOFToken.
Return null for LT(0) and any index that results in an absolute address
that is negative.
-	"""
+        """

raise NotImplementedError

@@ -290,7 +289,7 @@ class TokenStream(IntStream):
"""
Where is this stream pulling tokens from?  This is not the name, but
the object that provides Token objects.
-	"""
+        """

raise NotImplementedError

@@ -307,7 +306,7 @@ class TokenStream(IntStream):
indicate the start/end location.  Most often this will just delegate
to the other toString(int,int).  This is also parallel with
the TreeNodeStream.toString(Object,Object).
-	"""
+        """

raise NotImplementedError

@@ -334,31 +333,30 @@ class ANTLRStringStream(CharStream):
def __init__(self, data):
"""
@param data This should be a unicode string holding the data you want
-           to parse. If you pass in a byte string, the Lexer will choke on
-           non-ascii data.
-
+        to parse. If you pass in a byte string, the Lexer will choke on
+        non-ascii data.
"""

-        CharStream.__init__(self)
+        super().__init__()

-  	# The data being scanned
-        self.strdata = unicode(data)
+        # The data being scanned
+        self.strdata = str(data)
self.data = [ord(c) for c in self.strdata]

-	# How many characters are actually in the buffer
+        # How many characters are actually in the buffer
self.n = len(data)

- 	# 0..n-1 index into string of next char
+        # 0..n-1 index into string of next char
self.p = 0

-	# line number 1..n within the input
-        self.line = 1
+        # line number 1..n within the input
+        self._line = 1

- 	# The index of the character relative to the beginning of the
+        # The index of the character relative to the beginning of the
# line 0..n-1
-        self.charPositionInLine = 0
+        self._charPositionInLine = 0

-	# A list of CharStreamState objects that tracks the stream state
+        # A list of CharStreamState objects that tracks the stream state
# values line, charPositionInLine, and p that can change as you
# move through the input stream.  Indexed from 0..markDepth-1.
self._markers = [ ]
@@ -377,26 +375,25 @@ class ANTLRStringStream(CharStream):
"""

self.p = 0
-        self.line = 1
+        self._line = 1
self.charPositionInLine = 0
self._markers = [ ]
+        self.lastMarker = None
+        self.markDepth = 0


def consume(self):
-        try:
-            if self.data[self.p] == 10: # \n
-                self.line += 1
+        if self.p < self.n:
+            if self.data[self.p] == 10: # ord('\n')
+                self._line += 1
self.charPositionInLine = 0
else:
self.charPositionInLine += 1

self.p += 1

-        except IndexError:
-            # happend when we reached EOF and self.data[self.p] fails
-            # just do nothing
-            pass
-
+        # else we reached EOF
+        # just do nothing


def LA(self, i):
@@ -406,9 +403,9 @@ class ANTLRStringStream(CharStream):
if i < 0:
i += 1 # e.g., translate LA(-1) to use offset i=0; then data[p+0-1]

-        try:
-            return self.data[self.p+i-1]
-        except IndexError:
+        if self.p + i - 1 < self.n:
+            return self.data[self.p + i - 1]
+        else:
return EOF


@@ -420,9 +417,9 @@ class ANTLRStringStream(CharStream):
if i < 0:
i += 1 # e.g., translate LA(-1) to use offset i=0; then data[p+0-1]

-        try:
-            return self.strdata[self.p+i-1]
-        except IndexError:
+        if self.p + i - 1 < self.n:
+            return self.strdata[self.p + i - 1]
+        else:
return EOF


@@ -442,9 +439,9 @@ class ANTLRStringStream(CharStream):

def mark(self):
state = (self.p, self.line, self.charPositionInLine)
-        try:
+        if self.markDepth < len(self._markers):
self._markers[self.markDepth] = state
-        except IndexError:
+        else:
self._markers.append(state)
self.markDepth += 1

@@ -457,10 +454,10 @@ class ANTLRStringStream(CharStream):
if marker is None:
marker = self.lastMarker

-        p, line, charPositionInLine = self._markers[marker-1]
+        p, line, charPositionInLine = self._markers[marker - 1]

self.seek(p)
-        self.line = line
+        self._line = line
self.charPositionInLine = charPositionInLine
self.release(marker)

@@ -469,7 +466,7 @@ class ANTLRStringStream(CharStream):
if marker is None:
marker = self.lastMarker

-        self.markDepth = marker-1
+        self.markDepth = marker - 1


def seek(self, index):
@@ -488,33 +485,22 @@ class ANTLRStringStream(CharStream):


def substring(self, start, stop):
-        return self.strdata[start:stop+1]
+        return self.strdata[start:stop + 1]


-    def getLine(self):
-        """Using setter/getter methods is deprecated. Use o.line instead."""
-        return self.line
+    @property
+    def line(self):
+        return self._line


-    def getCharPositionInLine(self):
-        """
-        Using setter/getter methods is deprecated. Use o.charPositionInLine
-        instead.
-        """
-        return self.charPositionInLine

+    @property
+    def charPositionInLine(self):
+        return self._charPositionInLine

-    def setLine(self, line):
-        """Using setter/getter methods is deprecated. Use o.line instead."""
-        self.line = line
-
-
-    def setCharPositionInLine(self, pos):
-        """
-        Using setter/getter methods is deprecated. Use o.charPositionInLine
-        instead.
-        """
-        self.charPositionInLine = pos
+    @charPositionInLine.setter
+    def charPositionInLine(self, pos):
+        self._charPositionInLine = pos


def getSourceName(self):
@@ -529,31 +515,22 @@ class ANTLRFileStream(ANTLRStringStream):
all at once when you construct the object.
"""

-    def __init__(self, fileName, encoding=None):
+    def __init__(self, fileName):
"""
@param fileName The path to the file to be opened. The file will be
-           opened with mode 'rb'.
-
-        @param encoding If you set the optional encoding argument, then the
-           data will be decoded on the fly.
+           opened with mode 'r'.

"""

-        self.fileName = fileName
-
-        fp = codecs.open(fileName, 'rb', encoding)
-        try:
-            data = fp.read()
-        finally:
-            fp.close()
+        self._fileName = fileName

-        ANTLRStringStream.__init__(self, data)
+        with open(fileName, 'r') as fp:
+            super().__init__(fp.read())


-    def getSourceName(self):
-        """Deprecated, access o.fileName directly."""
-
-        return self.fileName
+    @property
+    def fileName(self):
+        return self._fileName


class ANTLRInputStream(ANTLRStringStream):
@@ -566,24 +543,16 @@ class ANTLRInputStream(ANTLRStringStream):
All input is consumed from the file, but it is not closed.
"""

-    def __init__(self, file, encoding=None):
+    def __init__(self, file):
"""
@param file A file-like object holding your input. Only the read()
method must be implemented.

-        @param encoding If you set the optional encoding argument, then the
-           data will be decoded on the fly.
-
"""

-        if encoding is not None:
-            # wrap input in a decoding reader
-            reader = codecs.lookup(encoding)[2]
-            file = reader(file)
-
data = file.read()

-        ANTLRStringStream.__init__(self, data)
+        super().__init__(data)


# I guess the ANTLR prefix exists only to avoid a name clash with some Java
@@ -624,28 +593,28 @@ class CommonTokenStream(TokenStream):

"""

-        TokenStream.__init__(self)
+        super().__init__()

self.tokenSource = tokenSource

-	# Record every single token pulled from the source so we can reproduce
+        # Record every single token pulled from the source so we can reproduce
# chunks of it later.
self.tokens = []

-	# Map<tokentype, channel> to override some Tokens' channel numbers
+        # Map<tokentype, channel> to override some Tokens' channel numbers
self.channelOverrideMap = {}

-	# Set<tokentype>; discard any tokens with this type
+        # Set<tokentype>; discard any tokens with this type
self.discardSet = set()

-	# Skip tokens on any channel but this one; this is how we skip
+        # Skip tokens on any channel but this one; this is how we skip
# whitespace...
self.channel = channel

-	# By default, track all incoming tokens
+        # By default, track all incoming tokens
self.discardOffChannelTokens = False

-	# The index into the tokens list of the current token (next token
+        # The index into the tokens list of the current token (next token
# to consume).  p==-1 indicates that the tokens list is empty
self.p = -1

@@ -677,31 +646,26 @@ class CommonTokenStream(TokenStream):
def fillBuffer(self):
"""
Load all tokens from the token source and put in tokens.
-	This is done upon first LT request because you might want to
+        This is done upon first LT request because you might want to
set some token type / channel overrides before filling buffer.
"""


index = 0
t = self.tokenSource.nextToken()
-        while t is not None and t.type != EOF:
+        while t and t.type != EOF:
discard = False

-            if self.discardSet is not None and t.type in self.discardSet:
+            if self.discardSet and t.type in self.discardSet:
discard = True

elif self.discardOffChannelTokens and t.channel != self.channel:
discard = True

# is there a channel override for token type?
-            try:
+            if t.type in self.channelOverrideMap:
overrideChannel = self.channelOverrideMap[t.type]

-            except KeyError:
-                # no override for this type
-                pass
-
-            else:
if overrideChannel == self.channel:
t.channel = overrideChannel
else:
@@ -741,12 +705,9 @@ class CommonTokenStream(TokenStream):
token.
"""

-        try:
-            while self.tokens[i].channel != self.channel:
-                i += 1
-        except IndexError:
-            # hit the end of token stream
-            pass
+        n = len(self.tokens)
+        while i < n and self.tokens[i].channel != self.channel:
+            i += 1

return i

@@ -765,7 +726,7 @@ class CommonTokenStream(TokenStream):
when interpreting, we cannot exec actions so we need to tell
the stream to force all WS and NEWLINE to be a different, ignored
channel.
-	"""
+        """

self.channelOverrideMap[ttype] = channel

@@ -787,13 +748,13 @@ class CommonTokenStream(TokenStream):
if stop is None or stop > len(self.tokens):
stop = len(self.tokens)

-        if start is None or stop < 0:
+        if start is None or start < 0:
start = 0

if start > stop:
return None

-        if isinstance(types, (int, long)):
+        if isinstance(types, int):
# called with a single type, wrap into set
types = set([types])

@@ -828,15 +789,15 @@ class CommonTokenStream(TokenStream):
# find k good tokens
while n < k:
# skip off-channel tokens
-            i = self.skipOffTokenChannels(i+1) # leave p on valid token
+            i = self.skipOffTokenChannels(i + 1) # leave p on valid token
n += 1

if i > self._range:
self._range = i

-        try:
+        if i < len(self.tokens):
return self.tokens[i]
-        except IndexError:
+        else:
return self.makeEOFToken()


@@ -857,7 +818,7 @@ class CommonTokenStream(TokenStream):
# find k good tokens looking backwards
while n <= k:
# skip off-channel tokens
-            i = self.skipOffTokenChannelsReverse(i-1) # leave p on valid token
+            i = self.skipOffTokenChannelsReverse(i - 1) # leave p on valid token
n += 1

if i < 0:
@@ -882,7 +843,7 @@ class CommonTokenStream(TokenStream):
if start < 0 or stop < 0:
return None

-        return self.tokens[start:stop+1]
+        return self.tokens[start:stop + 1]


def LA(self, i):
@@ -931,6 +892,7 @@ class CommonTokenStream(TokenStream):


def toString(self, start=None, stop=None):
+        """Returns a string of all tokens between start and stop (inclusive)."""
if self.p == -1:
self.fillBuffer()

@@ -947,7 +909,7 @@ class CommonTokenStream(TokenStream):
if stop >= len(self.tokens):
stop = len(self.tokens) - 1

-        return ''.join([t.text for t in self.tokens[start:stop+1]])
+        return ''.join([t.text for t in self.tokens[start:stop + 1]])


class RewriteOperation(object):
@@ -972,8 +934,7 @@ class RewriteOperation(object):

def toString(self):
opName = self.__class__.__name__
-        return '<%s@%d:"%s">' % (
-            opName, self.index, self.text)
+        return '<{opName}@{0.index}:"{0.text}">'.format(self, opName=opName)

__str__ = toString
__repr__ = toString
@@ -998,7 +959,7 @@ class ReplaceOp(RewriteOperation):
"""

def __init__(self, stream, first, last, text):
-        RewriteOperation.__init__(self, stream, first, text)
+        super().__init__(stream, first, text)
self.lastIndex = last


@@ -1011,10 +972,9 @@ class ReplaceOp(RewriteOperation):

def toString(self):
if self.text is None:
-            return '<DeleteOp@%d..%d>' % (self.index, self.lastIndex)
+            return '<DeleteOp@{0.index}..{0.lastindex}>'.format(self)

-        return '<ReplaceOp@%d..%d:"%s">' % (
-            self.index, self.lastIndex, self.text)
+        return '<ReplaceOp@{0.index}..{0.lastIndex}:"{0.text}">'.format(self)

__str__ = toString
__repr__ = toString
@@ -1079,7 +1039,7 @@ class TokenRewriteStream(CommonTokenStream):
MIN_TOKEN_INDEX = 0

def __init__(self, tokenSource=None, channel=DEFAULT_CHANNEL):
-        CommonTokenStream.__init__(self, tokenSource, channel)
+        super().__init__(tokenSource, channel)

# You may have multiple, named streams of rewrite operations.
# I'm calling these things "programs."
@@ -1087,7 +1047,7 @@ class TokenRewriteStream(CommonTokenStream):
self.programs = {}
self.programs[self.DEFAULT_PROGRAM_NAME] = []

- 	# Map String (program name) -> Integer index
+        # Map String (program name) -> Integer index
self.lastRewriteTokenIndexes = {}


@@ -1107,8 +1067,8 @@ class TokenRewriteStream(CommonTokenStream):
else:
raise TypeError("Invalid arguments")

-        p = self.programs.get(programName, None)
-        if p is not None:
+        p = self.programs.get(programName)
+        if p:
self.programs[programName] = (
p[self.MIN_TOKEN_INDEX:instructionIndex])

@@ -1138,7 +1098,7 @@ class TokenRewriteStream(CommonTokenStream):
index = index.index

# to insert after, just insert before next index (even if past end)
-        self.insertBefore(programName, index+1, text)
+        self.insertBefore(programName, index + 1, text)


def insertBefore(self, *args):
@@ -1156,7 +1116,7 @@ class TokenRewriteStream(CommonTokenStream):
raise TypeError("Invalid arguments")

if isinstance(index, Token):
-            # index is a Token, grap the stream index from it
+            # index is a Token, grab the stream index from it
index = index.index

op = InsertBeforeOp(self, index, text)
@@ -1197,8 +1157,8 @@ class TokenRewriteStream(CommonTokenStream):

if first > last or first < 0 or last < 0 or last >= len(self.tokens):
raise ValueError(
-                "replace: range invalid: %d..%d (size=%d)"
-                % (first, last, len(self.tokens)))
+                "replace: range invalid: {}..{} (size={})"
+                .format(first, last, len(self.tokens)))

op = ReplaceOp(self, first, last, text)
rewrites = self.getProgram(programName)
@@ -1219,8 +1179,8 @@ class TokenRewriteStream(CommonTokenStream):


def getProgram(self, name):
-        p = self.programs.get(name, None)
-        if p is  None:
+        p = self.programs.get(name)
+        if not p:
p = self.initializeProgram(name)

return p
@@ -1288,7 +1248,7 @@ class TokenRewriteStream(CommonTokenStream):
start = 0

rewrites = self.programs.get(programName)
-        if rewrites is None or len(rewrites) == 0:
+        if not rewrites:
# no instructions to execute
return self.toOriginalString(start, end)

@@ -1300,12 +1260,8 @@ class TokenRewriteStream(CommonTokenStream):
# Walk buffer, executing instructions and emitting tokens
i = start
while i <= end and i < len(self.tokens):
-            op = indexToOp.get(i)
# remove so any left have index size-1
-            try:
-                del indexToOp[i]
-            except KeyError:
-                pass
+            op = indexToOp.pop(i, None)

t = self.tokens[i]
if op is None:
@@ -1319,13 +1275,12 @@ class TokenRewriteStream(CommonTokenStream):

# include stuff after end if it's last index in buffer
# So, if they did an insertAfter(lastValidIndex, "foo"), include
-        # foo if end==lastValidIndex.
+        # foo if end == lastValidIndex.
if end == len(self.tokens) - 1:
# Scan any remaining operations after last token
# should be included (they will be inserts).
-            for i in sorted(indexToOp.keys()):
-                op = indexToOp[i]
-                if op.index >= len(self.tokens)-1:
+            for i, op in sorted(indexToOp.items()):
+                if op.index >= len(self.tokens) - 1:
buf.write(op.text)

return buf.getvalue()
@@ -1391,7 +1346,7 @@ class TokenRewriteStream(CommonTokenStream):

# WALK REPLACES
for i, rop in enumerate(rewrites):
-            if rop is None:
+            if not rop:
continue

if not isinstance(rop, ReplaceOp):
@@ -1484,9 +1439,9 @@ class TokenRewriteStream(CommonTokenStream):
def catOpText(self, a, b):
x = ""
y = ""
-        if a is not None:
+        if a:
x = a
-        if b is not None:
+        if b:
y = b
return x + y

@@ -1500,10 +1455,8 @@ class TokenRewriteStream(CommonTokenStream):
before = len(rewrites)

for i, op in enumerate(rewrites[:before]):
-            if op is None:
-                # ignore deleted
-                continue
-            if op.__class__ == kind:
+            # ignore deleted
+            if op and op.__class__ == kind:
yield i, op


diff --git a/runtime/Python3/antlr3/tokens.py b/runtime/Python3/antlr3/tokens.py
index d3f39b8..fcc2371 100644
--- a/runtime/Python3/antlr3/tokens.py
+++ b/runtime/Python3/antlr3/tokens.py
@@ -30,7 +30,7 @@
#
# end[licence]

-from antlr3.constants import EOF, DEFAULT_CHANNEL, INVALID_TOKEN_TYPE
+from .constants import DEFAULT_CHANNEL, EOF, INVALID_TOKEN_TYPE

############################################################################
#
@@ -41,101 +41,80 @@ from antlr3.constants import EOF, DEFAULT_CHANNEL, INVALID_TOKEN_TYPE
class Token(object):
"""@brief Abstract token baseclass."""

-    def getText(self):
-        """@brief Get the text of the token.
-
-        Using setter/getter methods is deprecated. Use o.text instead.
-        """
-        raise NotImplementedError
-
-    def setText(self, text):
-        """@brief Set the text of the token.
-
-        Using setter/getter methods is deprecated. Use o.text instead.
-        """
-        raise NotImplementedError
-
-
-    def getType(self):
-        """@brief Get the type of the token.
-
-        Using setter/getter methods is deprecated. Use o.type instead."""
-
-        raise NotImplementedError
-
-    def setType(self, ttype):
-        """@brief Get the type of the token.
-
-        Using setter/getter methods is deprecated. Use o.type instead."""
-
-        raise NotImplementedError
-
-
-    def getLine(self):
-        """@brief Get the line number on which this token was matched
-
-        Lines are numbered 1..n
-
-        Using setter/getter methods is deprecated. Use o.line instead."""
-
-        raise NotImplementedError
-
-    def setLine(self, line):
-        """@brief Set the line number on which this token was matched
-
-        Using setter/getter methods is deprecated. Use o.line instead."""
-
-        raise NotImplementedError
-
-
-    def getCharPositionInLine(self):
-        """@brief Get the column of the tokens first character,
+    def __init__(self, type=None, channel=DEFAULT_CHANNEL, text=None,
+                 index=-1, line=0, charPositionInLine=-1, input=None):
+        # We use -1 for index and charPositionInLine as an invalid index
+        self._type = type
+        self._channel = channel
+        self._text = text
+        self._index = index
+        self._line = 0
+        self._charPositionInLine = charPositionInLine
+        self.input = input

-        Columns are numbered 0..n-1
+    # To override a property, you'll need to override both the getter and setter.
+    @property
+    def text(self):
+        return self._text

-        Using setter/getter methods is deprecated. Use o.charPositionInLine instead."""
+    @text.setter
+    def text(self, value):
+        self._text = value

-        raise NotImplementedError

-    def setCharPositionInLine(self, pos):
-        """@brief Set the column of the tokens first character,
+    @property
+    def type(self):
+        return self._type

-        Using setter/getter methods is deprecated. Use o.charPositionInLine instead."""
+    @type.setter
+    def type(self, value):
+        self._type = value

-        raise NotImplementedError
+    # For compatibility
+    def getType(self):
+        return self._type

+
+    @property
+    def line(self):
+        """Lines are numbered 1..n."""
+        return self._line

-    def getChannel(self):
-        """@brief Get the channel of the token
+    @line.setter
+    def line(self, value):
+        self._line = value

-        Using setter/getter methods is deprecated. Use o.channel instead."""

-        raise NotImplementedError
+    @property
+    def charPositionInLine(self):
+        """Columns are numbered 0..n-1."""
+        return self._charPositionInLine

-    def setChannel(self, channel):
-        """@brief Set the channel of the token
+    @charPositionInLine.setter
+    def charPositionInLine(self, pos):
+        self._charPositionInLine = pos

-        Using setter/getter methods is deprecated. Use o.channel instead."""

-        raise NotImplementedError
+    @property
+    def channel(self):
+        return self._channel

+    @channel.setter
+    def channel(self, value):
+        self._channel = value

-    def getTokenIndex(self):
-        """@brief Get the index in the input stream.

+    @property
+    def index(self):
+        """
An index from 0..n-1 of the token object in the input stream.
This must be valid in order to use the ANTLRWorks debugger.
+        """
+        return self._index

-        Using setter/getter methods is deprecated. Use o.index instead."""
-
-        raise NotImplementedError
-
-    def setTokenIndex(self, index):
-        """@brief Set the index in the input stream.
-
-        Using setter/getter methods is deprecated. Use o.index instead."""
-
-        raise NotImplementedError
+    @index.setter
+    def index(self, value):
+        self._index = value


def getInputStream(self):
@@ -176,29 +155,20 @@ class CommonToken(Token):

def __init__(self, type=None, channel=DEFAULT_CHANNEL, text=None,
input=None, start=None, stop=None, oldToken=None):
-        Token.__init__(self)
-
-        if oldToken is not None:
-            self.type = oldToken.type
-            self.line = oldToken.line
-            self.charPositionInLine = oldToken.charPositionInLine
-            self.channel = oldToken.channel
-            self.index = oldToken.index
-            self._text = oldToken._text
-            self.input = oldToken.input
+
+        if oldToken:
+            super().__init__(oldToken.type, oldToken.channel, oldToken.text,
+                             oldToken.index, oldToken.line,
+                             oldToken.charPositionInLine, oldToken.input)
if isinstance(oldToken, CommonToken):
self.start = oldToken.start
self.stop = oldToken.stop
+            else:
+                self.start = start
+                self.stop = stop

else:
-            self.type = type
-            self.input = input
-            self.charPositionInLine = -1 # set to invalid position
-            self.line = 0
-            self.channel = channel
-
-	    #What token number is this from 0..n-1 tokens; < 0 implies invalid index
-            self.index = -1
+            super().__init__(type=type, channel=channel, input=input)

# We need to be able to change the text once in a while.  If
# this is non-null, then getText should return this.  Note that
@@ -213,11 +183,13 @@ class CommonToken(Token):
self.stop = stop


-    def getText(self):
+    @property
+    def text(self):
+        # Could be the empty string, and we want to return that.
if self._text is not None:
return self._text

-        if self.input is None:
+        if not self.input:
return None

if self.start < self.input.size() and self.stop < self.input.size():
@@ -225,57 +197,21 @@ class CommonToken(Token):

return '<EOF>'

-
-    def setText(self, text):
+    @text.setter
+    def text(self, value):
"""
Override the text for this token.  getText() will return this text
rather than pulling from the buffer.  Note that this does not mean
that start/stop indexes are not valid.  It means that that input
was converted to a new string in the token object.
-	"""
-        self._text = text
-
-    text = property(getText, setText)
-
-
-    def getType(self):
-        return self.type
+        """
+        self._text = value

-    def setType(self, ttype):
-        self.type = ttype

-    def getTypeName(self):
+    @property
+    def typeName(self):
return str(self.type)

-    typeName = property(lambda s: s.getTypeName())
-
-    def getLine(self):
-        return self.line
-
-    def setLine(self, line):
-        self.line = line
-
-
-    def getCharPositionInLine(self):
-        return self.charPositionInLine
-
-    def setCharPositionInLine(self, pos):
-        self.charPositionInLine = pos
-
-
-    def getChannel(self):
-        return self.channel
-
-    def setChannel(self, channel):
-        self.channel = channel
-
-
-    def getTokenIndex(self):
-        return self.index
-
-    def setTokenIndex(self, index):
-        self.index = index
-

def getInputStream(self):
return self.input
@@ -293,20 +229,18 @@ class CommonToken(Token):
channelStr = ",channel=" + str(self.channel)

txt = self.text
-        if txt is not None:
-            txt = txt.replace("\n","\\\\n")
-            txt = txt.replace("\r","\\\\r")
-            txt = txt.replace("\t","\\\\t")
+        if txt:
+            # Put 2 backslashes in front of each character
+            txt = txt.replace("\n", r"\\n")
+            txt = txt.replace("\r", r"\\r")
+            txt = txt.replace("\t", r"\\t")
else:
txt = "<no text>"

-        return "[@%d,%d:%d=%r,<%s>%s,%d:%d]" % (
-            self.index,
-            self.start, self.stop,
-            txt,
-            self.typeName, channelStr,
-            self.line, self.charPositionInLine
-            )
+        return ("[@{0.index},{0.start}:{0.stop}={txt!r},"
+                "<{0.typeName}>{channelStr},"
+                "{0.line}:{0.charPositionInLine}]"
+                .format(self, txt=txt, channelStr=channelStr))


class ClassicToken(Token):
@@ -321,65 +255,15 @@ class ClassicToken(Token):
"""

def __init__(self, type=None, text=None, channel=DEFAULT_CHANNEL,
-                 oldToken=None
-                 ):
-        Token.__init__(self)
+                 oldToken=None):
+        if oldToken:
+            super().__init__(type=oldToken.type, channel=oldToken.channel,
+                             text=oldToken.text, line=oldToken.line,
+                             charPositionInLine=oldToken.charPositionInLine)

-        if oldToken is not None:
-            self.text = oldToken.text
-            self.type = oldToken.type
-            self.line = oldToken.line
-            self.charPositionInLine = oldToken.charPositionInLine
-            self.channel = oldToken.channel
-
-        self.text = text
-        self.type = type
-        self.line = None
-        self.charPositionInLine = None
-        self.channel = channel
-        self.index = None
-
-
-    def getText(self):
-        return self.text
-
-    def setText(self, text):
-        self.text = text
-
-
-    def getType(self):
-        return self.type
-
-    def setType(self, ttype):
-        self.type = ttype
-
-
-    def getLine(self):
-        return self.line
-
-    def setLine(self, line):
-        self.line = line
-
-
-    def getCharPositionInLine(self):
-        return self.charPositionInLine
-
-    def setCharPositionInLine(self, pos):
-        self.charPositionInLine = pos
-
-
-    def getChannel(self):
-        return self.channel
-
-    def setChannel(self, channel):
-        self.channel = channel
-
-
-    def getTokenIndex(self):
-        return self.index
-
-    def setTokenIndex(self, index):
-        self.index = index
+        else:
+            super().__init__(type=type, channel=channel, text=text,
+                             index=None, line=None, charPositionInLine=None)


def getInputStream(self):
@@ -395,17 +279,12 @@ class ClassicToken(Token):
channelStr = ",channel=" + str(self.channel)

txt = self.text
-        if txt is None:
+        if not txt:
txt = "<no text>"

-        return "[@%r,%r,<%r>%s,%r:%r]" % (self.index,
-                                          txt,
-                                          self.type,
-                                          channelStr,
-                                          self.line,
-                                          self.charPositionInLine
-                                          )
-
+        return ("[@{0.index!r},{txt!r},<{0.type!r}>{channelStr},"
+                "{0.line!r}:{0.charPositionInLine!r}]"
+                .format(self, txt=txt, channelStr=channelStr))

__str__ = toString
__repr__ = toString
diff --git a/runtime/Python3/antlr3/tree.py b/runtime/Python3/antlr3/tree.py
index 7bc8446..c06a89e 100644
--- a/runtime/Python3/antlr3/tree.py
+++ b/runtime/Python3/antlr3/tree.py
@@ -613,7 +613,7 @@ class TreeAdaptor(object):
return self.createWithPayload(args[0])

if (len(args) == 2
-            and isinstance(args[0], (int, long))
+            and isinstance(args[0], int)
and isinstance(args[1], Token)
):
# Object create(int tokenType, Token fromToken);
@@ -625,9 +625,9 @@ class TreeAdaptor(object):
return self.createFromToken(args[0], args[1])

if (len(args) == 3
-            and isinstance(args[0], (int, long))
+            and isinstance(args[0], int)
and isinstance(args[1], Token)
-            and isinstance(args[2], basestring)
+            and isinstance(args[2], str)
):
# Object create(int tokenType, Token fromToken, String text);
##             warnings.warn(
@@ -638,8 +638,8 @@ class TreeAdaptor(object):
return self.createFromToken(args[0], args[1], args[2])

if (len(args) == 2
-            and isinstance(args[0], (int, long))
-            and isinstance(args[1], basestring)
+            and isinstance(args[0], int)
+            and isinstance(args[1], str)
):
# Object create(int tokenType, String text);
##             warnings.warn(
@@ -1109,9 +1109,9 @@ class BaseTreeAdaptor(TreeAdaptor):
if fromToken is None:
return self.createFromType(tokenType, text)

-        assert isinstance(tokenType, (int, long)), type(tokenType).__name__
+        assert isinstance(tokenType, int), type(tokenType).__name__
assert isinstance(fromToken, Token), type(fromToken).__name__
-        assert text is None or isinstance(text, basestring), type(text).__name__
+        assert text is None or isinstance(text, str), type(text).__name__

fromToken = self.createToken(fromToken)
fromToken.type = tokenType
@@ -1122,8 +1122,8 @@ class BaseTreeAdaptor(TreeAdaptor):


def createFromType(self, tokenType, text):
-        assert isinstance(tokenType, (int, long)), type(tokenType).__name__
-        assert isinstance(text, basestring) or text is None, type(text).__name__
+        assert isinstance(tokenType, int), type(tokenType).__name__
+        assert isinstance(text, str) or text is None, type(text).__name__

fromToken = self.createToken(tokenType=tokenType, text=text)
t = self.createWithPayload(fromToken)
@@ -1254,7 +1254,7 @@ class CommonTree(BaseTree):
if self.token is None:
return INVALID_TOKEN_TYPE

-        return self.token.getType()
+        return self.token.type

type = property(getType)

@@ -1269,33 +1269,33 @@ class CommonTree(BaseTree):


def getLine(self):
-        if self.token is None or self.token.getLine() == 0:
+        if self.token is None or self.token.line == 0:
if self.getChildCount():
return self.getChild(0).getLine()
else:
return 0

-        return self.token.getLine()
+        return self.token.line

line = property(getLine)


def getCharPositionInLine(self):
-        if self.token is None or self.token.getCharPositionInLine() == -1:
+        if self.token is None or self.token.charPositionInLine == -1:
if self.getChildCount():
return self.getChild(0).getCharPositionInLine()
else:
return 0

else:
-            return self.token.getCharPositionInLine()
+            return self.token.charPositionInLine

charPositionInLine = property(getCharPositionInLine)


def getTokenStartIndex(self):
-        if self.startIndex == -1 and self.token is not None:
-            return self.token.getTokenIndex()
+        if self.startIndex == -1 and self.token:
+            return self.token.index

return self.startIndex

@@ -1306,8 +1306,8 @@ class CommonTree(BaseTree):


def getTokenStopIndex(self):
-        if self.stopIndex == -1 and self.token is not None:
-            return self.token.getTokenIndex()
+        if self.stopIndex == -1 and self.token:
+            return self.token.index

return self.stopIndex

@@ -1325,7 +1325,7 @@ class CommonTree(BaseTree):

if self.children is None:
if self.startIndex < 0 or self.stopIndex < 0:
-                self.startIndex = self.stopIndex = self.token.getTokenIndex()
+                self.startIndex = self.stopIndex = self.token.index

return

@@ -1401,11 +1401,7 @@ class CommonErrorNode(CommonTree):
def __init__(self, input, start, stop, exc):
CommonTree.__init__(self, None)

-        if (stop is None or
-            (stop.getTokenIndex() < start.getTokenIndex() and
-             stop.getType() != EOF
-             )
-            ):
+        if (stop is None or (stop.index < start.index and stop.type != EOF)):
# sometimes resync does not consume a token (when LT(1) is
# in follow set.  So, stop will be 1 to left to start. adjust.
# Also handle case where start is the first token and no token
@@ -1428,9 +1424,9 @@ class CommonErrorNode(CommonTree):

def getText(self):
if isinstance(self.start, Token):
-            i = self.start.getTokenIndex()
-            j = self.stop.getTokenIndex()
-            if self.stop.getType() == EOF:
+            i = self.start.index
+            j = self.stop.index
+            if self.stop.type == EOF:
j = self.input.size()

badText = self.input.toString(i, j)
@@ -1560,14 +1556,14 @@ class CommonTreeAdaptor(BaseTreeAdaptor):
def getText(self, t):
if t is None:
return None
-        return t.getText()
+        return t.text


def getType(self, t):
if t is None:
return INVALID_TOKEN_TYPE

-        return t.getType()
+        return t.type


def getToken(self, t):
@@ -2308,7 +2304,7 @@ class TreeParser(BaseRecognizer):
return None


-    def matchAny(self, ignore): # ignore stream, copy of this.input
+    def matchAny(self):
"""
Match '.' in tree parser has special meaning.  Skip node or
entire tree if node has children.  If children, scan until
@@ -2357,14 +2353,12 @@ class TreeParser(BaseRecognizer):
"""

return (self.getGrammarFileName() +
-                ": node from %sline %s:%s"
-                % (['', "after "][e.approximateLineInfo],
-                   e.line,
-                   e.charPositionInLine
-                   )
-                )
+                ": node from {}line {}:{}".format(
+                    "after " if e.approximateLineInfo else '',
+                    e.line,
+                    e.charPositionInLine))

-    def getErrorMessage(self, e, tokenNames):
+    def getErrorMessage(self, e):
"""
Tree parsers parse nodes they usually have a token object as
payload. Set the exception token and do the default behavior.
@@ -2379,7 +2373,7 @@ class TreeParser(BaseRecognizer):
text=adaptor.getText(e.node)
)

-        return BaseRecognizer.getErrorMessage(self, e, tokenNames)
+        return BaseRecognizer.getErrorMessage(self, e)


def traceIn(self, ruleName, ruleIndex):
@@ -2499,7 +2493,7 @@ class TreeIterator(object):
return self.adaptor.getParent(self.tree) is not None


-    def next(self):
+    def __next__(self):
if not self.has_next():
raise StopIteration

diff --git a/runtime/Python3/antlr3/treewizard.py b/runtime/Python3/antlr3/treewizard.py
index d96ce78..dec3a35 100644
--- a/runtime/Python3/antlr3/treewizard.py
+++ b/runtime/Python3/antlr3/treewizard.py
@@ -36,9 +36,9 @@ See <http://www.antlr.org/wiki/display/~admin/2007/07/02/Exploring+Concept+of+Tr
#
# end[licence]

-from antlr3.constants import INVALID_TOKEN_TYPE
-from antlr3.tokens import CommonToken
-from antlr3.tree import CommonTree, CommonTreeAdaptor
+from .constants import INVALID_TOKEN_TYPE
+from .tokens import CommonToken
+from .tree import CommonTree, CommonTreeAdaptor


def computeTokenTypes(tokenNames):
@@ -47,10 +47,10 @@ def computeTokenTypes(tokenNames):
tokenNames (which maps int token types to names).
"""

-    if tokenNames is None:
-        return {}
+    if tokenNames:
+        return dict((name, type) for type, name in enumerate(tokenNames))

-    return dict((name, type) for type, name in enumerate(tokenNames))
+    return {}


## token types for pattern parser
@@ -275,18 +275,18 @@ class TreePattern(CommonTree):
"""

def __init__(self, payload):
-        CommonTree.__init__(self, payload)
+        super().__init__(payload)

self.label = None
self.hasTextArg = None


def toString(self):
-        if self.label is not None:
-            return '%' + self.label + ':' + CommonTree.toString(self)
+        if self.label:
+            return '%' + self.label + ':' + super().toString()

else:
-            return CommonTree.toString(self)
+            return super().toString()


class WildcardTreePattern(TreePattern):
@@ -330,7 +330,7 @@ class TreeWizard(object):
self.tokenNameToTypeMap = computeTokenTypes(tokenNames)

else:
-            if tokenNames is not None:
+            if tokenNames:
raise ValueError("Can't have both tokenNames and typeMap")

self.tokenNameToTypeMap = typeMap
@@ -339,9 +339,9 @@ class TreeWizard(object):
def getTokenType(self, tokenName):
"""Using the map of token names to token types, return the type."""

-        try:
+        if tokenName in self.tokenNameToTypeMap:
return self.tokenNameToTypeMap[tokenName]
-        except KeyError:
+        else:
return INVALID_TOKEN_TYPE


@@ -404,10 +404,10 @@ class TreeWizard(object):

"""

-        if isinstance(what, (int, long)):
+        if isinstance(what, int):
return self._findTokenType(tree, what)

-        elif isinstance(what, basestring):
+        elif isinstance(what, str):
return self._findPattern(tree, what)

else:
@@ -469,10 +469,10 @@ class TreeWizard(object):
label.
"""

-        if isinstance(what, (int, long)):
+        if isinstance(what, int):
self._visitType(tree, None, 0, what, visitor)

-        elif isinstance(what, basestring):
+        elif isinstance(what, str):
self._visitPattern(tree, what, visitor)

else:
diff --git a/runtime/Python3/tests/t055templates.py b/runtime/Python3/tests/t055templates.py
deleted file mode 100644
index 5090b01..0000000
--- a/runtime/Python3/tests/t055templates.py
+++ /dev/null
@@ -1,508 +0,0 @@
-import unittest
-import textwrap
-import antlr3
-import antlr3.tree
-import stringtemplate3
-import testbase
-import sys
-import os
-from StringIO import StringIO
-
-class T(testbase.ANTLRTest):
-    def execParser(self, grammar, grammarEntry, input, group=None):
-        lexerCls, parserCls = self.compileInlineGrammar(grammar)
-
-        cStream = antlr3.StringStream(input)
-        lexer = lexerCls(cStream)
-        tStream = antlr3.CommonTokenStream(lexer)
-        parser = parserCls(tStream)
-        if group is not None:
-            parser.templateLib = group
-        result = getattr(parser, grammarEntry)()
-        if result.st is not None:
-            return result.st.toString()
-        return None
-
-
-    def testInlineTemplate(self):
-        grammar = textwrap.dedent(
-            r'''grammar T;
-            options {
-              language=Python;
-              output=template;
-            }
-            a : ID INT
-              -> template(id={$ID.text}, int={$INT.text})
-                 "id=<id>, int=<int>"
-            ;
-
-            ID : 'a'..'z'+;
-            INT : '0'..'9'+;
-            WS : (' '|'\n') {$channel=HIDDEN;} ;
-            '''
-            )
-
-        found = self.execParser(
-            grammar, 'a',
-            "abc 34"
-            )
-
-        self.failUnlessEqual("id=abc, int=34", found)
-
-
-    def testExternalTemplate(self):
-        templates = textwrap.dedent(
-            '''\
-            group T;
-            expr(args, op) ::= <<
-            [<args; separator={<op>}>]
-            >>
-            '''
-            )
-
-        group = stringtemplate3.StringTemplateGroup(
-            file=StringIO(templates),
-            lexer='angle-bracket'
-            )
-
-        grammar = textwrap.dedent(
-            r'''grammar T2;
-            options {
-              language=Python;
-              output=template;
-            }
-            a : r+=arg OP r+=arg
-              -> expr(op={$OP.text}, args={$r})
-            ;
-            arg: ID -> template(t={$ID.text}) "<t>";
-
-            ID : 'a'..'z'+;
-            OP: '+';
-            WS : (' '|'\n') {$channel=HIDDEN;} ;
-            '''
-            )
-
-        found = self.execParser(
-            grammar, 'a',
-            "a + b",
-            group
-            )
-
-        self.failUnlessEqual("[a+b]", found)
-
-
-    def testEmptyTemplate(self):
-        grammar = textwrap.dedent(
-            r'''grammar T;
-            options {
-              language=Python;
-              output=template;
-            }
-            a : ID INT
-              ->
-            ;
-
-            ID : 'a'..'z'+;
-            INT : '0'..'9'+;
-            WS : (' '|'\n') {$channel=HIDDEN;} ;
-            '''
-            )
-
-        found = self.execParser(
-            grammar, 'a',
-            "abc 34"
-            )
-
-        self.failUnless(found is None)
-
-
-    def testList(self):
-        grammar = textwrap.dedent(
-            r'''grammar T;
-            options {
-              language=Python;
-              output=template;
-            }
-            a: (r+=b)* EOF
-              -> template(r={$r})
-                 "<r; separator=\",\">"
-            ;
-
-            b: ID
-              -> template(t={$ID.text}) "<t>"
-            ;
-
-            ID : 'a'..'z'+;
-            WS : (' '|'\n') {$channel=HIDDEN;} ;
-            '''
-            )
-
-        found = self.execParser(
-            grammar, 'a',
-            "abc def ghi"
-            )
-
-        self.failUnlessEqual("abc,def,ghi", found)
-
-
-    def testAction(self):
-        grammar = textwrap.dedent(
-            r'''grammar T;
-            options {
-              language=Python;
-              output=template;
-            }
-            a: ID
-              -> { stringtemplate3.StringTemplate("hello") }
-            ;
-
-            ID : 'a'..'z'+;
-            WS : (' '|'\n') {$channel=HIDDEN;} ;
-            '''
-            )
-
-        found = self.execParser(
-            grammar, 'a',
-            "abc"
-            )
-
-        self.failUnlessEqual("hello", found)
-
-
-    def testTemplateExpressionInAction(self):
-        grammar = textwrap.dedent(
-            r'''grammar T;
-            options {
-              language=Python;
-              output=template;
-            }
-            a: ID
-              { $st = %{"hello"} }
-            ;
-
-            ID : 'a'..'z'+;
-            WS : (' '|'\n') {$channel=HIDDEN;} ;
-            '''
-            )
-
-        found = self.execParser(
-            grammar, 'a',
-            "abc"
-            )
-
-        self.failUnlessEqual("hello", found)
-
-
-    def testTemplateExpressionInAction2(self):
-        grammar = textwrap.dedent(
-            r'''grammar T;
-            options {
-              language=Python;
-              output=template;
-            }
-            a: ID
-              {
-                res = %{"hello <foo>"}
-                %res.foo = "world";
-              }
-              -> { res }
-            ;
-
-            ID : 'a'..'z'+;
-            WS : (' '|'\n') {$channel=HIDDEN;} ;
-            '''
-            )
-
-        found = self.execParser(
-            grammar, 'a',
-            "abc"
-            )
-
-        self.failUnlessEqual("hello world", found)
-
-
-    def testIndirectTemplateConstructor(self):
-        templates = textwrap.dedent(
-            '''\
-            group T;
-            expr(args, op) ::= <<
-            [<args; separator={<op>}>]
-            >>
-            '''
-            )
-
-        group = stringtemplate3.StringTemplateGroup(
-            file=StringIO(templates),
-            lexer='angle-bracket'
-            )
-
-        grammar = textwrap.dedent(
-            r'''grammar T;
-            options {
-              language=Python;
-              output=template;
-            }
-            a: ID
-              {
-                $st = %({"expr"})(args={[1, 2, 3]}, op={"+"})
-              }
-            ;
-
-            ID : 'a'..'z'+;
-            WS : (' '|'\n') {$channel=HIDDEN;} ;
-            '''
-            )
-
-        found = self.execParser(
-            grammar, 'a',
-            "abc",
-            group
-            )
-
-        self.failUnlessEqual("[1+2+3]", found)
-
-
-    def testPredicates(self):
-        grammar = textwrap.dedent(
-            r'''grammar T3;
-            options {
-              language=Python;
-              output=template;
-            }
-            a : ID INT
-              -> {$ID.text=='a'}? template(int={$INT.text})
-                                  "A: <int>"
-              -> {$ID.text=='b'}? template(int={$INT.text})
-                                  "B: <int>"
-              ->                  template(int={$INT.text})
-                                  "C: <int>"
-            ;
-
-            ID : 'a'..'z'+;
-            INT : '0'..'9'+;
-            WS : (' '|'\n') {$channel=HIDDEN;} ;
-            '''
-            )
-
-        found = self.execParser(
-            grammar, 'a',
-            "b 34"
-            )
-
-        self.failUnlessEqual("B: 34", found)
-
-
-    def testBacktrackingMode(self):
-        grammar = textwrap.dedent(
-            r'''grammar T4;
-            options {
-              language=Python;
-              output=template;
-              backtrack=true;
-            }
-            a : (ID INT)=> ID INT
-              -> template(id={$ID.text}, int={$INT.text})
-                 "id=<id>, int=<int>"
-            ;
-
-            ID : 'a'..'z'+;
-            INT : '0'..'9'+;
-            WS : (' '|'\n') {$channel=HIDDEN;} ;
-            '''
-            )
-
-        found = self.execParser(
-            grammar, 'a',
-            "abc 34"
-            )
-
-        self.failUnlessEqual("id=abc, int=34", found)
-
-
-    def testRewrite(self):
-        grammar = textwrap.dedent(
-            r'''grammar T5;
-            options {
-              language=Python;
-              output=template;
-              rewrite=true;
-            }
-
-            prog: stat+;
-
-            stat
-                : 'if' '(' expr ')' stat
-                | 'return' return_expr ';'
-                | '{' stat* '}'
-                | ID '=' expr ';'
-                ;
-
-            return_expr
-                : expr
-                  -> template(t={$text}) <<boom(<t>)>>
-                ;
-
-            expr
-                : ID
-                | INT
-                ;
-
-            ID:  'a'..'z'+;
-            INT: '0'..'9'+;
-            WS: (' '|'\n')+ {$channel=HIDDEN;} ;
-            COMMENT: '/*' (options {greedy=false;} : .)* '*/' {$channel = HIDDEN;} ;
-            '''
-            )
-
-        input = textwrap.dedent(
-            '''\
-            if ( foo ) {
-              b = /* bla */ 2;
-              return 1 /* foo */;
-            }
-
-            /* gnurz */
-            return 12;
-            '''
-            )
-
-        lexerCls, parserCls = self.compileInlineGrammar(grammar)
-
-        cStream = antlr3.StringStream(input)
-        lexer = lexerCls(cStream)
-        tStream = antlr3.TokenRewriteStream(lexer)
-        parser = parserCls(tStream)
-        result = parser.prog()
-
-        found = tStream.toString()
-
-        expected = textwrap.dedent(
-            '''\
-            if ( foo ) {
-              b = /* bla */ 2;
-              return boom(1) /* foo */;
-            }
-
-            /* gnurz */
-            return boom(12);
-            '''
-            )
-
-        self.failUnlessEqual(expected, found)
-
-
-    def testTreeRewrite(self):
-        grammar = textwrap.dedent(
-            r'''grammar T6;
-            options {
-              language=Python;
-              output=AST;
-            }
-
-            tokens {
-              BLOCK;
-              ASSIGN;
-            }
-
-            prog: stat+;
-
-            stat
-                : IF '(' e=expr ')' s=stat
-                  -> ^(IF $e $s)
-                | RETURN expr ';'
-                  -> ^(RETURN expr)
-                | '{' stat* '}'
-                  -> ^(BLOCK stat*)
-                | ID '=' expr ';'
-                  -> ^(ASSIGN ID expr)
-                ;
-
-            expr
-                : ID
-                | INT
-                ;
-
-            IF: 'if';
-            RETURN: 'return';
-            ID:  'a'..'z'+;
-            INT: '0'..'9'+;
-            WS: (' '|'\n')+ {$channel=HIDDEN;} ;
-            COMMENT: '/*' (options {greedy=false;} : .)* '*/' {$channel = HIDDEN;} ;
-            '''
-            )
-
-        treeGrammar = textwrap.dedent(
-            r'''tree grammar T6Walker;
-            options {
-              language=Python;
-              tokenVocab=T6;
-              ASTLabelType=CommonTree;
-              output=template;
-              rewrite=true;
-            }
-
-            prog: stat+;
-
-            stat
-                : ^(IF expr stat)
-                | ^(RETURN return_expr)
-                | ^(BLOCK stat*)
-                | ^(ASSIGN ID expr)
-                ;
-
-            return_expr
-                : expr
-                  -> template(t={$text}) <<boom(<t>)>>
-                ;
-
-            expr
-                : ID
-                | INT
-                ;
-            '''
-            )
-
-        input = textwrap.dedent(
-            '''\
-            if ( foo ) {
-              b = /* bla */ 2;
-              return 1 /* foo */;
-            }
-
-            /* gnurz */
-            return 12;
-            '''
-            )
-
-        lexerCls, parserCls = self.compileInlineGrammar(grammar)
-        walkerCls = self.compileInlineGrammar(treeGrammar)
-
-        cStream = antlr3.StringStream(input)
-        lexer = lexerCls(cStream)
-        tStream = antlr3.TokenRewriteStream(lexer)
-        parser = parserCls(tStream)
-        tree = parser.prog().tree
-        nodes = antlr3.tree.CommonTreeNodeStream(tree)
-        nodes.setTokenStream(tStream)
-        walker = walkerCls(nodes)
-        walker.prog()
-
-        found = tStream.toString()
-
-        expected = textwrap.dedent(
-            '''\
-            if ( foo ) {
-              b = /* bla */ 2;
-              return boom(1) /* foo */;
-            }
-
-            /* gnurz */
-            return boom(12);
-            '''
-            )
-
-        self.failUnlessEqual(expected, found)
-
-
-if __name__ == '__main__':
-    unittest.main()
diff --git a/runtime/Python3/tests/t056lexer.py b/runtime/Python3/tests/t056lexer.py
deleted file mode 100644
index a53f92a..0000000
--- a/runtime/Python3/tests/t056lexer.py
+++ /dev/null
@@ -1,49 +0,0 @@
-import unittest
-import textwrap
-import antlr3
-import antlr3.tree
-import stringtemplate3
-import testbase
-import sys
-import os
-from StringIO import StringIO
-
-# FIXME: port other tests from TestLexer.java
-
-class T(testbase.ANTLRTest):
-    def execParser(self, grammar, grammarEntry, input):
-        lexerCls, parserCls = self.compileInlineGrammar(grammar)
-
-        cStream = antlr3.StringStream(input)
-        lexer = lexerCls(cStream)
-        tStream = antlr3.CommonTokenStream(lexer)
-        parser = parserCls(tStream)
-        result = getattr(parser, grammarEntry)()
-        return result
-
-
-    def testRefToRuleDoesNotSetChannel(self):
-        # this must set channel of A to HIDDEN.  $channel is local to rule
-        # like $type.
-        grammar = textwrap.dedent(
-            r'''
-            grammar P;
-            options {
-              language=Python;
-            }
-            a returns [foo]: A EOF { $foo = '\%s, channel=\%d' \% ($A.text, $A.channel); } ;
-            A : '-' WS I ;
-            I : '0'..'9'+ ;
-            WS : (' '|'\n') {$channel=HIDDEN;} ;
-            ''')
-
-        found = self.execParser(
-            grammar, 'a',
-            "- 34"
-            )
-
-        self.failUnlessEqual("- 34, channel=0", found)
-
-
-if __name__ == '__main__':
-    unittest.main()
diff --git a/runtime/Python3/unittests/testdottreegen.py b/runtime/Python3/unittests/testdottreegen.py
deleted file mode 100644
index 05b797e..0000000
--- a/runtime/Python3/unittests/testdottreegen.py
+++ /dev/null
@@ -1,74 +0,0 @@
-# -*- coding: utf-8 -*-
-
-import os
-import unittest
-from StringIO import StringIO
-import textwrap
-
-import stringtemplate3
-
-from antlr3.dottreegen import toDOT
-from antlr3.treewizard import TreeWizard
-from antlr3.tree import CommonTreeAdaptor
-
-
-class TestToDOT(unittest.TestCase):
-    """Test case for the toDOT function."""
-
-    def setUp(self):
-        self.adaptor = CommonTreeAdaptor()
-        self.tokens = [
-            "", "", "", "", "", "A", "B", "C", "D", "E", "ID", "VAR"
-            ]
-        self.wiz = TreeWizard(self.adaptor, self.tokens)
-
-
-    def testNone(self):
-        """toDOT()"""
-
-        treeST = stringtemplate3.StringTemplate(
-            template=(
-            "digraph {\n" +
-            "  $nodes$\n" +
-            "  $edges$\n" +
-            "}\n")
-            )
-
-        edgeST = stringtemplate3.StringTemplate(
-            template="$parent$ -> $child$\n"
-            )
-
-        tree = self.wiz.create("(A B (B C C) (B (C D D)))")
-        st = toDOT(tree, self.adaptor, treeST, edgeST)
-
-        result = st.toString()
-        expected = textwrap.dedent(
-            '''\
-            digraph {
-              n0 [label="A"];
-              n1 [label="B"];
-              n2 [label="B"];
-              n3 [label="C"];
-              n4 [label="C"];
-              n5 [label="B"];
-              n6 [label="C"];
-              n7 [label="D"];
-              n8 [label="D"];
-
-              n0 -> n1
-              n0 -> n2
-              n2 -> n3
-              n2 -> n4
-              n0 -> n5
-              n5 -> n6
-              n6 -> n7
-              n6 -> n8
-
-            }
-            '''
-            )
-        self.assertEqual(result, expected)
-
-
-if __name__ == "__main__":
-    unittest.main(testRunner=unittest.TextTestRunner(verbosity=2))
diff --git a/tool/src/main/resources/org/antlr/codegen/templates/Python3/Dbg.stg b/tool/src/main/resources/org/antlr/codegen/templates/Python3/Dbg.stg
index 222bdef..b1e24c4 100644
--- a/tool/src/main/resources/org/antlr/codegen/templates/Python3/Dbg.stg
+++ b/tool/src/main/resources/org/antlr/codegen/templates/Python3/Dbg.stg
@@ -185,7 +185,7 @@ public <name>(<inputStreamType> input, DebugEventListener dbg) {
try:
self._dbg.enterRule(self.getGrammarFileName(), "<ruleName>")
if self.getRuleLevel() == 0:
-        self._dbg.commence();
+        self._dbg.commence()
self.incRuleLevel()
<! ST uses zero-based columns, we want one-base !>
self._dbg.location(<ruleDescriptor.tree.line>, <ruleDescriptor.tree.charPositionInLine>+1)
@@ -307,7 +307,7 @@ try:
self.isCyclicDecision = True
<super.dfaDecision(...)>

-except NoViableAltException, nvae:
+except NoViableAltException as nvae:
self._dbg.recognitionException(nvae)
raise

diff --git a/tool/src/main/resources/org/antlr/codegen/templates/Python3/Python3.stg b/tool/src/main/resources/org/antlr/codegen/templates/Python3/Python3.stg
index 8f1436c..a4cb813 100644
--- a/tool/src/main/resources/org/antlr/codegen/templates/Python3/Python3.stg
+++ b/tool/src/main/resources/org/antlr/codegen/templates/Python3/Python3.stg
@@ -31,16 +31,6 @@
*/
apiVersion() ::= "1"

-// System.Boolean.ToString() returns "True" and "False", but the proper C# literals are "true" and "false"
-// The Java version of Boolean returns "true" and "false", so they map to themselves here.
-booleanLiteral ::= [
-	       "True":"true",
-	       "False":"false",
-	       "true":"true",
-	       "false":"false",
-	       default:"false"
-]
-
/** The overall file structure of a recognizer; stores methods for rules
*  and cyclic DFAs plus support code.
*/
@@ -60,7 +50,6 @@ from antlr3 import *
<if(TREE_PARSER)>
from antlr3.tree import *<\n>
<endif>
-from antlr3.compat import set, frozenset
<@end>

<actions.(actionScope).header>
@@ -106,8 +95,14 @@ if __name__ == '__main__':

lexer(grammar, name, tokens, scopes, rules, numRules, filterMode,
labelType="CommonToken", superClass="Lexer") ::= <<
-<grammar.directDelegates:
- {g|from <g.recognizerName> import <g.recognizerName>}; separator="\n">
+<if(grammar.directDelegates)>
+if __name__ == "__main__":
+    <grammar.directDelegates:
+     {g|from <g.recognizerName> import <g.recognizerName>}; separator="\n">
+else:
+    <grammar.directDelegates:
+     {g|from .<g.recognizerName> import <g.recognizerName>}; separator="\n">
+<endif>

class <grammar.recognizerName>(<@superClassName><superClass><@end>):
<scopes:{it|<if(it.isDynamicGlobalScope)><globalAttributeScope(scope=it)><endif>}>
@@ -118,7 +113,7 @@ class <grammar.recognizerName>(<@superClassName><superClass><@end>):
def __init__(self<grammar.delegators:{g|, <g:delegateName()>}>, input=None, state=None):
if state is None:
state = RecognizerSharedState()
-        super(<grammar.recognizerName>, self).__init__(input, state)
+        super().__init__(input, state)

<if(memoize)>
<if(grammar.grammarIsRoot)>
@@ -133,7 +128,7 @@ class <grammar.recognizerName>(<@superClassName><superClass><@end>):
<grammar.delegators:
{g|self.<g:delegateName()> = <g:delegateName()>}; separator="\n">
<last(grammar.delegators):
-    	 {g|self.gParent = <g:delegateName()>}; separator="\n">
+         {g|self.gParent = <g:delegateName()>}; separator="\n">
self.delegates = [<grammar.delegates: {g|self.<g:delegateName()>}; separator = ", ">]

<cyclicDFAs:{dfa | <cyclicDFAInit(dfa)>}; separator="\n">
@@ -194,7 +189,7 @@ def nextToken(self):
self.emit()
return self._state.token

-        except RecognitionException, re:
+        except RecognitionException as re:
# shouldn't happen in backtracking mode, but...
self.reportError(re)
self.recover(re)
@@ -203,12 +198,12 @@ def nextToken(self):
def memoize(self, input, ruleIndex, ruleStartIndex, success):
if self._state.backtracking > 1:
# is Lexer always superclass?
-        super(<grammar.recognizerName>, self).memoize(input, ruleIndex, ruleStartIndex, success)
+        super().memoize(input, ruleIndex, ruleStartIndex, success)


def alreadyParsedRule(self, input, ruleIndex):
if self._state.backtracking > 1:
-        return super(<grammar.recognizerName>, self).alreadyParsedRule(input, ruleIndex)
+        return super().alreadyParsedRule(input, ruleIndex)
return False


@@ -222,7 +217,7 @@ filteringActionGate() ::= "self._state.backtracking == 1"

genericParser(grammar, name, scopes, tokens, tokenNames, rules, numRules,
bitsets, inputStreamType, superClass, labelType, members,
-	      rewriteElementType, filterMode, init, ASTLabelType="Object") ::= <<
+              rewriteElementType, filterMode, init, ASTLabelType="Object") ::= <<
<if(grammar.grammarIsRoot)>
# token names
tokenNames = [
@@ -234,8 +229,18 @@ from <grammar.composite.rootGrammar.recognizerName> import tokenNames<\n>
<endif>
<scopes:{it|<if(it.isDynamicGlobalScope)><globalAttributeScopeClass(scope=it)><endif>}>

-<grammar.directDelegates:
- {g|from <g.recognizerName> import <g.recognizerName>}; separator="\n">
+<! I just want to say this is an awful hack. The delegates' grammars need to
+   import tokenNames from us, and it works despite the circular dependency
+   because we define it before we import them. !>
+
+<if(grammar.directDelegates)>
+if __name__ == "__main__":
+    <grammar.directDelegates:
+     {g|from <g.recognizerName> import <g.recognizerName>}; separator="\n">
+else:
+    <grammar.directDelegates:
+     {g|from .<g.recognizerName> import <g.recognizerName>}; separator="\n">
+<endif>

<rules:{it|<ruleAttributeScopeClass(scope=it.ruleDescriptor.ruleScope)>}>

@@ -249,7 +254,7 @@ class <grammar.recognizerName>(<@superClassName><superClass><@end>):
state = RecognizerSharedState()

<@args()>
-        super(<grammar.recognizerName>, self).__init__(input, state, *args, **kwargs)
+        super().__init__(input, state, *args, **kwargs)

<if(memoize)>
<if(grammar.grammarIsRoot)>
@@ -260,7 +265,7 @@ class <grammar.recognizerName>(<@superClassName><superClass><@end>):
<cyclicDFAs:{dfa | <cyclicDFAInit(dfa)>}; separator="\n">

<scopes:{it | <if(it.isDynamicGlobalScope)><globalAttributeScopeStack(scope=it)><endif>}>
-	<rules:{it | <ruleAttributeScopeStack(scope=it.ruleDescriptor.ruleScope)>}>
+        <rules:{it | <ruleAttributeScopeStack(scope=it.ruleDescriptor.ruleScope)>}>

<init>

@@ -271,10 +276,10 @@ class <grammar.recognizerName>(<@superClassName><superClass><@end>):
<grammar.directDelegates:
{g|<g.delegates:{h|self.<h:delegateName()> = self.<g:delegateName()>.<h:delegateName()>}; separator="\n">}; separator="\n">
<last(grammar.delegators):
-    	 {g|self.gParent = self.<g:delegateName()>}; separator="\n">
+         {g|self.gParent = self.<g:delegateName()>}; separator="\n">
self.delegates = [<grammar.delegates: {g|self.<g:delegateName()>}; separator = ", ">]

-	<@init><@end>
+        <@init><@end>


<@members><@end>
@@ -320,7 +325,7 @@ treeParser(grammar, name, scopes, tokens, tokenNames, globalAction, rules,
numRules, bitsets, filterMode, labelType={<ASTLabelType>}, ASTLabelType="Object",
superClass={<if(filterMode)><if(buildAST)>TreeRewriter<else>TreeFilter<endif><else>TreeParser<endif>},
members={<actions.treeparser.members>},
-	   init={<actions.treeparser.init>}
+           init={<actions.treeparser.init>}
) ::= <<
<genericParser(grammar, name, scopes, tokens, tokenNames, rules, numRules,
bitsets, "TreeNodeStream", superClass,
@@ -448,7 +453,7 @@ try:
<if(actions.(actionScope).rulecatch)>
<actions.(actionScope).rulecatch>
<else>
-    except RecognitionException, re:
+    except RecognitionException as re:
self.reportError(re)
self.recover(self.input, re)
<@setErrorReturnValue()>
@@ -603,7 +608,7 @@ def m<ruleName>(self, <ruleDescriptor.parameterScope:parameterScope()>):
<if(trace)>
self.traceOut("<ruleName>", <ruleDescriptor.index>)<\n>
<endif>
-	<ruleScopeCleanUp()>
+        <ruleScopeCleanUp()>
<memoize()>
pass

@@ -844,8 +849,8 @@ self.match(<string>)
<label>StartLine<elementIndex> = self.getLine()
<label>StartCharPos<elementIndex> = self.getCharPositionInLine()
<label> = <labelType>(input=self.input, type=INVALID_TOKEN_TYPE, channel=DEFAULT_CHANNEL, start=<label>Start, stop=self.getCharIndex()-1)
-<label>.setLine(<label>StartLine<elementIndex>)
-<label>.setCharPositionInLine(<label>StartCharPos<elementIndex>)
+<label>.line = <label>StartLine<elementIndex>
+<label>.charPositionInLine = <label>StartCharPos<elementIndex>
<else>
self.match(<string>)
<endif>
@@ -855,7 +860,7 @@ wildcard(token,label,elementIndex,terminalOptions) ::= <<
<if(label)>
<label> = self.input.LT(1)<\n>
<endif>
-self.matchAny(self.input)
+self.matchAny()
>>

wildcardAndListLabel(token,label,elementIndex,terminalOptions) ::= <<
@@ -911,8 +916,8 @@ self.<if(scope)><scope:delegateName()>.<endif>m<rule.name>(<args; separator=", "
channel=DEFAULT_CHANNEL,
start=<label>Start<elementIndex>,
stop=self.getCharIndex()-1)
-<label>.setLine(<label>StartLine<elementIndex>)
-<label>.setCharPositionInLine(<label>StartCharPos<elementIndex>)
+<label>.line = <label>StartLine<elementIndex>
+<label>.charPositionInLine = <label>StartCharPos<elementIndex>
<else>
self.<if(scope)><scope:delegateName()>.<endif>m<rule.name>(<args; separator=", ">)
<endif>
@@ -932,8 +937,8 @@ lexerMatchEOF(label,elementIndex) ::= <<
<label>StartCharPos<elementIndex> = self.getCharPositionInLine()
self.match(EOF)
<label> = <labelType>(input=self.input, type=EOF, channel=DEFAULT_CHANNEL, start=<label>Start<elementIndex>, stop=self.getCharIndex()-1)
-<label>.setLine(<label>StartLine<elementIndex>)
-<label>.setCharPositionInLine(<label>StartCharPos<elementIndex>)
+<label>.line = <label>StartLine<elementIndex>
+<label>.charPositionInLine = <label>StartCharPos<elementIndex>
<else>
self.match(EOF)
<endif>
@@ -1099,32 +1104,32 @@ cyclicDFA(dfa) ::= <<
# lookup tables for DFA #<dfa.decisionNumber>

DFA<dfa.decisionNumber>_eot = DFA.unpack(
-    u"<dfa.javaCompressedEOT; wrap="\"\n    u\"">"
+    "<dfa.javaCompressedEOT; wrap="\"\n    \"">"
)

DFA<dfa.decisionNumber>_eof = DFA.unpack(
-    u"<dfa.javaCompressedEOF; wrap="\"\n    u\"">"
+    "<dfa.javaCompressedEOF; wrap="\"\n    \"">"
)

DFA<dfa.decisionNumber>_min = DFA.unpack(
-    u"<dfa.javaCompressedMin; wrap="\"\n    u\"">"
+    "<dfa.javaCompressedMin; wrap="\"\n    \"">"
)

DFA<dfa.decisionNumber>_max = DFA.unpack(
-    u"<dfa.javaCompressedMax; wrap="\"\n    u\"">"
+    "<dfa.javaCompressedMax; wrap="\"\n    \"">"
)

DFA<dfa.decisionNumber>_accept = DFA.unpack(
-    u"<dfa.javaCompressedAccept; wrap="\"\n    u\"">"
+    "<dfa.javaCompressedAccept; wrap="\"\n    \"">"
)

DFA<dfa.decisionNumber>_special = DFA.unpack(
-    u"<dfa.javaCompressedSpecial; wrap="\"\n    u\"">"
+    "<dfa.javaCompressedSpecial; wrap="\"\n    \"">"
)


DFA<dfa.decisionNumber>_transition = [
-    <dfa.javaCompressedTransition:{s|DFA.unpack(u"<s; wrap="\"\nu\"">")}; separator=",\n">
+    <dfa.javaCompressedTransition:{s|DFA.unpack("<s; wrap="\"\n\"">")}; separator=",\n">
]

# class definition for DFA #<dfa.decisionNumber>
@@ -1286,7 +1291,7 @@ returnScope(scope) ::= <<
<if(ruleDescriptor.hasMultipleReturnValues)>
class <ruleDescriptor:returnStructName()>(<if(TREE_PARSER)>Tree<else>Parser<endif>RuleReturnScope):
def __init__(self):
-        super(<grammar.recognizerName>.<ruleDescriptor:returnStructName()>, self).__init__()
+        super().__init__()

<scope.attributes:{it | self.<it.decl> = None}; separator="\n">
<@ruleReturnInit()>

