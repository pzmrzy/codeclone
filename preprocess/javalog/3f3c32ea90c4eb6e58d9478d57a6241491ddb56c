commit 3f3c32ea90c4eb6e58d9478d57a6241491ddb56c
Author:     Benjamin Niemann <pink@odahoda.de>
AuthorDate: Fri May 13 08:47:57 2011 -0800
Commit:     Benjamin Niemann <pink@odahoda.de>
CommitDate: Fri May 13 08:47:57 2011 -0800

[Python] Almost there, but the ST4 conversion broke everything again ;)

[git-p4: depot-paths = "//depot/code/antlr/antlr3-main/": change = 8475]

diff --git a/pom.xml b/pom.xml
index 549f14c..642287c 100644
--- a/pom.xml
+++ b/pom.xml
@@ -118,7 +118,7 @@
<dependency>
<groupId>org.antlr</groupId>
<artifactId>stringtemplate</artifactId>
-                <version>3.2.1</version>
+                <version>4.0.2</version>
<scope>compile</scope>
</dependency>

diff --git a/runtime/Python/antlr3/debug.py b/runtime/Python/antlr3/debug.py
index a71b0d1..6668fa5 100644
--- a/runtime/Python/antlr3/debug.py
+++ b/runtime/Python/antlr3/debug.py
@@ -940,7 +940,8 @@ class DebugEventSocketProxy(DebugEventListener):


def enterDecision(self, decisionNumber, couldBacktrack):
-        self.transmit("enterDecision\t%d" % decisionNumber)
+        self.transmit(
+            "enterDecision\t%d\t%d" % (decisionNumber, couldBacktrack))


def exitDecision(self, decisionNumber):
diff --git a/runtime/Python/antlr3/main.py b/runtime/Python/antlr3/main.py
index 19a08b5..ae3906f 100644
--- a/runtime/Python/antlr3/main.py
+++ b/runtime/Python/antlr3/main.py
@@ -43,7 +43,7 @@ class _Main(object):
self.stdout = sys.stdout
self.stderr = sys.stderr

-
+
def parseOptions(self, argv):
optParser = optparse.OptionParser()
optParser.add_option(
@@ -92,7 +92,7 @@ class _Main(object):
)

self.setupOptions(optParser)
-
+
return optParser.parse_args(argv[1:])


@@ -104,7 +104,7 @@ class _Main(object):
options, args = self.parseOptions(argv)

self.setUp(options)
-
+
if options.interactive:
while True:
try:
@@ -112,10 +112,10 @@ class _Main(object):
except (EOFError, KeyboardInterrupt):
self.stdout.write("\nBye.\n")
break
-
+
inStream = antlr3.ANTLRStringStream(input)
self.parseStream(options, inStream)
-
+
else:
if options.input is not None:
inStream = antlr3.ANTLRStringStream(options.input)
@@ -166,7 +166,7 @@ class _Main(object):
def setUp(self, options):
pass

-
+
def parseStream(self, options, inStream):
raise NotImplementedError

@@ -185,8 +185,8 @@ class LexerMain(_Main):
_Main.__init__(self)

self.lexerClass = lexerClass
-
-
+
+
def parseStream(self, options, inStream):
lexer = self.lexerClass(inStream)
for token in lexer:
@@ -200,8 +200,8 @@ class ParserMain(_Main):
self.lexerClassName = lexerClassName
self.lexerClass = None
self.parserClass = parserClass
-
-
+
+
def setupOptions(self, optParser):
optParser.add_option(
"--lexer",
@@ -222,7 +222,7 @@ class ParserMain(_Main):
lexerMod = __import__(options.lexerClass)
self.lexerClass = getattr(lexerMod, options.lexerClass)

-
+
def parseStream(self, options, inStream):
kwargs = {}
if options.port is not None:
@@ -235,9 +235,8 @@ class ParserMain(_Main):
parser = self.parserClass(tokenStream, **kwargs)
result = getattr(parser, options.parserRule)()
if result is not None:
-            if hasattr(result, 'tree'):
-                if result.tree is not None:
-                    self.writeln(options, result.tree.toStringTree())
+            if hasattr(result, 'tree') and result.tree is not None:
+                self.writeln(options, result.tree.toStringTree())
else:
self.writeln(options, repr(result))

@@ -249,8 +248,8 @@ class WalkerMain(_Main):
self.lexerClass = None
self.parserClass = None
self.walkerClass = walkerClass
-
-
+
+
def setupOptions(self, optParser):
optParser.add_option(
"--lexer",
@@ -287,7 +286,7 @@ class WalkerMain(_Main):
parserMod = __import__(options.parserClass)
self.parserClass = getattr(parserMod, options.parserClass)

-
+
def parseStream(self, options, inStream):
lexer = self.lexerClass(inStream)
tokenStream = antlr3.CommonTokenStream(lexer)
@@ -304,4 +303,3 @@ class WalkerMain(_Main):
self.writeln(options, result.tree.toStringTree())
else:
self.writeln(options, repr(result))
-
diff --git a/runtime/Python/antlr3/recognizers.py b/runtime/Python/antlr3/recognizers.py
index f7c295d..d48280a 100644
--- a/runtime/Python/antlr3/recognizers.py
+++ b/runtime/Python/antlr3/recognizers.py
@@ -415,6 +415,9 @@ class BaseRecognizer(object):
What is the error header, normally line/character position information?
"""

+        source_name = self.getSourceName()
+        if source_name is not None:
+            return "%s line %d:%d" % (source_name, e.line, e.charPositionInLine)
return "line %d:%d" % (e.line, e.charPositionInLine)


@@ -1467,6 +1470,7 @@ class ParserRuleReturnScope(RuleReturnScope):
def __init__(self):
self.start = None
self.stop = None
+        self.tree = None  # only used when output=AST


def getStart(self):
@@ -1475,3 +1479,7 @@ class ParserRuleReturnScope(RuleReturnScope):

def getStop(self):
return self.stop
+
+
+    def getTree(self):
+        return self.tree
diff --git a/runtime/Python/antlr3/streams.py b/runtime/Python/antlr3/streams.py
index ce1929a..c9ba7ca 100644
--- a/runtime/Python/antlr3/streams.py
+++ b/runtime/Python/antlr3/streams.py
@@ -955,6 +955,11 @@ class RewriteOperation(object):

def __init__(self, stream, index, text):
self.stream = stream
+
+        # What index into rewrites List are we?
+        self.instructionIndex = None
+
+        # Token buffer index.
self.index = index
self.text = text

@@ -967,7 +972,8 @@ class RewriteOperation(object):

def toString(self):
opName = self.__class__.__name__
-        return '<%s@%d:"%s">' % (opName, self.index, self.text)
+        return '<%s@%d:"%s">' % (
+            opName, self.index, self.text)

__str__ = toString
__repr__ = toString
@@ -1004,6 +1010,9 @@ class ReplaceOp(RewriteOperation):


def toString(self):
+        if self.text is None:
+            return '<DeleteOp@%d..%d>' % (self.index, self.lastIndex)
+
return '<ReplaceOp@%d..%d:"%s">' % (
self.index, self.lastIndex, self.text)

@@ -1011,22 +1020,6 @@ class ReplaceOp(RewriteOperation):
__repr__ = toString


-class DeleteOp(ReplaceOp):
-    """
-    @brief Internal helper class.
-    """
-
-    def __init__(self, stream, first, last):
-        ReplaceOp.__init__(self, stream, first, last, None)
-
-
-    def toString(self):
-        return '<DeleteOp@%d..%d>' % (self.index, self.lastIndex)
-
-    __str__ = toString
-    __repr__ = toString
-
-
class TokenRewriteStream(CommonTokenStream):
"""@brief CommonTokenStream that can be modified.

@@ -1168,6 +1161,7 @@ class TokenRewriteStream(CommonTokenStream):

op = InsertBeforeOp(self, index, text)
rewrites = self.getProgram(programName)
+        op.instructionIndex = len(rewrites)
rewrites.append(op)


@@ -1203,11 +1197,12 @@ class TokenRewriteStream(CommonTokenStream):

if first > last or first < 0 or last < 0 or last >= len(self.tokens):
raise ValueError(
-                "replace: range invalid: "+first+".."+last+
-                "(size="+len(self.tokens)+")")
+                "replace: range invalid: %d..%d (size=%d)"
+                % (first, last, len(self.tokens)))

op = ReplaceOp(self, first, last, text)
rewrites = self.getProgram(programName)
+        op.instructionIndex = len(rewrites)
rewrites.append(op)


@@ -1352,8 +1347,16 @@ class TokenRewriteStream(CommonTokenStream):
R.i-j.u R.x-y.v | x-y in i-j          ERROR
R.i-j.u R.x-y.v | boundaries overlap  ERROR

-        I.i.u R.x-y.v   | i in x-y            delete I
-        I.i.u R.x-y.v   | i not in x-y        leave alone, nonoverlapping
+        Delete special case of replace (text==null):
+        D.i-j.u D.x-y.v |                     boundaries overlapcombine to
+                                              max(min)..max(right)
+
+        I.i.u R.x-y.v   |                     i in (x+1)-ydelete I (since
+                                              insert before we're not deleting
+                                              i)
+        I.i.u R.x-y.v   |                     i not in (x+1)-yleave alone,
+                                              nonoverlapping
+
R.x-y.v I.i.u   | i in x-y            ERROR
R.x-y.v I.x.u                         R.x-y.uv (combine, delete I)
R.x-y.v I.i.u   | i not in x-y        leave alone, nonoverlapping
@@ -1396,14 +1399,22 @@ class TokenRewriteStream(CommonTokenStream):

# Wipe prior inserts within range
for j, iop in self.getKindOfOps(rewrites, InsertBeforeOp, i):
-                if iop.index >= rop.index and iop.index <= rop.lastIndex:
-                    rewrites[j] = None  # delete insert as it's a no-op.
+                if iop.index == rop.index:
+                    # E.g., insert before 2, delete 2..2; update replace
+                    # text to include insert before, kill insert
+                    rewrites[iop.instructionIndex] = None
+                    rop.text = self.catOpText(iop.text, rop.text)
+
+                elif iop.index > rop.index and iop.index <= rop.lastIndex:
+                    # delete insert as it's a no-op.
+                    rewrites[j] = None

# Drop any prior replaces contained within
for j, prevRop in self.getKindOfOps(rewrites, ReplaceOp, i):
if (prevRop.index >= rop.index
and prevRop.lastIndex <= rop.lastIndex):
-                    rewrites[j] = None  # delete replace as it's a no-op.
+                    # delete replace as it's a no-op.
+                    rewrites[j] = None
continue

# throw exception unless disjoint or identical
@@ -1411,7 +1422,18 @@ class TokenRewriteStream(CommonTokenStream):
or prevRop.index > rop.lastIndex)
same = (prevRop.index == rop.index
and prevRop.lastIndex == rop.lastIndex)
-                if not disjoint and not same:
+
+                # Delete special case of replace (text==null):
+                # D.i-j.u D.x-y.v| boundaries overlapcombine to
+                # max(min)..max(right)
+                if prevRop.text is None and rop.text is None and not disjoint:
+                    # kill first delete
+                    rewrites[prevRop.instructionIndex] = None
+
+                    rop.index = min(prevRop.index, rop.index)
+                    rop.lastIndex = max(prevRop.lastIndex, rop.lastIndex)
+
+                elif not disjoint and not same:
raise ValueError(
"replace op boundaries of %s overlap with previous %s"
% (rop, prevRop))
@@ -1431,13 +1453,15 @@ class TokenRewriteStream(CommonTokenStream):
# whole token buffer so no lazy eval issue with any
# templates
iop.text = self.catOpText(iop.text, prevIop.text)
-                    rewrites[j] = None  # delete redundant prior insert
+                    # delete redundant prior insert
+                    rewrites[j] = None

# look for replaces where iop.index is in range; error
for j, rop in self.getKindOfOps(rewrites, ReplaceOp, i):
if iop.index == rop.index:
rop.text = self.catOpText(iop.text, rop.text)
-                    rewrites[i] = None  # delete current insert
+                    # delete current insert
+                    rewrites[i] = None
continue

if iop.index >= rop.index and iop.index <= rop.lastIndex:
@@ -1448,7 +1472,8 @@ class TokenRewriteStream(CommonTokenStream):
m = {}
for i, op in enumerate(rewrites):
if op is None:
-                continue # ignore deleted ops
+                # ignore deleted ops
+                continue

assert op.index not in m, "should only be one op per index"
m[op.index] = op
@@ -1467,6 +1492,8 @@ class TokenRewriteStream(CommonTokenStream):


def getKindOfOps(self, rewrites, kind, before=None):
+        """Get all operations before an index of a particular kind."""
+
if before is None:
before = len(rewrites)
elif before > len(rewrites):
diff --git a/runtime/Python/antlr3/tree.py b/runtime/Python/antlr3/tree.py
index 86c796f..7bc8446 100644
--- a/runtime/Python/antlr3/tree.py
+++ b/runtime/Python/antlr3/tree.py
@@ -2790,6 +2790,9 @@ class RewriteRuleSubtreeStream(RewriteRuleElementStream):

# test size above then fetch
el = self._next()
+        while self.adaptor.isNil(el) and self.adaptor.getChildCount(el) == 1:
+            el = self.adaptor.getChild(el, 0)
+
# dup just the root (want node here)
return self.adaptor.dupNode(el)

diff --git a/runtime/Python/antlr3/treewizard.py b/runtime/Python/antlr3/treewizard.py
index 1d8f4eb..d96ce78 100644
--- a/runtime/Python/antlr3/treewizard.py
+++ b/runtime/Python/antlr3/treewizard.py
@@ -89,7 +89,7 @@ class TreePatternLexer(object):
'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_'
)
__idChar = __idStartChar | frozenset('0123456789')
-
+
def nextToken(self):
self.sval = ""
while self.c != EOF:
@@ -213,7 +213,7 @@ class TreePatternParser(object):
def parseNode(self):
# "%label:" prefix
label = None
-
+
if self.ttype == PERCENT:
self.ttype = self.tokenizer.nextToken()
if self.ttype != ID:
@@ -223,7 +223,7 @@ class TreePatternParser(object):
self.ttype = self.tokenizer.nextToken()
if self.ttype != COLON:
return None
-
+
self.ttype = self.tokenizer.nextToken() # move to ID following colon

# Wildcard?
@@ -241,7 +241,7 @@ class TreePatternParser(object):

tokenName = self.tokenizer.sval
self.ttype = self.tokenizer.nextToken()
-
+
if tokenName == "nil":
return self.adaptor.nil()

@@ -279,12 +279,12 @@ class TreePattern(CommonTree):

self.label = None
self.hasTextArg = None
-
+

def toString(self):
if self.label is not None:
return '%' + self.label + ':' + CommonTree.toString(self)
-
+
else:
return CommonTree.toString(self)

@@ -320,7 +320,12 @@ class TreeWizard(object):
"""

def __init__(self, adaptor=None, tokenNames=None, typeMap=None):
-        self.adaptor = adaptor
+        if adaptor is None:
+            self.adaptor = CommonTreeAdaptor()
+
+        else:
+            self.adaptor = adaptor
+
if typeMap is None:
self.tokenNameToTypeMap = computeTokenTypes(tokenNames)

@@ -344,18 +349,18 @@ class TreeWizard(object):
"""
Create a tree or node from the indicated tree pattern that closely
follows ANTLR tree grammar tree element syntax:
-
+
(root child1 ... child2).
-
+
You can also just pass in a node: ID
-
+
Any node can have a text argument: ID[foo]
(notice there are no quotes around foo--it's clear it's a string).
-
+
nil is a special name meaning "give me a nil node".  Useful for
making lists: (nil A B C) is a list of A B C.
"""
-
+
tokenizer = TreePatternLexer(pattern)
parser = TreePatternParser(tokenizer, self, self.adaptor)
return parser.pattern()
@@ -363,7 +368,7 @@ class TreeWizard(object):

def index(self, tree):
"""Walk the entire tree and make a node name to nodes mapping.
-
+
For now, use recursion but later nonrecursive version may be
more efficient.  Returns a dict int -> list where the list is
of your AST node type.  The int is the token type of the node.
@@ -396,9 +401,9 @@ class TreeWizard(object):

what may either be an integer specifzing the token type to find or
a string with a pattern that must be matched.
-
+
"""
-
+
if isinstance(what, (int, long)):
return self._findTokenType(tree, what)

@@ -424,14 +429,14 @@ class TreeWizard(object):

def _findPattern(self, t, pattern):
"""Return a List of subtrees matching pattern."""
-
+
subtrees = []
-
+
# Create a TreePattern from the pattern
tokenizer = TreePatternLexer(pattern)
parser = TreePatternParser(tokenizer, self, TreePatternTreeAdaptor())
tpattern = parser.pattern()
-
+
# don't allow invalid patterns
if (tpattern is None or tpattern.isNil()
or isinstance(tpattern, WildcardTreePattern)):
@@ -442,7 +447,7 @@ class TreeWizard(object):
def visitor(tree, parent, childIndex, label):
if self._parse(tree, tpattern, None):
subtrees.append(tree)
-
+
self.visit(t, rootTokenType, visitor)

return subtrees
@@ -472,11 +477,11 @@ class TreeWizard(object):

else:
raise TypeError("'what' must be string or integer")
-
-
+
+
def _visitType(self, t, parent, childIndex, ttype, visitor):
"""Do the recursive work for visit"""
-
+
if t is None:
return

@@ -497,7 +502,7 @@ class TreeWizard(object):
tokenizer = TreePatternLexer(pattern)
parser = TreePatternParser(tokenizer, self, TreePatternTreeAdaptor())
tpattern = parser.pattern()
-
+
# don't allow invalid patterns
if (tpattern is None or tpattern.isNil()
or isinstance(tpattern, WildcardTreePattern)):
@@ -509,9 +514,9 @@ class TreeWizard(object):
labels = {}
if self._parse(tree, tpattern, labels):
visitor(tree, parent, childIndex, labels)
-
+
self.visit(tree, rootTokenType, rootvisitor)
-
+

def parse(self, t, pattern, labels=None):
"""
@@ -539,7 +544,7 @@ class TreeWizard(object):
text arguments on nodes.  Fill labels map with pointers to nodes
in tree matched against nodes in pattern with labels.
"""
-
+
# make sure both are non-null
if t1 is None or tpattern is None:
return False
@@ -578,7 +583,7 @@ class TreeWizard(object):
Compare t1 and t2; return true if token types/text, structure match
exactly.
The trees are examined in their entirety so that (A B) does not match
-        (A B C) nor (A (B C)).
+        (A B C) nor (A (B C)).
"""

if adaptor is None:
@@ -598,7 +603,7 @@ class TreeWizard(object):

if adaptor.getText(t1) != adaptor.getText(t2):
return False
-
+
# check children
n1 = adaptor.getChildCount(t1)
n2 = adaptor.getChildCount(t2)
diff --git a/runtime/Python/setup.py b/runtime/Python/setup.py
index daa283a..c68b919 100644
--- a/runtime/Python/setup.py
+++ b/runtime/Python/setup.py
@@ -210,6 +210,7 @@ class functest(Command):
classpath.extend([
os.path.join(rootDir, 'lib', 'antlr-2.7.7.jar'),
os.path.join(rootDir, 'lib', 'stringtemplate-3.2.1.jar'),
+            os.path.join(rootDir, 'lib', 'ST-4.0.2.jar'),
os.path.join(rootDir, 'lib', 'junit-4.2.jar')
])
os.environ['CLASSPATH'] = ':'.join(classpath)
@@ -221,17 +222,15 @@ class functest(Command):

# collect tests from all tests/t*.py files
testFiles = []
-        for testPath in glob.glob(os.path.join(testDir, 't*.py')):
-            if (testPath.endswith('Lexer.py')
-                or testPath.endswith('Parser.py')
-                ):
+        test_glob = 't[0-9][0-9][0-9]*.py'
+        for testPath in glob.glob(os.path.join(testDir, test_glob)):
+            if testPath.endswith('Lexer.py') or testPath.endswith('Parser.py'):
continue

# if a single testcase has been selected, filter out all other
# tests
if (self.testcase is not None
-                and os.path.basename(testPath)[:-3] != self.testcase
-                ):
+                and not os.path.basename(testPath)[:-3].startswith(self.testcase)):
continue

testFiles.append(testPath)
@@ -245,20 +244,17 @@ class functest(Command):
= imp.find_module(testID, [testDir])

testMod = imp.load_module(
-                    testID, modFile, modPathname, modDescription
-                    )
+                    testID, modFile, modPathname, modDescription)

suite.addTests(
-                    unittest.defaultTestLoader.loadTestsFromModule(testMod)
-                    )
+                    unittest.defaultTestLoader.loadTestsFromModule(testMod))

except Exception:
buf = StringIO.StringIO()
traceback.print_exc(file=buf)

loadFailures.append(
-                    (os.path.basename(testPath), buf.getvalue())
-                    )
+                    (os.path.basename(testPath), buf.getvalue()))

if self.xml_output:
import xmlrunner
diff --git a/runtime/Python/tests/t009lexer.py b/runtime/Python/tests/t009lexer.py
index a385868..c32cbbf 100644
--- a/runtime/Python/tests/t009lexer.py
+++ b/runtime/Python/tests/t009lexer.py
@@ -5,8 +5,8 @@ import unittest
class t009lexer(testbase.ANTLRTest):
def setUp(self):
self.compileGrammar()
-
-
+
+
def lexerClass(self, base):
class TLexer(base):
def emitErrorMessage(self, msg):
@@ -18,8 +18,8 @@ class t009lexer(testbase.ANTLRTest):
raise re

return TLexer
-
-
+
+
def testValid(self):
stream = antlr3.StringStream('085')
lexer = self.getLexer(stream)
@@ -55,15 +55,13 @@ class t009lexer(testbase.ANTLRTest):
token = lexer.nextToken()
raise AssertionError, token

-        except antlr3.MismatchedRangeException, exc:
-            assert exc.a == '0', repr(exc.a)
-            assert exc.b == '9', repr(exc.b)
+        except antlr3.MismatchedSetException, exc:
+            # TODO: This should provide more useful information
+            assert exc.expecting is None
assert exc.unexpectedType == 'a', repr(exc.unexpectedType)
assert exc.charPositionInLine == 1, repr(exc.charPositionInLine)
assert exc.line == 1, repr(exc.line)
-
+

if __name__ == '__main__':
unittest.main()
-
-
diff --git a/runtime/Python/tests/t048rewrite.py b/runtime/Python/tests/t048rewrite.py
index 276591b..685bf86 100644
--- a/runtime/Python/tests/t048rewrite.py
+++ b/runtime/Python/tests/t048rewrite.py
@@ -16,15 +16,15 @@ class T1(testbase.ANTLRTest):
cStream = antlr3.StringStream(input)
lexer = self.getLexer(cStream)
tStream = antlr3.TokenRewriteStream(lexer)
-        tStream.LT(1) # fill buffer
+        tStream.fillBuffer()

return tStream

-
+
def testInsertBeforeIndex0(self):
tokens = self._parse("abc")
tokens.insertBefore(0, "0")
-
+
result = tokens.toString()
expecting = "0abc"
self.failUnlessEqual(result, expecting)
@@ -33,7 +33,7 @@ class T1(testbase.ANTLRTest):
def testInsertAfterLastIndex(self):
tokens = self._parse("abc")
tokens.insertAfter(2, "x")
-
+
result = tokens.toString()
expecting = "abcx"
self.failUnlessEqual(result, expecting)
@@ -43,7 +43,7 @@ class T1(testbase.ANTLRTest):
tokens = self._parse("abc")
tokens.insertBefore(1, "x")
tokens.insertAfter(1, "x")
-
+
result = tokens.toString()
expecting = "axbxc"
self.failUnlessEqual(result, expecting)
@@ -91,7 +91,7 @@ class T1(testbase.ANTLRTest):
tokens.insertBefore(0, "_")
tokens.replace(1, "x")
tokens.replace(1, "y")
-
+
result = tokens.toString()
expecting = "_ayc"
self.failUnlessEqual(expecting, result)
@@ -127,7 +127,7 @@ class T1(testbase.ANTLRTest):
tokens.replace(0, "x")  # supercedes insert at 0

result = tokens.toString()
-        expecting = "xbc"
+        expecting = "0xbc"
self.failUnlessEqual(result, expecting)


@@ -148,7 +148,7 @@ class T1(testbase.ANTLRTest):
tokens.replace(0, "z")

result = tokens.toString()
-        expecting = "zbc"
+        expecting = "yxzbc"
self.failUnlessEqual(result, expecting)


@@ -168,7 +168,7 @@ class T1(testbase.ANTLRTest):
tokens.replace(2, "x")

result = tokens.toString()
-        expecting = "abx"
+        expecting = "abyx"
self.failUnlessEqual(result, expecting)


@@ -176,7 +176,7 @@ class T1(testbase.ANTLRTest):
tokens = self._parse("abc")
tokens.replace(2, "x")
tokens.insertAfter(2, "y")
-
+
result = tokens.toString()
expecting = "abxy"
self.failUnlessEqual(result, expecting)
@@ -269,7 +269,7 @@ class T1(testbase.ANTLRTest):
tokens = self._parse("abcba")
tokens.replace(2, 2, "xyz")
tokens.replace(0, 3, "foo")
-
+
result = tokens.toString()
expecting = "fooa"
self.failUnlessEqual(result, expecting)
@@ -374,11 +374,11 @@ class T1(testbase.ANTLRTest):


def testDropPrevCoveredInsert(self):
-        tokens = self._parse("abcc")
+        tokens = self._parse("abc")
tokens.insertBefore(1, "foo")
tokens.replace(1, 2, "foo") # kill prev insert
result = tokens.toString()
-        expecting = "afooc"
+        expecting = "afoofoo"
self.failUnlessEqual(expecting, result)


@@ -400,6 +400,15 @@ class T1(testbase.ANTLRTest):
self.failUnlessEqual(expecting, result)


+    def testInsertBeforeTokenThenDeleteThatToken(self):
+        tokens = self._parse("abc")
+        tokens.insertBefore(2, "y")
+        tokens.delete(2)
+        result = tokens.toString()
+        expecting = "aby"
+        self.failUnlessEqual(expecting, result)
+
+
class T2(testbase.ANTLRTest):
def setUp(self):
self.compileGrammar('t048rewrite2.g')
@@ -409,11 +418,11 @@ class T2(testbase.ANTLRTest):
cStream = antlr3.StringStream(input)
lexer = self.getLexer(cStream)
tStream = antlr3.TokenRewriteStream(lexer)
-        tStream.LT(1) # fill buffer
+        tStream.fillBuffer()

return tStream

-
+
def testToStringStartStop(self):
# Tokens: 0123456789
# Input:  x = 3 * 0
@@ -468,7 +477,7 @@ class T2(testbase.ANTLRTest):
self.failUnlessEqual(expecting, result)

tokens.insertAfter(17, "// comment")
-        result = tokens.toString(12, 17)
+        result = tokens.toString(12, 18)
expecting = "2 * 0;// comment"
self.failUnlessEqual(expecting, result)

@@ -479,4 +488,3 @@ class T2(testbase.ANTLRTest):

if __name__ == '__main__':
unittest.main()
-
diff --git a/runtime/Python/tests/t053hetero.py b/runtime/Python/tests/t053hetero.py
index 363eed2..db3e9db 100644
--- a/runtime/Python/tests/t053hetero.py
+++ b/runtime/Python/tests/t053hetero.py
@@ -125,6 +125,26 @@ class T(testbase.ANTLRTest):
self.failUnlessEqual("a<V>", found)


+    def testTokenCommonTree(self):
+        grammar = textwrap.dedent(
+            r'''
+            grammar T;
+            options {
+                language=Python;
+                output=AST;
+            }
+            a : ID<CommonTree> ;
+            ID : 'a'..'z'+ ;
+            WS : (' '|'\n') {$channel=HIDDEN;} ;
+            ''')
+
+        found = self.execParser(
+            grammar, 'a',
+            input="a")
+
+        self.failUnlessEqual("a", found)
+
+
def testTokenWithQualifiedType(self):
grammar = textwrap.dedent(
r'''
diff --git a/runtime/Python/tests/t054main.py b/runtime/Python/tests/t054main.py
index 9bb47a9..bb26510 100644
--- a/runtime/Python/tests/t054main.py
+++ b/runtime/Python/tests/t054main.py
@@ -29,14 +29,14 @@ class T(testbase.ANTLRTest):
def main(argv):
raise RuntimeError("no")
}
-
+
ID: ('a'..'z' | '\u00c0'..'\u00ff')+;
WS: ' '+ { $channel = HIDDEN; };
""")
-
-
+
+
stdout = StringIO()
-
+
lexerMod = self.compileInlineGrammar(grammar, returnModule=True)
try:
lexerMod.main(
@@ -45,12 +45,12 @@ class T(testbase.ANTLRTest):
self.fail()
except RuntimeError:
pass
-
+

def testLexerFromFile(self):
input = "foo bar"
inputPath = self.writeFile("input.txt", input)
-
+
grammar = textwrap.dedent(
r"""lexer grammar T1;
options {
@@ -60,10 +60,10 @@ class T(testbase.ANTLRTest):
ID: 'a'..'z'+;
WS: ' '+ { $channel = HIDDEN; };
""")
-
+

stdout = StringIO()
-
+
lexerMod = self.compileInlineGrammar(grammar, returnModule=True)
lexerMod.main(
['lexer.py', inputPath],
@@ -71,11 +71,11 @@ class T(testbase.ANTLRTest):
)

self.failUnlessEqual(len(stdout.getvalue().splitlines()), 3)
-
+

def testLexerFromStdIO(self):
input = "foo bar"
-
+
grammar = textwrap.dedent(
r"""lexer grammar T2;
options {
@@ -85,10 +85,10 @@ class T(testbase.ANTLRTest):
ID: 'a'..'z'+;
WS: ' '+ { $channel = HIDDEN; };
""")
-
-
+
+
stdout = StringIO()
-
+
lexerMod = self.compileInlineGrammar(grammar, returnModule=True)
lexerMod.main(
['lexer.py'],
@@ -97,11 +97,11 @@ class T(testbase.ANTLRTest):
)

self.failUnlessEqual(len(stdout.getvalue().splitlines()), 3)
-
+

def testLexerEncoding(self):
input = u"föö bär".encode('utf-8')
-
+
grammar = textwrap.dedent(
r"""lexer grammar T3;
options {
@@ -111,10 +111,10 @@ class T(testbase.ANTLRTest):
ID: ('a'..'z' | '\u00c0'..'\u00ff')+;
WS: ' '+ { $channel = HIDDEN; };
""")
-
-
+
+
stdout = StringIO()
-
+
lexerMod = self.compileInlineGrammar(grammar, returnModule=True)
lexerMod.main(
['lexer.py', '--encoding', 'utf-8'],
@@ -123,11 +123,11 @@ class T(testbase.ANTLRTest):
)

self.failUnlessEqual(len(stdout.getvalue().splitlines()), 3)
-
+

def testCombined(self):
input = "foo bar"
-
+
grammar = textwrap.dedent(
r"""grammar T4;
options {
@@ -135,14 +135,14 @@ class T(testbase.ANTLRTest):
}

r returns [res]: (ID)+ EOF { $res = $text; };
-
+
ID: 'a'..'z'+;
WS: ' '+ { $channel = HIDDEN; };
""")
-
-
+
+
stdout = StringIO()
-
+
lexerMod, parserMod = self.compileInlineGrammar(grammar, returnModule=True)
parserMod.main(
['combined.py', '--rule', 'r'],
@@ -152,11 +152,11 @@ class T(testbase.ANTLRTest):

stdout = stdout.getvalue()
self.failUnlessEqual(len(stdout.splitlines()), 1, stdout)
-
+

def testCombinedOutputAST(self):
input = "foo + bar"
-
+
grammar = textwrap.dedent(
r"""grammar T5;
options {
@@ -165,15 +165,15 @@ class T(testbase.ANTLRTest):
}

r: ID OP^ ID EOF!;
-
+
ID: 'a'..'z'+;
OP: '+';
WS: ' '+ { $channel = HIDDEN; };
""")
-
-
+
+
stdout = StringIO()
-
+
lexerMod, parserMod = self.compileInlineGrammar(grammar, returnModule=True)
parserMod.main(
['combined.py', '--rule', 'r'],
@@ -183,7 +183,7 @@ class T(testbase.ANTLRTest):

stdout = stdout.getvalue().strip()
self.failUnlessEqual(stdout, "(+ foo bar)")
-
+

def testTreeParser(self):
grammar = textwrap.dedent(
@@ -194,12 +194,12 @@ class T(testbase.ANTLRTest):
}

r: ID OP^ ID EOF!;
-
+
ID: 'a'..'z'+;
OP: '+';
WS: ' '+ { $channel = HIDDEN; };
''')
-
+
treeGrammar = textwrap.dedent(
r'''tree grammar T6Walker;
options {
@@ -224,7 +224,7 @@ class T(testbase.ANTLRTest):

stdout = stdout.getvalue().strip()
self.failUnlessEqual(stdout, "u'a + b'")
-
+

def testTreeParserRewrite(self):
grammar = textwrap.dedent(
@@ -235,12 +235,12 @@ class T(testbase.ANTLRTest):
}

r: ID OP^ ID EOF!;
-
+
ID: 'a'..'z'+;
OP: '+';
WS: ' '+ { $channel = HIDDEN; };
''')
-
+
treeGrammar = textwrap.dedent(
r'''tree grammar T7Walker;
options {
@@ -268,7 +268,7 @@ class T(testbase.ANTLRTest):
stdout = stdout.getvalue().strip()
self.failUnlessEqual(stdout, "(+ (ARG a) (ARG b))")

-
+

def testGrammarImport(self):
slave = textwrap.dedent(
@@ -277,7 +277,7 @@ class T(testbase.ANTLRTest):
options {
language=Python;
}
-
+
a : B;
''')

@@ -288,7 +288,7 @@ class T(testbase.ANTLRTest):
del sys.modules[parserName+'Parser']
except KeyError:
pass
-
+
master = textwrap.dedent(
r'''
grammar T8M;
@@ -302,7 +302,7 @@ class T(testbase.ANTLRTest):
''')

stdout = StringIO()
-
+
lexerMod, parserMod = self.compileInlineGrammar(master, returnModule=True)
parserMod.main(
['import.py', '--rule', 's'],
diff --git a/runtime/Python/tests/t059debug.py b/runtime/Python/tests/t059debug.py
index 2097b66..4058745 100644
--- a/runtime/Python/tests/t059debug.py
+++ b/runtime/Python/tests/t059debug.py
@@ -268,7 +268,7 @@ class T(testbase.ANTLRTest):
['consumeHiddenToken', '1', '6', '99', '1', '1', '"'],
['location', '6', '8'],
['enterSubRule', '1'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['LT', '1', '2', '5', '0', '1', '2', '"1'],
['exitDecision', '1'],
['enterAlt', '1'],
@@ -276,7 +276,7 @@ class T(testbase.ANTLRTest):
['LT', '1', '2', '5', '0', '1', '2', '"1'],
['consumeToken', '2', '5', '0', '1', '2', '"1'],
['consumeHiddenToken', '3', '6', '99', '1', '3', '"'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['LT', '1', '4', '4', '0', '1', '4', '"b'],
['exitDecision', '1'],
['enterAlt', '1'],
@@ -284,7 +284,7 @@ class T(testbase.ANTLRTest):
['LT', '1', '4', '4', '0', '1', '4', '"b'],
['consumeToken', '4', '4', '0', '1', '4', '"b'],
['consumeHiddenToken', '5', '6', '99', '1', '5', '"'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['LT', '1', '6', '4', '0', '1', '6', '"c'],
['exitDecision', '1'],
['enterAlt', '1'],
@@ -292,14 +292,14 @@ class T(testbase.ANTLRTest):
['LT', '1', '6', '4', '0', '1', '6', '"c'],
['consumeToken', '6', '4', '0', '1', '6', '"c'],
['consumeHiddenToken', '7', '6', '99', '1', '7', '"'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['LT', '1', '8', '5', '0', '1', '8', '"3'],
['exitDecision', '1'],
['enterAlt', '1'],
['location', '6', '8'],
['LT', '1', '8', '5', '0', '1', '8', '"3'],
['consumeToken', '8', '5', '0', '1', '8', '"3'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['LT', '1', '-1', '-1', '0', '1', '9', '"<EOF>'],
['exitDecision', '1'],
['exitSubRule', '1'],
@@ -343,7 +343,7 @@ class T(testbase.ANTLRTest):
['consumeHiddenToken', '1', '6', '99', '1', '1', '"'],
['location', '6', '8'],
['enterSubRule', '1'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['LT', '1', '2', '5', '0', '1', '2', '"1'],
['exitDecision', '1'],
['enterAlt', '1'],
@@ -351,7 +351,7 @@ class T(testbase.ANTLRTest):
['LT', '1', '2', '5', '0', '1', '2', '"1'],
['consumeToken', '2', '5', '0', '1', '2', '"1'],
['consumeHiddenToken', '3', '6', '99', '1', '3', '"'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['LT', '1', '4', '4', '0', '1', '4', '"b'],
['exitDecision', '1'],
['enterAlt', '1'],
@@ -359,7 +359,7 @@ class T(testbase.ANTLRTest):
['LT', '1', '4', '4', '0', '1', '4', '"b'],
['consumeToken', '4', '4', '0', '1', '4', '"b'],
['consumeHiddenToken', '5', '6', '99', '1', '5', '"'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['LT', '1', '6', '4', '0', '1', '6', '"c'],
['exitDecision', '1'],
['enterAlt', '1'],
@@ -367,14 +367,14 @@ class T(testbase.ANTLRTest):
['LT', '1', '6', '4', '0', '1', '6', '"c'],
['consumeToken', '6', '4', '0', '1', '6', '"c'],
['consumeHiddenToken', '7', '6', '99', '1', '7', '"'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['LT', '1', '8', '5', '0', '1', '8', '"3'],
['exitDecision', '1'],
['enterAlt', '1'],
['location', '6', '8'],
['LT', '1', '8', '5', '0', '1', '8', '"3'],
['consumeToken', '8', '5', '0', '1', '8', '"3'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['LT', '1', '-1', '-1', '0', '1', '9', '"<EOF>'],
['exitDecision', '1'],
['exitSubRule', '1'],
@@ -462,7 +462,7 @@ class T(testbase.ANTLRTest):
['consumeHiddenToken', '1', '6', '99', '1', '1', '"'],
['location', '6', '8'],
['enterSubRule', '1'],
-                     ['enterDecision', '1'],
+                     ['enterDecision', '1', '0'],
['LT', '1', '2', '5', '0', '1', '2', '"1'],
['exitDecision', '1'],
['enterAlt', '2'],
@@ -520,7 +520,7 @@ class T(testbase.ANTLRTest):
['consumeHiddenToken', '1', '7', '99', '1', '1', '"'],
['location', '6', '8'],
['enterSubRule', '1'],
-                     ['enterDecision', '1'],
+                     ['enterDecision', '1', '0'],
['LT', '1', '2', '6', '0', '1', '2', '"!'],
['LT', '1', '2', '6', '0', '1', '2', '"!'],
['LT', '1', '2', '6', '0', '1', '2', '"!'],
@@ -563,7 +563,7 @@ class T(testbase.ANTLRTest):
self.assertTrue(debugger.success)
expected = [['enterRule', 'T.g', 'a'],
['location', '6', '1'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['LT', '1', '0', '5', '0', '1', '0', '"1'],
['exitDecision', '1'],
['enterAlt', '2'],
@@ -693,7 +693,7 @@ class T(testbase.ANTLRTest):
['enterAlt', '1'],
['location', '6', '5'],
['enterSubRule', '1'],
-                    ['enterDecision', '1'],
+                    ['enterDecision', '1', '0'],
['mark', '0'],
['LT', '1', '0', '4', '0', '1', '0', '"a'],
['consumeToken', '0', '4', '0', '1', '0', '"a'],
@@ -708,7 +708,7 @@ class T(testbase.ANTLRTest):
['enterAlt', '1'],
['location', '8', '5'],
['enterSubRule', '3'],
-                    ['enterDecision', '3'],
+                    ['enterDecision', '3', '0'],
['LT', '1', '0', '4', '0', '1', '0', '"a'],
['exitDecision', '3'],
['enterAlt', '1'],
@@ -716,7 +716,7 @@ class T(testbase.ANTLRTest):
['LT', '1', '0', '4', '0', '1', '0', '"a'],
['LT', '1', '0', '4', '0', '1', '0', '"a'],
['consumeToken', '0', '4', '0', '1', '0', '"a'],
-                    ['enterDecision', '3'],
+                    ['enterDecision', '3', '0'],
['LT', '1', '1', '6', '0', '1', '1', '"!'],
['exitDecision', '3'],
['exitSubRule', '3'],
diff --git a/runtime/Python/tests/t060leftrecursion.py b/runtime/Python/tests/t060leftrecursion.py
index ef85723..5940ac2 100644
--- a/runtime/Python/tests/t060leftrecursion.py
+++ b/runtime/Python/tests/t060leftrecursion.py
@@ -341,7 +341,7 @@ class TestLeftRecursion(testbase.ANTLRTest):
|   e '.'^ 'this'
|   e '.'^ 'super' '('^ expressionList? ')'!
|   e '.'^ 'new'^ ID '('! expressionList? ')'!
-                    |       'new'^ type ( '(' expressionList? ')'! | (options {k=1;}:'[' e ']'!)+) // ugly; simplified
+                |       'new'^ type ( '(' expressionList? ')'! | (options {k=1;}:'[' e ']'!)+) // ugly; simplified
|   e '['^ e ']'!
|   '('^ type ')'! e
|   e ('++'^ | '--'^)
diff --git a/runtime/Python/tests/testbase.py b/runtime/Python/tests/testbase.py
index b358f2f..19c7fec 100644
--- a/runtime/Python/tests/testbase.py
+++ b/runtime/Python/tests/testbase.py
@@ -67,7 +67,7 @@ if 'CLASSPATH' not in os.environ:
baseDir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
libDir = os.path.join(baseDir, 'lib')

-    jar = os.path.join(libDir, 'stringtemplate-3.1b1.jar')
+    jar = os.path.join(libDir, 'ST-4.0.1.jar')
if not os.path.isfile(jar):
raise DistutilsFileError(
"Missing file '%s'. Grap it from a distribution package."
@@ -211,70 +211,69 @@ class ANTLRTest(unittest.TestCase):
return

try:
-            # get dependencies from antlr
-            if grammarName in dependencyCache:
-                dependencies = dependencyCache[grammarName]
-
-            else:
-                dependencies = []
-                cmd = ('cd %s; java %s %s org.antlr.Tool -o . -depend %s 2>&1'
-                       % (self.baseDir, javaOptions, classpath, grammarPath)
-                       )
-
-                output = ""
-                failed = False
-
-                fp = os.popen(cmd)
-                for line in fp:
-                    output += line
-
-                    if line.startswith('error('):
-                        failed = True
-                    elif ':' in line:
-                        a, b = line.strip().split(':', 1)
-                        dependencies.append(
-                            (os.path.join(self.baseDir, a.strip()),
-                             [os.path.join(self.baseDir, b.strip())])
-                            )
-
-                rc = fp.close()
-                if rc is not None:
-                    failed = True
-
-                if failed:
-                    raise GrammarCompileError(
-                        "antlr -depend failed with code %s on grammar '%s':\n\n"
-                        % (rc, grammarName)
-                        + cmd
-                        + "\n"
-                        + output
-                        )
-
-                # add dependencies to my .stg files
-                templateDir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'tool', 'src', 'main', 'resources', 'org', 'antlr', 'codegen', 'templates', 'Python'))
-                templates = glob.glob(os.path.join(templateDir, '*.stg'))
-
-                for dst, src in dependencies:
-                    src.extend(templates)
-
-                dependencyCache[grammarName] = dependencies
-
-
-            rebuild = False
-
-            for dest, sources in dependencies:
-                if not os.path.isfile(dest):
-                    rebuild = True
-                    break
-
-                for source in sources:
-                    if os.path.getmtime(source) > os.path.getmtime(dest):
-                        rebuild = True
-                        break
-
-
-            if rebuild:
-                self._invokeantlr(self.baseDir, grammarPath, options, javaOptions)
+        #     # get dependencies from antlr
+        #     if grammarName in dependencyCache:
+        #         dependencies = dependencyCache[grammarName]
+
+        #     else:
+        #         dependencies = []
+        #         cmd = ('cd %s; java %s %s org.antlr.Tool -o . -depend %s 2>&1'
+        #                % (self.baseDir, javaOptions, classpath, grammarPath))
+
+        #         output = ""
+        #         failed = False
+
+        #         fp = os.popen(cmd)
+        #         for line in fp:
+        #             output += line
+
+        #             if line.startswith('error('):
+        #                 failed = True
+        #             elif ':' in line:
+        #                 a, b = line.strip().split(':', 1)
+        #                 dependencies.append(
+        #                     (os.path.join(self.baseDir, a.strip()),
+        #                      [os.path.join(self.baseDir, b.strip())])
+        #                     )
+
+        #         rc = fp.close()
+        #         if rc is not None:
+        #             failed = True
+
+        #         if failed:
+        #             raise GrammarCompileError(
+        #                 "antlr -depend failed with code %s on grammar '%s':\n\n"
+        #                 % (rc, grammarName)
+        #                 + cmd
+        #                 + "\n"
+        #                 + output
+        #                 )
+
+        #         # add dependencies to my .stg files
+        #         templateDir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'tool', 'src', 'main', 'resources', 'org', 'antlr', 'codegen', 'templates', 'Python'))
+        #         templates = glob.glob(os.path.join(templateDir, '*.stg'))
+
+        #         for dst, src in dependencies:
+        #             src.extend(templates)
+
+        #         dependencyCache[grammarName] = dependencies
+
+        #     rebuild = False
+        #     for dest, sources in dependencies:
+        #         if not os.path.isfile(dest):
+        #             rebuild = True
+        #             break
+
+        #         for source in sources:
+        #             if os.path.getmtime(source) > os.path.getmtime(dest):
+        #                 rebuild = True
+        #                 break
+
+
+        #     if rebuild:
+        #         self._invokeantlr(self.baseDir, grammarPath, options, javaOptions)
+
+            self._invokeantlr(self.baseDir, grammarPath, options, javaOptions)

except:
# mark grammar as broken
diff --git a/tool/src/main/java/org/antlr/analysis/SemanticContext.java b/tool/src/main/java/org/antlr/analysis/SemanticContext.java
index ffdb829..51262a6 100644
--- a/tool/src/main/java/org/antlr/analysis/SemanticContext.java
+++ b/tool/src/main/java/org/antlr/analysis/SemanticContext.java
@@ -323,7 +323,7 @@ public abstract class SemanticContext {
hashcode = calculateHashCode();
}

-		public CommutativePredicate(Iterable<SemanticContext> contexts){
+		public CommutativePredicate(HashSet<SemanticContext> contexts){
for (SemanticContext context : contexts){
if (context.getClass() == this.getClass()){
CommutativePredicate predicate = (CommutativePredicate)context;
@@ -442,7 +442,7 @@ public abstract class SemanticContext {
super(a,b);
}

-		public AND(Iterable<SemanticContext> contexts) {
+		public AND(HashSet<SemanticContext> contexts) {
super(contexts);
}

@@ -497,7 +497,7 @@ public abstract class SemanticContext {
super(a,b);
}

-		public OR(Iterable<SemanticContext> contexts) {
+		public OR(HashSet<SemanticContext> contexts) {
super(contexts);
}

diff --git a/tool/src/main/java/org/antlr/tool/GrammarAST.java b/tool/src/main/java/org/antlr/tool/GrammarAST.java
index b010845..023c824 100644
--- a/tool/src/main/java/org/antlr/tool/GrammarAST.java
+++ b/tool/src/main/java/org/antlr/tool/GrammarAST.java
@@ -389,7 +389,7 @@ public class GrammarAST extends CommonTree {
return this;
}
// else check children
-		Iterable<Tree> descendants = descendants(this);
+		List<Tree> descendants = descendants(this);
for (Tree child : descendants) {
if ( child.getType()==ttype ) {
return (GrammarAST)child;
diff --git a/tool/src/main/resources/org/antlr/codegen/templates/Python/AST.stg b/tool/src/main/resources/org/antlr/codegen/templates/Python/AST.stg
index d213f08..ec41a71 100644
--- a/tool/src/main/resources/org/antlr/codegen/templates/Python/AST.stg
+++ b/tool/src/main/resources/org/antlr/codegen/templates/Python/AST.stg
@@ -26,10 +26,6 @@
THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*/

-/* in sync with Java/AST.stg revision 36 */
-
-group AST;
-
finishedBacktracking(block) ::= <<
<if(backtracking)>
if <actions.(actionScope).synpredgate>:
@@ -84,12 +80,12 @@ ruleLabelDefs() ::= <<
<super.ruleLabelDefs()>
<[ruleDescriptor.tokenLabels,ruleDescriptor.wildcardTreeLabels,
ruleDescriptor.wildcardTreeListLabels]
-    :{<it.label.text>_tree = None}; separator="\n">
-<ruleDescriptor.tokenListLabels:{<it.label.text>_tree = None}; separator="\n">
+    :{it | <it.label.text>_tree = None}; separator="\n">
+<ruleDescriptor.tokenListLabels:{it | <it.label.text>_tree = None}; separator="\n">
<ruleDescriptor.allTokenRefsInAltsWithRewrites
-    :{stream_<it> = RewriteRule<rewriteElementType>Stream(self._adaptor, "token <it>")}; separator="\n">
+    :{it | stream_<it> = RewriteRule<rewriteElementType>Stream(self._adaptor, "token <it>")}; separator="\n">
<ruleDescriptor.allRuleRefsInAltsWithRewrites
-    :{stream_<it> = RewriteRuleSubtreeStream(self._adaptor, "rule <it>")}; separator="\n">
+    :{it | stream_<it> = RewriteRuleSubtreeStream(self._adaptor, "rule <it>")}; separator="\n">
>>

/** When doing auto AST construction, we must define some variables;
@@ -108,7 +104,7 @@ root_0 = self._adaptor.nil()<\n>
>>


-// // T r a c k i n g  R u l e  E l e m e n t s
+// T r a c k i n g  R u l e  E l e m e n t s

/** ID and track it for use in a rewrite rule */
tokenRefTrack(token,label,elementIndex,terminalOptions) ::= <<
@@ -145,7 +141,7 @@ ruleRefTrack(rule,label,elementIndex,args,scope) ::= <<
/** x+=rule when output=AST and tracking for rewrite */
ruleRefTrackAndListLabel(rule,label,elementIndex,args,scope) ::= <<
<ruleRefTrack(...)>
-<listLabel(elem=label+".tree",...)>
+<listLabel(label, {<label>.tree})>
>>

/** ^(rule ...) rewrite */
@@ -157,7 +153,7 @@ ruleRefRuleRootTrack(rule,label,elementIndex,args,scope) ::= <<
/** ^(x+=rule ...) rewrite */
ruleRefRuleRootTrackAndListLabel(rule,label,elementIndex,args,scope) ::= <<
<ruleRefRuleRootTrack(...)>
-<listLabel(elem=label+".tree",...)>
+<listLabel(label, {<label>.tree})>
>>

// R e w r i t e
@@ -214,23 +210,23 @@ self.input.replaceChildren(

rewriteCodeLabels() ::= <<
<referencedTokenLabels
-    :{stream_<it> = RewriteRule<rewriteElementType>Stream(self._adaptor, "token <it>", <it>)};
+    :{it | stream_<it> = RewriteRule<rewriteElementType>Stream(self._adaptor, "token <it>", <it>)};
separator="\n"
>
<referencedTokenListLabels
-    :{stream_<it> = RewriteRule<rewriteElementType>Stream(self._adaptor, "token <it>", list_<it>)};
+    :{it | stream_<it> = RewriteRule<rewriteElementType>Stream(self._adaptor, "token <it>", list_<it>)};
separator="\n"
>
<referencedWildcardLabels
-    :{stream_<it> = RewriteRuleSubtreeStream(self._adaptor, "wildcard <it>", <it>)};
+    :{it | stream_<it> = RewriteRuleSubtreeStream(self._adaptor, "wildcard <it>", <it>)};
separator="\n"
>
<referencedWildcardListLabels
-    :{stream_<it> = RewriteRuleSubtreeStream(self._adaptor, "wildcard <it>", list_<it>)};
+    :{it | stream_<it> = RewriteRuleSubtreeStream(self._adaptor, "wildcard <it>", list_<it>)};
separator="\n"
>
<referencedRuleLabels
-    :{
+    :{it |
if <it> is not None:
stream_<it> = RewriteRuleSubtreeStream(self._adaptor, "rule <it>", <it>.tree)
else:
@@ -239,7 +235,7 @@ else:
separator="\n"
>
<referencedRuleListLabels
-    :{stream_<it> = RewriteRuleSubtreeStream(self._adaptor, "token <it>", list_<it>)};
+    :{it| stream_<it> = RewriteRuleSubtreeStream(self._adaptor, "token <it>", list_<it>)};
separator="\n"
>
>>
@@ -290,12 +286,10 @@ while <referencedElements:{el | stream_<el>.hasNext()}; separator=" or ">:
>>

rewriteAltRest(a) ::= <<
-<if(a.pred)>
-if <a.pred>:
+<if(a.pred)>if <a.pred>:
# <a.description>
<a.alt>
-<else>
-se: <! little hack to get if .. elif .. else block right !>
+<else>se: <! little hack to get if .. elif .. else block right !>
# <a.description>
<a.alt>
<endif>
@@ -359,11 +353,11 @@ root_<treeLevel> = self._adaptor.becomeRoot(<createRewriteNodeFromElement(...)>,
>>

rewriteImaginaryTokenRef(args,token,terminalOptions,elementIndex) ::= <<
-self._adaptor.addChild(root_<treeLevel>, <createImaginaryNode(tokenType=token, ...)>)<\n>
+self._adaptor.addChild(root_<treeLevel>, <createImaginaryNode(token, terminalOptions, args)>)<\n>
>>

rewriteImaginaryTokenRefRoot(args,token,terminalOptions,elementIndex) ::= <<
-root_<treeLevel> = self._adaptor.becomeRoot(<createImaginaryNode(tokenType=token, ...)>, root_<treeLevel>)<\n>
+root_<treeLevel> = self._adaptor.becomeRoot(<createImaginaryNode(token, terminalOptions, args)>, root_<treeLevel>)<\n>
>>

/** plain -> {foo} action */
diff --git a/tool/src/main/resources/org/antlr/codegen/templates/Python/ASTParser.stg b/tool/src/main/resources/org/antlr/codegen/templates/Python/ASTParser.stg
index 9627451..64ffa68 100644
--- a/tool/src/main/resources/org/antlr/codegen/templates/Python/ASTParser.stg
+++ b/tool/src/main/resources/org/antlr/codegen/templates/Python/ASTParser.stg
@@ -37,7 +37,6 @@
*  The situation is not too bad as rewrite (->) usage makes ^ and !
*  invalid. There is no huge explosion of combinations.
*/
-group ASTParser;

finishedBacktracking(block) ::= <<
<if(backtracking)>
@@ -64,7 +63,7 @@ self._adaptor.addChild(root_0, <label>_tree)
>>

/** ID! and output=AST (same as plain tokenRef) */
-tokenRefBang(token,label,elementIndex) ::= "<super.tokenRef(...)>"
+tokenRefBang(token,label,elementIndex,terminalOptions) ::= "<super.tokenRef(...)>"

/** ID^ and output=AST */
tokenRefRuleRoot(token,label,elementIndex,terminalOptions) ::= <<
@@ -106,15 +105,15 @@ tokenRefRuleRootAndListLabel(token,label,terminalOptions,elementIndex) ::= <<
// rather than just added on code.  Investigate that refactoring when
// I have more time.

-matchSet(s,label,terminalOptions,elementIndex,postmatchCode) ::= <<
-<super.matchSet(..., postmatchCode={<finishedBacktracking({self._adaptor.addChild(root_0, <createNodeFromToken(...)>)})>})>
->>
+matchSet(s,label,terminalOptions,elementIndex,postmatchCode) ::= <%
+<super.matchSet(postmatchCode={<finishedBacktracking({self._adaptor.addChild(root_0, <createNodeFromToken(...)>)})>}, ...)>
+%>

matchRuleBlockSet(s,label,terminalOptions,elementIndex,postmatchCode,treeLevel="0") ::= <<
<matchSet(...)>
>>

-matchSetBang(s,label,elementIndex,postmatchCode) ::= "<super.matchSet(...)>"
+matchSetBang(s,label,elementIndex,terminalOptions,postmatchCode) ::= "<super.matchSet(...)>"

// note there is no matchSetTrack because -> rewrites force sets to be
// plain old blocks of alts: (A|B|...|C)
@@ -123,7 +122,7 @@ matchSetRuleRoot(s,label,terminalOptions,elementIndex,debug) ::= <<
<if(label)>
<label> = self.input.LT(1)<\n>
<endif>
-<super.matchSet(..., postmatchCode={<finishedBacktracking({root_0 = self._adaptor.becomeRoot(<createNodeFromToken(...)>, root_0)})>})>
+<super.matchSet(postmatchCode={<finishedBacktracking({root_0 = self._adaptor.becomeRoot(<createNodeFromToken(...)>, root_0)})>}, ...)>
>>

// RULE REF AST
@@ -146,24 +145,24 @@ ruleRefRuleRoot(rule,label,elementIndex,args,scope) ::= <<
/** x+=rule when output=AST */
ruleRefAndListLabel(rule,label,elementIndex,args,scope) ::= <<
<ruleRef(...)>
-<listLabel(elem=label+".tree",...)>
+<listLabel(label, {<label>.tree})>
>>

/** x+=rule! when output=AST is a rule ref with list addition */
ruleRefBangAndListLabel(rule,label,elementIndex,args,scope) ::= <<
<ruleRefBang(...)>
-<listLabel(elem=label+".tree",...)>
+<listLabel(label, {<label>.tree})>
>>

/** x+=rule^ */
ruleRefRuleRootAndListLabel(rule,label,elementIndex,args,scope) ::= <<
<ruleRefRuleRoot(...)>
-<listLabel(elem=label+".tree",...)>
+<listLabel(label, {<label>.tree})>
>>

// WILDCARD AST

-wildcard(label,elementIndex) ::= <<
+wildcard(token,label,elementIndex,terminalOptions) ::= <<
<super.wildcard(...)>
<finishedBacktracking({
<label>_tree = self._adaptor.createWithPayload(<label>)
@@ -173,7 +172,7 @@ self._adaptor.addChild(root_0, <label>_tree)

wildcardBang(label,elementIndex) ::= "<super.wildcard(...)>"

-wildcardRuleRoot(label,elementIndex) ::= <<
+wildcardRuleRoot(token,label,elementIndex,terminalOptions) ::= <<
<super.wildcard(...)>
<finishedBacktracking({
<label>_tree = self._adaptor.createWithPayload(<label>)
@@ -181,13 +180,13 @@ root_0 = self._adaptor.becomeRoot(<label>_tree, root_0)
})>
>>

-createNodeFromToken(label,terminalOptions) ::= <<
+createNodeFromToken(label,terminalOptions) ::= <%
<if(terminalOptions.node)>
<terminalOptions.node>(<label>) <! new MethodNode(IDLabel) !>
<else>
self._adaptor.createWithPayload(<label>)
<endif>
->>
+%>

ruleCleanUp() ::= <<
<super.ruleCleanUp()>
diff --git a/tool/src/main/resources/org/antlr/codegen/templates/Python/Dbg.stg b/tool/src/main/resources/org/antlr/codegen/templates/Python/Dbg.stg
index a16b054..364ba03 100644
--- a/tool/src/main/resources/org/antlr/codegen/templates/Python/Dbg.stg
+++ b/tool/src/main/resources/org/antlr/codegen/templates/Python/Dbg.stg
@@ -28,7 +28,6 @@
/** Template overrides to add debugging to normal Python output;
*  If ASTs are built, then you'll also get ASTDbg.stg loaded.
*/
-group Dbg;

@outputFile.imports() ::= <<
<@super.imports()>
@@ -188,11 +187,12 @@ try:
if self.getRuleLevel() == 0:
self._dbg.commence();
self.incRuleLevel()
-    self._dbg.location(<ruleDescriptor.tree.line>, <ruleDescriptor.tree.column>)
+    <% ST uses zero-based columns, we want one-base %>
+    self._dbg.location(<ruleDescriptor.tree.line>, <ruleDescriptor.tree.charPositionInLine>+1)

<@super.body()>

-    self._dbg.location(<ruleDescriptor.EORNode.line>, <ruleDescriptor.EORNode.column>)
+    self._dbg.location(<ruleDescriptor.EORNode.line>, <ruleDescriptor.EORNode.charPositionInLine>+1)
finally:
self._dbg.exitRule(self.getGrammarFileName(), "<ruleName>")
self.decRuleLevel()
@@ -290,10 +290,10 @@ finally:
self._dbg.exitDecision(<decisionNumber>)
>>

-@altSwitchCase.prealt() ::= "<enterAlt(n=i)>"
+@altSwitchCase.prealt() ::= "<enterAlt(altNum)>"

@element.prematch() ::=
-    "self._dbg.location(<it.line>, <it.pos>)"
+    "self._dbg.location(<e.line>, <e.pos>)"

@matchSet.mismatchedSetException() ::=
"self._dbg.recognitionException(mse)"
diff --git a/tool/src/main/resources/org/antlr/codegen/templates/Python/Python.stg b/tool/src/main/resources/org/antlr/codegen/templates/Python/Python.stg
index 319ece3..71c324c 100644
--- a/tool/src/main/resources/org/antlr/codegen/templates/Python/Python.stg
+++ b/tool/src/main/resources/org/antlr/codegen/templates/Python/Python.stg
@@ -26,13 +26,21 @@
THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*/

-group Python;
-
/** The API version of the runtime that recognizers generated by this runtime
*  need.
*/
apiVersion() ::= "1"

+// System.Boolean.ToString() returns "True" and "False", but the proper C# literals are "true" and "false"
+// The Java version of Boolean returns "true" and "false", so they map to themselves here.
+booleanLiteral ::= [
+	       "True":"true",
+	       "False":"false",
+	       "true":"true",
+	       "false":"false",
+	       default:"false"
+]
+
/** The overall file structure of a recognizer; stores methods for rules
*  and cyclic DFAs plus support code.
*/
@@ -63,7 +71,7 @@ from antlr3.compat import set, frozenset
HIDDEN = BaseRecognizer.HIDDEN

# token types
-<tokens:{<it.name>=<it.type>}; separator="\n">
+<tokens:{it | <it.name>=<it.type>}; separator="\n">

<recognizer>

@@ -96,13 +104,13 @@ if __name__ == '__main__':

>>

-lexer(grammar, name, tokens, scopes, rules, numRules, labelType="CommonToken",
-      filterMode, superClass="Lexer") ::= <<
+lexer(grammar, name, tokens, scopes, rules, numRules, filterMode,
+      labelType="CommonToken", superClass="Lexer") ::= <<
<grammar.directDelegates:
{g|from <g.recognizerName> import <g.recognizerName>}; separator="\n">

class <grammar.recognizerName>(<@superClassName><superClass><@end>):
-    <scopes:{<if(it.isDynamicGlobalScope)><globalAttributeScope(scope=it)><endif>}>
+    <scopes:{it|<if(it.isDynamicGlobalScope)><globalAttributeScope(scope=it)><endif>}>

grammarFileName = "<fileName>"
api_version = <apiVersion()>
@@ -124,6 +132,7 @@ class <grammar.recognizerName>(<@superClassName><superClass><@end>):
{g|self.<g:delegateName()> = <g:delegateName()>}; separator="\n">
<last(grammar.delegators):
{g|self.gParent = <g:delegateName()>}; separator="\n">
+        self.delegates = [<grammar.delegates: {g|self.<g:delegateName()>}; separator = ", ">]

<cyclicDFAs:{dfa | <cyclicDFAInit(dfa)>}; separator="\n">

@@ -192,12 +201,12 @@ def nextToken(self):
def memoize(self, input, ruleIndex, ruleStartIndex, success):
if self._state.backtracking > 1:
# is Lexer always superclass?
-        <@superClassName><superClass><@end>.memoize(self, input, ruleIndex, ruleStartIndex, success)
+        super(<grammar.recognizerName>, self).memoize(input, ruleIndex, ruleStartIndex, success)


def alreadyParsedRule(self, input, ruleIndex):
if self._state.backtracking > 1:
-        return <@superClassName><superClass><@end>.alreadyParsedRule(self, input, ruleIndex)
+        return super(<grammar.recognizerName>, self).alreadyParsedRule(input, ruleIndex)
return False


@@ -210,9 +219,8 @@ filteringActionGate() ::= "self._state.backtracking == 1"
/** How to generate a parser */

genericParser(grammar, name, scopes, tokens, tokenNames, rules, numRules,
-              bitsets, inputStreamType, superClass, filterMode,
-              ASTLabelType="Object", labelType, members, rewriteElementType,
-              init) ::= <<
+              bitsets, inputStreamType, superClass, labelType, members,
+	      rewriteElementType, filterMode, init, ASTLabelType="Object") ::= <<
<if(grammar.grammarIsRoot)>
# token names
tokenNames = [
@@ -222,12 +230,12 @@ tokenNames = [
<else>
from <grammar.composite.rootGrammar.recognizerName> import tokenNames<\n>
<endif>
-<scopes:{<if(it.isDynamicGlobalScope)><globalAttributeScopeClass(scope=it)><endif>}>
+<scopes:{it|<if(it.isDynamicGlobalScope)><globalAttributeScopeClass(scope=it)><endif>}>

<grammar.directDelegates:
{g|from <g.recognizerName> import <g.recognizerName>}; separator="\n">

-<rules:{<ruleAttributeScopeClass(scope=it.ruleDescriptor.ruleScope)>}>
+<rules:{it|<ruleAttributeScopeClass(scope=it.ruleDescriptor.ruleScope)>}>

class <grammar.recognizerName>(<@superClassName><superClass><@end>):
grammarFileName = "<fileName>"
@@ -249,8 +257,8 @@ class <grammar.recognizerName>(<@superClassName><superClass><@end>):

<cyclicDFAs:{dfa | <cyclicDFAInit(dfa)>}; separator="\n">

-        <scopes:{<if(it.isDynamicGlobalScope)><globalAttributeScopeStack(scope=it)><endif>}>
-	<rules:{<ruleAttributeScopeStack(scope=it.ruleDescriptor.ruleScope)>}>
+        <scopes:{it | <if(it.isDynamicGlobalScope)><globalAttributeScopeStack(scope=it)><endif>}>
+	<rules:{it | <ruleAttributeScopeStack(scope=it.ruleDescriptor.ruleScope)>}>

<init>

@@ -258,17 +266,16 @@ class <grammar.recognizerName>(<@superClassName><superClass><@end>):
{g|self.<g:delegateName()> = <g:delegateName()>}; separator="\n">
<grammar.directDelegates:
{g|self.<g:delegateName()> = <g.recognizerName>(<trunc(g.delegators):{p|<p:delegateName()>, }>self, input, state)}; separator="\n">
-        <!grammar.directDelegates:
-         {g|self.<g:delegateName()> = <g.recognizerName>(self<grammar.delegators:{g|, <g:delegateName()>}>, input, state)}; separator="\n"!>
+        <grammar.indirectDelegates:
+         {g|<g:delegateName()> = <g.delegator:delegateName()>.<g:delegateName()>}; separator="\n">
<last(grammar.delegators):
{g|self.gParent = self.<g:delegateName()>}; separator="\n">
+        self.delegates = [<grammar.delegates: {g|self.<g:delegateName()>}; separator = ", ">]

-        <@init>
-        <@end>
+	<@init><@end>


-    <@members>
-    <@end>
+    <@members><@end>

<members>

@@ -276,33 +283,47 @@ class <grammar.recognizerName>(<@superClassName><superClass><@end>):

<! generate rule/method definitions for imported rules so they
appear to be defined in this recognizer. !>
-    # Delegated rules
<grammar.delegatedRules:{ruleDescriptor| <delegateRule(ruleDescriptor)> }; separator="\n">

<synpreds:{p | <synpred(p)>}>

<cyclicDFAs:cyclicDFA()> <! dump tables for all DFA !>

-    <bitsets:{FOLLOW_<it.name>_in_<it.inName><it.tokenIndex> = frozenset([<it.tokenTypes:{<it>};separator=", ">])<\n>}>
+    <bitsets:{it | FOLLOW_<it.name>_in_<it.inName><it.tokenIndex> = frozenset([<it.tokenTypes:{it | <it>};separator=", ">])<\n>}>

>>

delegateRule(ruleDescriptor) ::= <<
-def <ruleDescriptor.name>(self, <ruleDescriptor.parameterScope:parameterScope(scope=it)>):
+def <ruleDescriptor.name>(self, <ruleDescriptor.parameterScope:parameterScope()>):
<\ >   <if(ruleDescriptor.hasReturnValue)>return <endif>self.<ruleDescriptor.grammar:delegateName()>.<ruleDescriptor.name>(<ruleDescriptor.parameterScope.attributes:{a|<a.name>}; separator=", ">)


>>

-parser(grammar, name, scopes, tokens, tokenNames, rules, numRules, bitsets, ASTLabelType="object", superClass="Parser", labelType="Token", members={<actions.parser.members>}) ::= <<
-<genericParser(inputStreamType="TokenStream", rewriteElementType="Token", init={<actions.parser.init>}, ...)>
+parser(grammar, name, scopes, tokens, tokenNames, rules, numRules, bitsets,
+       ASTLabelType="Object", superClass="Parser", labelType="Token",
+       members={<actions.parser.members>},
+       init={<actions.parser.init>}
+       ) ::= <<
+<genericParser(grammar, name, scopes, tokens, tokenNames, rules, numRules,
+              bitsets, "TokenStream", superClass,
+              labelType, members, "Token",
+              false, init, ASTLabelType)>
>>

/** How to generate a tree parser; same as parser except the input
*  stream is a different type.
*/
-treeParser(grammar, name, scopes, tokens, tokenNames, globalAction, rules, numRules, bitsets, labelType={<ASTLabelType>}, ASTLabelType="Object", superClass="TreeParser", members={<actions.treeparser.members>}, filterMode) ::= <<
-<genericParser(inputStreamType="TreeNodeStream", rewriteElementType="Node", init={<actions.treeparser.init>}, ...)>
+treeParser(grammar, name, scopes, tokens, tokenNames, globalAction, rules,
+           numRules, bitsets, filterMode, labelType={<ASTLabelType>}, ASTLabelType="Object",
+           superClass={<if(filterMode)><if(buildAST)>TreeRewriter<else>TreeFilter<endif><else>TreeParser<endif>},
+           members={<actions.treeparser.members>},
+	   init={<actions.treeparser.init>}
+           ) ::= <<
+<genericParser(grammar, name, scopes, tokens, tokenNames, rules, numRules,
+              bitsets, "TreeNodeStream", superClass,
+              labelType, members, "Node",
+              filterMode, init, ASTLabelType)>
>>

/** A simpler version of a rule template that is specific to the imaginary
@@ -315,7 +336,7 @@ treeParser(grammar, name, scopes, tokens, tokenNames, globalAction, rules, numRu
synpredRule(ruleName, ruleDescriptor, block, description, nakedBlock) ::=
<<
# $ANTLR start "<ruleName>"
-def <ruleName>_fragment(self, <ruleDescriptor.parameterScope:parameterScope(scope=it)>):
+def <ruleName>_fragment(self, <ruleDescriptor.parameterScope:parameterScope()>):
<ruleLabelDefs()>
<if(trace)>
self.traceIn("<ruleName>_fragment", <ruleDescriptor.index>)
@@ -384,7 +405,7 @@ rule(ruleName,ruleDescriptor,block,emptyRule,description,exceptions,finally,memo
# $ANTLR start "<ruleName>"
# <fileName>:<description>
<ruleDescriptor.actions.decorate>
-def <ruleName>(self, <ruleDescriptor.parameterScope:parameterScope(scope=it)>):
+def <ruleName>(self, <ruleDescriptor.parameterScope:parameterScope()>):
<if(trace)>
self.traceIn("<ruleName>", <ruleDescriptor.index>)<\n>
<endif>
@@ -468,44 +489,44 @@ retval.start = self.input.LT(1)<\n>
>>

ruleScopeSetUp() ::= <<
-<ruleDescriptor.useScopes:{self.<it>_stack.append(<it>_scope())}; separator="\n">
-<ruleDescriptor.ruleScope:{self.<it.name>_stack.append(<it.name>_scope())}; separator="\n">
+<ruleDescriptor.useScopes:{it | self.<it>_stack.append(<it>_scope())}; separator="\n">
+<ruleDescriptor.ruleScope:{it | self.<it.name>_stack.append(<it.name>_scope())}; separator="\n">
>>

ruleScopeCleanUp() ::= <<
-<ruleDescriptor.useScopes:{self.<it>_stack.pop()}; separator="\n">
-<ruleDescriptor.ruleScope:{self.<it.name>_stack.pop()}; separator="\n">
+<ruleDescriptor.useScopes:{it | self.<it>_stack.pop()}; separator="\n">
+<ruleDescriptor.ruleScope:{it | self.<it.name>_stack.pop()}; separator="\n">
>>

ruleLabelDefs() ::= <<
<[ruleDescriptor.tokenLabels,ruleDescriptor.tokenListLabels,
ruleDescriptor.wildcardTreeLabels,ruleDescriptor.wildcardTreeListLabels]
-    :{<it.label.text> = None}; separator="\n"
+    :{it | <it.label.text> = None}; separator="\n"
>
<[ruleDescriptor.tokenListLabels,ruleDescriptor.ruleListLabels,
ruleDescriptor.wildcardTreeListLabels]
-    :{list_<it.label.text> = None}; separator="\n"
+    :{it | list_<it.label.text> = None}; separator="\n"
>
<[ruleDescriptor.ruleLabels,ruleDescriptor.ruleListLabels]
-    :ruleLabelDef(label=it); separator="\n"
+    :ruleLabelDef(); separator="\n"
>
-<ruleDescriptor.ruleListLabels:{<it.label.text> = None}; separator="\n">
+<ruleDescriptor.ruleListLabels:{it | <it.label.text> = None}; separator="\n">
>>

lexerRuleLabelDefs() ::= <<
<[ruleDescriptor.tokenLabels,
ruleDescriptor.tokenListLabels,
ruleDescriptor.ruleLabels]
-    :{<it.label.text> = None}; separator="\n"
+    :{it | <it.label.text> = None}; separator="\n"
>
-<ruleDescriptor.charLabels:{<it.label.text> = None}; separator="\n">
+<ruleDescriptor.charLabels:{it | <it.label.text> = None}; separator="\n">
<[ruleDescriptor.tokenListLabels,
ruleDescriptor.ruleListLabels]
-    :{list_<it.label.text> = None}; separator="\n"
+    :{it | list_<it.label.text> = None}; separator="\n"
>
>>

-ruleReturnValue() ::= <<
+ruleReturnValue() ::= <%
<if(!ruleDescriptor.isSynPred)>
<if(ruleDescriptor.hasReturnValue)>
<if(ruleDescriptor.hasSingleReturnValue)>
@@ -515,7 +536,7 @@ retval
<endif>
<endif>
<endif>
->>
+%>

ruleCleanUp() ::= <<
<if(ruleDescriptor.hasMultipleReturnValues)>
@@ -540,7 +561,7 @@ if self._state.backtracking > 0:
*/
lexerRule(ruleName,nakedBlock,ruleDescriptor,block,memoize) ::= <<
# $ANTLR start "<ruleName>"
-def m<ruleName>(self, <ruleDescriptor.parameterScope:parameterScope(scope=it)>):
+def m<ruleName>(self, <ruleDescriptor.parameterScope:parameterScope()>):
<if(trace)>
self.traceIn("<ruleName>", <ruleDescriptor.index>)<\n>
<endif>
@@ -614,7 +635,7 @@ blockBody() ::= <<
<@decision><decision><@end>
<@postdecision()>
<@prebranch()>
-<alts:altSwitchCase(); separator="\nel">
+<alts:{a | <altSwitchCase(i, a)>}; separator="\nel">
<@postbranch()>
>>

@@ -626,7 +647,7 @@ alt<decisionNumber> = <maxAlt>
<@predecision()>
<@decision><decision><@end>
<@postdecision()>
-<alts:altSwitchCase(); separator="\nel">
+<alts:{a | <altSwitchCase(i, a)>}; separator="\nel">
>>

ruleBlockSingleAlt(alts,decls,decision,enclosingBlockLevel,blockLevel,decisionNumber,description) ::= <<
@@ -664,7 +685,7 @@ while True: #loop<decisionNumber>
<@predecision()>
<@decisionBody><decision><@end>
<@postdecision()>
-    <alts:altSwitchCase(); separator="\nel">
+    <alts:{a | <altSwitchCase(i, a)>}; separator="\nel">
else:
if cnt<decisionNumber> >= 1:
break #loop<decisionNumber>
@@ -696,7 +717,7 @@ while True: #loop<decisionNumber>
<@predecision()>
<@decisionBody><decision><@end>
<@postdecision()>
-    <alts:altSwitchCase(); separator="\nel">
+    <alts:{a | <altSwitchCase(i, a)>}; separator="\nel">
else:
break #loop<decisionNumber>
>>
@@ -714,10 +735,10 @@ optionalBlockSingleAlt ::= block
*  number.  A DFA predicts the alternative and then a simple switch
*  does the jump to the code that actually matches that alternative.
*/
-altSwitchCase() ::= <<
-if alt<decisionNumber> == <i>:
+altSwitchCase(altNum,alt) ::= <<
+if alt<decisionNumber> == <altNum>:
<@prealt()>
-    <it>
+    <alt>
>>

/** An alternative is just a list of elements; at outermost level */
@@ -738,20 +759,20 @@ noRewrite(rewriteBlockLevel, treeLevel) ::= ""
// E L E M E N T S

/** Dump the elements one per line */
-element() ::= <<
+element(e) ::= <<
<@prematch()>
-<it.el><\n>
+<e.el><\n>
>>

/** match a token optionally with a label in front */
tokenRef(token,label,elementIndex,terminalOptions) ::= <<
-<if(label)><label>=<endif>self.match(self.input, <token>, self.FOLLOW_<token>_in_<ruleName><elementIndex>)
+<if(label)><label> = <endif>self.match(self.input, <token>, self.FOLLOW_<token>_in_<ruleName><elementIndex>)
>>

/** ids+=ID */
tokenRefAndListLabel(token,label,elementIndex,terminalOptions) ::= <<
-<tokenRef(...)>
-<listLabel(elem=label,...)>
+<tokenRef(token,label,elementIndex,terminalOptions)>
+<listLabel(label, label)>
>>

listLabel(label, elem) ::= <<
@@ -777,7 +798,7 @@ self.matchRange(<a>, <b>)
>>

/** For now, sets are interval tests and must be tested inline */
-matchSet(s,label,elementIndex,postmatchCode="") ::= <<
+matchSet(s,label,elementIndex,terminalOptions,postmatchCode="") ::= <<
<if(label)>
<label> = self.input.LT(1)<\n>
<endif>
@@ -810,7 +831,7 @@ matchRuleBlockSet ::= matchSet

matchSetAndListLabel(s,label,elementIndex,postmatchCode) ::= <<
<matchSet(...)>
-<listLabel(elem=label,...)>
+<listLabel(label, label)>
>>

/** Match a string literal */
@@ -828,16 +849,16 @@ self.match(<string>)
<endif>
>>

-wildcard(label,elementIndex) ::= <<
+wildcard(token,label,elementIndex,terminalOptions) ::= <<
<if(label)>
<label> = self.input.LT(1)<\n>
<endif>
self.matchAny(self.input)
>>

-wildcardAndListLabel(label,elementIndex) ::= <<
+wildcardAndListLabel(token,label,elementIndex,terminalOptions) ::= <<
<wildcard(...)>
-<listLabel(elem=label,...)>
+<listLabel(label,label)>
>>

/** Match . wildcard in lexer */
@@ -849,8 +870,8 @@ self.matchAny()
>>

wildcardCharListLabel(label, elementIndex) ::= <<
-<wildcardChar(...)>
-<listLabel(elem=label,...)>
+<wildcardChar(label, elementIndex)>
+<listLabel(label, label)>
>>

/** Match a rule reference by invoking it possibly with arguments
@@ -867,8 +888,8 @@ self._state.following.pop()

/** ids+=rule */
ruleRefAndListLabel(rule,label,elementIndex,args,scope) ::= <<
-<ruleRef(...)>
-<listLabel(elem=label,...)>
+<ruleRef(rule,label,elementIndex,args,scope)>
+<listLabel(label, label)>
>>

/** A lexer rule reference
@@ -897,8 +918,8 @@ self.<if(scope)><scope:delegateName()>.<endif>m<rule.name>(<args; separator=", "

/** i+=INT in lexer */
lexerRuleRefAndListLabel(rule,label,args,elementIndex,scope) ::= <<
-<lexerRuleRef(...)>
-<listLabel(elem=label,...)>
+<lexerRuleRef(rule,label,args,elementIndex,scope)>
+<listLabel(label, label)>
>>

/** EOF in the lexer */
@@ -919,9 +940,9 @@ self.match(EOF)
// used for left-recursive rules
recRuleDefArg()                       ::= "<recRuleArg()>"
recRuleArg()                          ::= "_p"
-recRuleAltPredicate(ruleName,opPrec)  ::= "<recRuleArg()> \<= <opPrec>"
+recRuleAltPredicate(ruleName, opPrec) ::= "<recRuleArg()> \<= <opPrec>"
recRuleSetResultAction()              ::= "root_0 = $<ruleName>_primary.tree"
-recRuleSetReturnAction(src, name)      ::= "$<name> = $<src>.<name>"
+recRuleSetReturnAction(src, name)     ::= "$<name> = $<src>.<name>"

/** match ^(root children) in tree parser */
tree(root, actionsAfterRoot, children, nullableChildList,
@@ -945,7 +966,7 @@ self.match(self.input, UP, None)
*  also hoisted into a prediction expression).
*/
validateSemanticPredicate(pred,description) ::= <<
-if not (<evalPredicate(...)>):
+if not (<evalPredicate(pred, description)>):
<ruleBacktrackFailure()>
raise FailedPredicateException(self.input, "<ruleName>", "<description>")

@@ -1050,7 +1071,7 @@ else:
>>

dfaEdgeSwitch(labels, targetState) ::= <<
-if <labels:{LA<decisionNumber> == <it>}; separator=" or ">:
+if <labels:{it | LA<decisionNumber> == <it>}; separator=" or ">:
<targetState>
>>

@@ -1192,7 +1213,7 @@ andPredicates(left,right) ::= "((<left>) and (<right>))"

orPredicates(operands) ::= "(<first(operands)><rest(operands):{o |  or <o>}>)"

-notPredicate(pred) ::= "not (<evalPredicate(...)>)"
+notPredicate(pred) ::= "not (<evalPredicate(pred, {})>)"

evalPredicate(pred,description) ::= "(<pred>)"

@@ -1205,9 +1226,9 @@ lookaheadTest(atom,k,atomAsInt) ::= "LA<decisionNumber>_<stateNumber> == <atom>"
*/
isolatedLookaheadTest(atom,k,atomAsInt) ::= "self.input.LA(<k>) == <atom>"

-lookaheadRangeTest(lower,upper,k,rangeNumber,lowerAsInt,upperAsInt) ::= <<
+lookaheadRangeTest(lower,upper,k,rangeNumber,lowerAsInt,upperAsInt) ::= <%
(<lower> \<= LA<decisionNumber>_<stateNumber> \<= <upper>)
->>
+%>

isolatedLookaheadRangeTest(lower,upper,k,rangeNumber,lowerAsInt,upperAsInt) ::= "(<lower> \<= self.input.LA(<k>) \<= <upper>)"

@@ -1219,7 +1240,7 @@ globalAttributeScopeClass(scope) ::= <<
<if(scope.attributes)>
class <scope.name>_scope(object):
def __init__(self):
-        <scope.attributes:{self.<it.decl> = None}; separator="\n">
+        <scope.attributes:{it | self.<it.decl> = None}; separator="\n">

<endif>
>>
@@ -1234,7 +1255,7 @@ ruleAttributeScopeClass(scope) ::= <<
<if(scope.attributes)>
class <scope.name>_scope(object):
def __init__(self):
-        <scope.attributes:{self.<it.decl> = None}; separator="\n">
+        <scope.attributes:{it | self.<it.decl> = None}; separator="\n">

<endif>
>>
@@ -1245,8 +1266,8 @@ self.<scope.name>_stack = []<\n>
<endif>
>>

-delegateName() ::= <<
-<if(it.label)><it.label><else>g<it.name><endif>
+delegateName(d) ::= <<
+<if(d.label)><d.label><else>g<d.name><endif>
>>

/** Define a rule label including default value */
@@ -1254,7 +1275,7 @@ ruleLabelDef(label) ::= <<
<label.label.text> = None<\n>
>>

-returnStructName() ::= "<it.name>_return"
+returnStructName(r) ::= "<r.name>_return"

/** Define a return struct for a rule if the code needs to access its
*  start/stop tokens, tree stuff, attributes, ...  Leave a hole for
@@ -1266,7 +1287,7 @@ class <ruleDescriptor:returnStructName()>(<if(TREE_PARSER)>Tree<else>Parser<endi
def __init__(self):
super(<grammar.recognizerName>.<ruleDescriptor:returnStructName()>, self).__init__()

-        <scope.attributes:{self.<it.decl> = None}; separator="\n">
+        <scope.attributes:{it | self.<it.decl> = None}; separator="\n">
<@ruleReturnInit()>


@@ -1276,13 +1297,13 @@ class <ruleDescriptor:returnStructName()>(<if(TREE_PARSER)>Tree<else>Parser<endi
>>

parameterScope(scope) ::= <<
-<scope.attributes:{<it.decl>}; separator=", ">
+<scope.attributes:{it | <it.decl>}; separator=", ">
>>

parameterAttributeRef(attr) ::= "<attr.name>"
parameterSetAttributeRef(attr,expr) ::= "<attr.name> = <expr>"

-scopeAttributeRef(scope,attr,index,negIndex) ::= <<
+scopeAttributeRef(scope,attr,index,negIndex) ::= <%
<if(negIndex)>
self.<scope>_stack[-<negIndex>].<attr.name>
<else>
@@ -1292,7 +1313,7 @@ self.<scope>_stack[<index>].<attr.name>
self.<scope>_stack[-1].<attr.name>
<endif>
<endif>
->>
+%>

/* not applying patch because of bug in action parser!

@@ -1308,7 +1329,7 @@ self.<scope>_stack[-1].<attr.name>

*/

-scopeSetAttributeRef(scope,attr,expr,index,negIndex) ::= <<
+scopeSetAttributeRef(scope,attr,expr,index,negIndex) ::= <%
<if(negIndex)>
<!FIXME: this seems not to be used by ActionTranslator...!>
self.<scope>_stack[-<negIndex>].<attr.name> = <expr>
@@ -1320,7 +1341,7 @@ self.<scope>_stack[<index>].<attr.name> = <expr>
self.<scope>_stack[-1].<attr.name> = <expr>
<endif>
<endif>
->>
+%>

/** $x is either global scope or x is rule with dynamic scope; refers
*  to stack itself not top of stack.  This is useful for predicates
@@ -1329,29 +1350,29 @@ self.<scope>_stack[-1].<attr.name> = <expr>
isolatedDynamicScopeRef(scope) ::= "self.<scope>_stack"

/** reference an attribute of rule; might only have single return value */
-ruleLabelRef(referencedRule,scope,attr) ::= <<
+ruleLabelRef(referencedRule,scope,attr) ::= <%
<if(referencedRule.hasMultipleReturnValues)>
((<scope> is not None) and [<scope>.<attr.name>] or [None])[0]
<else>
<scope>
<endif>
->>
+%>

-returnAttributeRef(ruleDescriptor,attr) ::= <<
+returnAttributeRef(ruleDescriptor,attr) ::= <%
<if(ruleDescriptor.hasMultipleReturnValues)>
retval.<attr.name>
<else>
<attr.name>
<endif>
->>
+%>

-returnSetAttributeRef(ruleDescriptor,attr,expr) ::= <<
+returnSetAttributeRef(ruleDescriptor,attr,expr) ::= <%
<if(ruleDescriptor.hasMultipleReturnValues)>
retval.<attr.name> = <expr>
<else>
<attr.name> = <expr>
<endif>
->>
+%>

/** How to translate $tokenLabel */
tokenLabelRef(label) ::= "<label>"
@@ -1374,7 +1395,7 @@ tokenLabelPropertyRef_tree(scope,attr) ::= "<scope>_tree"
ruleLabelPropertyRef_start(scope,attr) ::= "<scope>.start"
ruleLabelPropertyRef_stop(scope,attr) ::= "<scope>.stop"
ruleLabelPropertyRef_tree(scope,attr) ::= "<scope>.tree"
-ruleLabelPropertyRef_text(scope,attr) ::= <<
+ruleLabelPropertyRef_text(scope,attr) ::= <%
<if(TREE_PARSER)>
((<scope> is not None) and [self.input.getTokenStream().toString(
self.input.getTreeAdaptor().getTokenStartIndex(<scope>.start),
@@ -1383,7 +1404,7 @@ ruleLabelPropertyRef_text(scope,attr) ::= <<
<else>
((<scope> is not None) and [self.input.toString(<scope>.start,<scope>.stop)] or [None])[0]
<endif>
->>
+%>
ruleLabelPropertyRef_st(scope,attr) ::= "((<scope> is not None) and [<scope>.st] or [None])[0]"

/** Isolated $RULE ref ok in lexer as it's a Token */
@@ -1449,5 +1470,5 @@ execForcedAction(action) ::= "<action>"

codeFileExtension() ::= ".py"

-true() ::= "True"
-false() ::= "False"
+true_value() ::= "True"
+false_value() ::= "False"

